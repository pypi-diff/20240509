# Comparing `tmp/Abava_SDK-1.0.6-py3-none-any.whl.zip` & `tmp/Abava_SDK-1.0.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,9 +1,9 @@
-Zip file size: 50988 bytes, number of entries: 55
--rw-rw-rw-  2.0 fat     6638 b- defN 24-Mar-15 10:15 abava/__init__.py
+Zip file size: 52730 bytes, number of entries: 56
+-rw-rw-rw-  2.0 fat     6677 b- defN 24-Mar-26 10:32 abava/__init__.py
 -rw-rw-rw-  2.0 fat       76 b- defN 24-Jan-02 08:40 abava/_version.py
 -rw-rw-rw-  2.0 fat     1851 b- defN 24-Jan-02 08:40 abava/abava_data.py
 -rw-rw-rw-  2.0 fat      837 b- defN 24-Jan-05 12:21 abava/api.py
 -rw-rw-rw-  2.0 fat       84 b- defN 24-Jan-02 08:40 abava/const.py
 -rw-rw-rw-  2.0 fat     1014 b- defN 24-Mar-12 09:24 abava/exception.py
 -rw-rw-rw-  2.0 fat       23 b- defN 24-Jan-02 08:40 abava/check/__init__.py
 -rw-rw-rw-  2.0 fat      385 b- defN 24-Jan-02 08:40 abava/check/check.py
@@ -26,32 +26,33 @@
 -rw-rw-rw-  2.0 fat     2080 b- defN 24-Jan-02 08:40 abava/export_format/voc/voc.py
 -rw-rw-rw-  2.0 fat       48 b- defN 24-Jan-02 08:40 abava/export_format/yolo/__init__.py
 -rw-rw-rw-  2.0 fat     4052 b- defN 24-Jan-02 08:40 abava/export_format/yolo/export_yolo.py
 -rw-rw-rw-  2.0 fat      789 b- defN 24-Jan-02 08:40 abava/export_format/yolo/yolo.py
 -rw-rw-rw-  2.0 fat       48 b- defN 24-Jan-02 08:40 abava/factory/__init__.py
 -rw-rw-rw-  2.0 fat     5517 b- defN 24-Mar-15 10:12 abava/factory/data_factory.py
 -rw-rw-rw-  2.0 fat       50 b- defN 24-Mar-12 09:24 abava/post_process/__init__.py
--rw-rw-rw-  2.0 fat     1654 b- defN 24-Mar-14 09:57 abava/post_process/process.py
+-rw-rw-rw-  2.0 fat     1629 b- defN 24-Mar-26 08:22 abava/post_process/process.py
 -rw-rw-rw-  2.0 fat       52 b- defN 24-Mar-12 09:24 abava/post_process/coco/__init__.py
--rw-rw-rw-  2.0 fat     4671 b- defN 24-Mar-15 09:58 abava/post_process/coco/coco_postprocess.py
+-rw-rw-rw-  2.0 fat     4743 b- defN 24-Apr-23 10:20 abava/post_process/coco/coco_postprocess.py
+-rw-rw-rw-  2.0 fat     3648 b- defN 24-Mar-26 10:24 abava/post_process/coco/test.py
 -rw-rw-rw-  2.0 fat       72 b- defN 24-Jan-02 08:40 abava/utils/__init__.py
 -rw-rw-rw-  2.0 fat    23727 b- defN 24-Jan-10 15:16 abava/utils/cv_tools.py
 -rw-rw-rw-  2.0 fat     6088 b- defN 24-Mar-12 10:00 abava/utils/general.py
--rw-rw-rw-  2.0 fat    18084 b- defN 24-Mar-19 03:52 abava/utils/pc_tools.py
+-rw-rw-rw-  2.0 fat    19049 b- defN 24-May-09 11:04 abava/utils/pc_tools.py
 -rw-rw-rw-  2.0 fat       21 b- defN 24-Jan-02 08:40 abava/visualize/__init__.py
 -rw-rw-rw-  2.0 fat     4649 b- defN 24-Jan-02 08:40 abava/visualize/visual.py
 -rw-rw-rw-  2.0 fat       48 b- defN 24-Jan-02 08:40 abava/visualize/coco/__init__.py
 -rw-rw-rw-  2.0 fat     1864 b- defN 24-Jan-02 08:40 abava/visualize/coco/visual_coco.py
 -rw-rw-rw-  2.0 fat       48 b- defN 24-Jan-02 08:40 abava/visualize/labelme/__init__.py
 -rw-rw-rw-  2.0 fat     2343 b- defN 24-Jan-02 08:40 abava/visualize/labelme/visual_labelme.py
 -rw-rw-rw-  2.0 fat       48 b- defN 24-Jan-02 08:40 abava/visualize/source/__init__.py
 -rw-rw-rw-  2.0 fat     2691 b- defN 24-Jan-02 08:40 abava/visualize/source/visual_source.py
 -rw-rw-rw-  2.0 fat       48 b- defN 24-Jan-02 08:40 abava/visualize/voc/__init__.py
 -rw-rw-rw-  2.0 fat     3035 b- defN 24-Jan-02 08:40 abava/visualize/voc/visual_voc.py
 -rw-rw-rw-  2.0 fat       48 b- defN 24-Jan-02 08:40 abava/visualize/yolo/__init__.py
 -rw-rw-rw-  2.0 fat     2213 b- defN 24-Jan-02 08:40 abava/visualize/yolo/visual_yolo.py
--rw-rw-rw-  2.0 fat     1079 b- defN 24-Mar-19 10:10 Abava_SDK-1.0.6.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     2059 b- defN 24-Mar-19 10:10 Abava_SDK-1.0.6.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Mar-19 10:10 Abava_SDK-1.0.6.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 24-Mar-19 10:10 Abava_SDK-1.0.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     4737 b- defN 24-Mar-19 10:10 Abava_SDK-1.0.6.dist-info/RECORD
-55 files, 154168 bytes uncompressed, 43382 bytes compressed:  71.9%
+-rw-rw-rw-  2.0 fat     1079 b- defN 24-May-09 11:06 Abava_SDK-1.0.7.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     2059 b- defN 24-May-09 11:06 Abava_SDK-1.0.7.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-09 11:06 Abava_SDK-1.0.7.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 24-May-09 11:06 Abava_SDK-1.0.7.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4825 b- defN 24-May-09 11:06 Abava_SDK-1.0.7.dist-info/RECORD
+56 files, 158955 bytes uncompressed, 44986 bytes compressed:  71.7%
```

## zipnote {}

```diff
@@ -96,14 +96,17 @@
 
 Filename: abava/post_process/coco/__init__.py
 Comment: 
 
 Filename: abava/post_process/coco/coco_postprocess.py
 Comment: 
 
+Filename: abava/post_process/coco/test.py
+Comment: 
+
 Filename: abava/utils/__init__.py
 Comment: 
 
 Filename: abava/utils/cv_tools.py
 Comment: 
 
 Filename: abava/utils/general.py
@@ -144,23 +147,23 @@
 
 Filename: abava/visualize/yolo/__init__.py
 Comment: 
 
 Filename: abava/visualize/yolo/visual_yolo.py
 Comment: 
 
-Filename: Abava_SDK-1.0.6.dist-info/LICENSE
+Filename: Abava_SDK-1.0.7.dist-info/LICENSE
 Comment: 
 
-Filename: Abava_SDK-1.0.6.dist-info/METADATA
+Filename: Abava_SDK-1.0.7.dist-info/METADATA
 Comment: 
 
-Filename: Abava_SDK-1.0.6.dist-info/WHEEL
+Filename: Abava_SDK-1.0.7.dist-info/WHEEL
 Comment: 
 
-Filename: Abava_SDK-1.0.6.dist-info/top_level.txt
+Filename: Abava_SDK-1.0.7.dist-info/top_level.txt
 Comment: 
 
-Filename: Abava_SDK-1.0.6.dist-info/RECORD
+Filename: Abava_SDK-1.0.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## abava/__init__.py

```diff
@@ -170,9 +170,9 @@
     postprocess_f = PostProcessFactory()
 
     @classmethod
     def coco_split(cls, data_path, out_pat=None, test_size=0.3, train_size=None, shuffle=True):
         cls.postprocess_f.post_process_product(data_path, out_pat).split(test_size, train_size, shuffle)
 
     @classmethod
-    def coco_merge(cls, data_path, out_pat=None):
-        cls.postprocess_f.post_process_product(data_path, out_pat).split()
+    def coco_merge(cls, data_path, out_pat=None, merged_file_name=None):
+        cls.postprocess_f.post_process_product(data_path, out_pat).merge(merged_file_name)
```

## abava/post_process/process.py

```diff
@@ -41,12 +41,11 @@
             file_path = 'merge_labels'
 
         if self.out_path is None:
             out_root = './' + file_path
         else:
             out_root = join(self.out_path, file_path)
 
-        print(out_root)
         if not os.path.exists(out_root):
             os.makedirs(out_root)
         with open(join(out_root, filename), 'w', encoding='utf-8') as write_f:
             json.dump(final_data, write_f, indent=2, ensure_ascii=False)
```

## abava/post_process/coco/coco_postprocess.py

```diff
@@ -6,15 +6,14 @@
 
 from tqdm import tqdm
 
 from ..process import PostProcess
 from ...utils import load_json
 from ...abava_data import ABAVA
 from ...export_format.coco.coco import COCO
-# from .coco import COCO
 from datetime import datetime
 
 
 class CocoProcess(PostProcess):
     def __init__(self, data_path, out_path=None):
         super(CocoProcess, self).__init__(data_path, out_path)
         COCO.info = {
@@ -23,70 +22,85 @@
             'version': 'ABAVA SDK V1.0',
             'year': f"{datetime.utcnow().year}",
             'contributor': '',
             'date_created': datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
         }
 
     def split(self, test_size=0.3, train_size=None, shuffle=True):
+        def process_data(image_list, mode):
+            annotations = []
+            for image in tqdm(image_list):
+                id = image.id
+                selected_annotations = [item for item in coco_data.annotations if item['image_id'] == id]
+                annotations.extend(selected_annotations)
+            COCO.images = sorted(image_list, key=lambda item: item['id'])
+            sorted_annotations = sorted(annotations, key=lambda item: item['image_id'])
+            COCO.annotations = [{**self.abava2dict(item), 'id': idx + 1} for idx, item in
+                                enumerate(sorted_annotations)]
+            COCO.categories = coco_data.categories
+            file_name = Path(self.data_path).parts[-1].replace('.json', f'_{mode}.json')
+            self.save_labels(self.abava2dict(COCO), 'split', file_name)
+
         if train_size is None:
             train_size = 1 - test_size
 
         coco_data = ABAVA(load_json(self.data_path))
-        train_annotations = []
-        test_annotations = []
 
         if shuffle:
             random.shuffle(coco_data.images)
 
         test_images = coco_data.images[:int(len(coco_data.images) * test_size)]
-        for image in tqdm(test_images):
-            test_id = image.id
-            test_selected_annotations = [item for item in coco_data.annotations if item['image_id'] == test_id]
-            test_annotations.extend(test_selected_annotations)
-        COCO.images = sorted(test_images, key=lambda item: item['id'])
-        sorted_test_annotations = sorted(test_annotations, key=lambda item: item['image_id'])
-        COCO.annotations = [{**self.abava2dict(item), 'id': idx + 1} for idx, item in enumerate(sorted_test_annotations)]
-        test_file_name = Path(self.data_path).parts[-1].replace('.json', '_test.json')
-        self.save_labels(self.abava2dict(COCO), 'split', test_file_name)
-
-        train_images = coco_data.images[int(len(coco_data.images) * test_size):int(len(coco_data.images) * (test_size + train_size))]
-        for image in tqdm(train_images):
-            train_id = image.id
-            train_selected_annotations = [item for item in coco_data.annotations if item['image_id'] == train_id]
-            train_annotations.extend(train_selected_annotations)
-        COCO.images = sorted(train_images, key=lambda x: x['id'])
-        sorted_train_annotations = sorted(train_annotations, key=lambda x: x['image_id'])
-        COCO.annotations = [{**self.abava2dict(item), 'id': idx + 1} for idx, item in enumerate(sorted_train_annotations)]
-        train_file_name = Path(self.data_path).parts[-1].replace('.json', '_train.json')
-        self.save_labels(self.abava2dict(COCO), 'split', train_file_name)
-
+        train_images = coco_data.images[
+                       int(len(coco_data.images) * test_size):int(len(coco_data.images) * (test_size + train_size))]
         eval_images = coco_data.images[int(len(coco_data.images) * (test_size + train_size)):]
+
+        process_data(test_images, 'test')
+        process_data(train_images, 'train')
         if len(eval_images) > 0:
-            eval_annotations = [item for item in coco_data.annotations if
-                                item not in test_annotations and item not in train_annotations]
-            sorted_eval_annotations = sorted(eval_annotations, key=lambda x: x['image_id'])
-            COCO.images = sorted(eval_images, key=lambda x: x['id'])
-            COCO.annotations = [{**self.abava2dict(item), 'id': idx + 1} for idx, item in enumerate(sorted_eval_annotations)]
-            eval_file_name = Path(self.data_path).parts[-1].replace('.json', '_val.json')
-            self.save_labels(self.abava2dict(COCO), 'split', eval_file_name)
+            process_data(eval_images, 'eval')
 
-    def merge(self):
+    def merge(self, merged_file_name=None):
         merged_images = []
         merged_annotations = []
+        merged_categories = []
         coco_paths = glob.glob(self.data_path + '/*')
         for coco_path in coco_paths:
             coco_data = ABAVA(load_json(coco_path))
             image_length = len(merged_images)
             images_id = [item['id'] for item in coco_data.images]
             mapping = {id: i + 1 + image_length for i, id in enumerate(images_id)}
             merged_images.extend([{**self.abava2dict(item), 'id': mapping[item['id']]} for item in coco_data.images])
-            merged_annotations.extend([{**self.abava2dict(item), 'image_id': mapping[item['image_id']]} for item in coco_data.annotations])
+            categories = coco_data.categories
+
+            merged_categories_dict = {item['name']: self.abava2dict(item) for item in merged_categories}
+            categories_dict = {item['name']: self.abava2dict(item) for item in categories}
+            for k in categories_dict:
+                categories_dict[k]['source'] = "temp"
+            merged_categories_dict.update(categories_dict)
+
+            id_mapping = {}
+
+            for i, item in enumerate(merged_categories_dict.values()):
+                if item.get('source') == "temp":
+                    old_id = item['id']
+                    item['id'] = i
+                    id_mapping[old_id] = i
+            merged_categories = list(merged_categories_dict.values())
+            merged_annotations.extend(
+                [{**self.abava2dict(item), 'image_id': mapping[item['image_id']],
+                  'category_id': id_mapping[item['category_id']]} for item in coco_data.annotations])
+
+        for item in merged_categories:
+            if 'source' in item:
+                del item['source']
 
         sorted_merged_images = sorted(merged_images, key=lambda item: item['id'])
         sorted_merged_annotations = sorted(merged_annotations, key=lambda item: item['image_id'])
+        sorted_merged_categories = sorted(merged_categories, key=lambda item: item['id'])
 
         COCO.images = sorted_merged_images
         COCO.annotations = sorted_merged_annotations
+        COCO.categories = sorted_merged_categories
 
-        merged_file_name = 'merged_data.json'
-        self.save_labels(self.abava2dict(COCO), 'merge', merged_file_name)
-
+        if merged_file_name is None:
+            merged_file_name = 'merged_data'
+        self.save_labels(self.abava2dict(COCO), 'merge', merged_file_name + '.json')
```

## abava/utils/pc_tools.py

```diff
@@ -1,27 +1,26 @@
 # -*-coding:utf-8 -*-
 import math
 import re
 import struct
-import time
 
 import cv2
+import lzf
 import numpy as np
-import os
 import dask.dataframe as dd
-from ..exception import AbavaParameterException, AbavaNotImplementException
+from ..exception import AbavaNotImplementException
 
 
 def read_pcd(pcd_path):
     """
     read pcd file
     :param pcd_path:
     :return:
     """
-    global pc_points
+    # global pc_points
     headers_lines = 11
     try:
         with open(pcd_path, 'r') as f:
             header = [next(f) for _ in range(headers_lines)]
     except UnicodeDecodeError:
         with open(pcd_path, 'rb') as f:
             header = [next(f).decode('ISO-8859-1') for _ in range(headers_lines)]
@@ -37,32 +36,32 @@
                 headers[key] = fields[1]
                 headers['data_start'] = i + 1
             elif key == 'POINTS':
                 headers[key] = int(fields[1])
             else:
                 headers[key] = fields[1:]
 
-    type_size_map = {('U', '1'): np.uint8, ('U', '2'): np.uint16, ('U', '4'): np.uint32,
-                     ('F', '4'): np.float32,
+    type_size_map = {('U', '1'): np.uint8, ('U', '2'): np.uint16, ('U', '4'): np.uint32, ('U', '8'): np.uint64,
+                     ('F', '4'): np.float32, ('F', '8'): np.float64,
                      ('I', '1'): np.int8, ('I', '2'): np.int16, ('I', '4'): np.int32}
 
+    dtype_list = [(name, type_size_map[(field_type, size)]) for name, field_type, size in
+                  zip(headers["FIELDS"], headers['TYPE'], headers["SIZE"])]
+    dt = np.dtype(dtype_list)
+
     num_fields = len(headers['FIELDS'])
     if headers['DATA'] == 'ascii':
-        dtypes = {idx: type_size_map[(type, headers['SIZE'][idx])] for idx, type in enumerate(headers['TYPE'])}
         df = dd.read_csv(pcd_path, skiprows=headers['data_start'], sep=" ", header=None, assume_missing=True,
-                         dtype=dtypes)
+                         dtype=dt)
         pc_points = df.to_dask_array(lengths=True).reshape((-1, num_fields)).compute()
 
     elif headers['DATA'] == 'binary':
         with open(pcd_path, 'rb') as f:
             for _ in range(headers['data_start']):
                 _ = f.readline()
-            dtype_list = [(name, type_size_map[(field_type, size)]) for name, field_type, size in
-                          zip(headers["FIELDS"], headers['TYPE'], headers["SIZE"])]
-            dt = np.dtype(dtype_list)
             data = np.fromfile(f, dtype=dt)
 
         names = headers["FIELDS"]
         counter_dict = {}
         new_names = []
         for i, el in enumerate(names):
             if names.count(el) > 1:
@@ -73,20 +72,40 @@
                 new_names.append(el + str(counter_dict[el]))
             else:
                 new_names.append(el)
 
         for old_name, new_name in zip(names, new_names):
             data.dtype.names = [name.replace(old_name, new_name) for name in data.dtype.names]
 
-        pc_points = np.zeros((headers['POINTS'], len(new_names)), dtype=np.float32)
+        pc_points = np.zeros((headers['POINTS'],), dtype=dt)
         for i, name in enumerate(data.dtype.names):
-            pc_points[:, i] = data[name]
+            pc_points[name] = data[name]
+        pc_points = np.array([list(item) for item in pc_points])
     elif headers['DATA'] == 'binary_compressed':
-        # TODO: binary_compressed
-        raise AbavaNotImplementException('Temporarily unable to read binary_compressed data.')
+        with open(pcd_path, 'rb') as f:
+            for _ in range(headers['data_start']):
+                _ = f.readline()
+
+            compressed_size = np.frombuffer(f.read(4), dtype=np.uint32)[0]
+            decompressed_size = np.frombuffer(f.read(4), dtype=np.uint32)[0]
+            compressed_data = f.read(compressed_size)
+
+            decompressed_data = lzf.decompress(compressed_data, decompressed_size)
+
+        pc_points = np.empty(int(headers['WIDTH'][0]), dtype=dt)
+
+        buffer = memoryview(decompressed_data)
+
+        for name in dt.names:
+            itemsize = dt.fields[name][0].itemsize
+            bytes = itemsize * int(headers['WIDTH'][0])
+            column = np.frombuffer(buffer[:bytes], dt.fields[name][0])
+            pc_points[name] = column
+            buffer = buffer[bytes:]
+
     else:
         raise 'Unknown pcd data type.'
 
     headers.pop('data_start')
     return pc_points, headers
 
 
@@ -119,16 +138,16 @@
              f'TYPE {" ".join(head["TYPE"])}\n' \
              f'COUNT {" ".join(head["COUNT"])}\n' \
              f'WIDTH {point_num}\n' \
              'HEIGHT 1\n' \
              'VIEWPOINT 0 0 0 1 0 0 0\n' \
              f'POINTS {point_num}\n' \
              f'DATA {data_mode}'
-    type_map = {('U', '1'): 'B', ('U', '2'): 'H', ('U', '4'): 'I',
-                ('F', '4'): 'f',
+    type_map = {('U', '1'): 'B', ('U', '2'): 'H', ('U', '4'): 'I', ('U', '8'): 'Q',
+                ('F', '4'): 'f', ('F', '8'): 'd',
                 ('I', '1'): 'c', ('I', '2'): 'h', ('I', '4'): 'i'}
 
     if data_mode == 'ascii':
         handle = open(out_path, 'w')
         handle.write(header)
         for point in points:
             str_points = [str(p) for p in point]
@@ -166,50 +185,41 @@
     :param bin_folder:
     :return:
     """
     data, headers = read_pcd(pcd_path)
     data.tofile(out_path)
 
 
-def bin2pcd(bin_path, pcd_path, head=None):
+def bin2pcd(bin_path, pcd_path, head=None, data_mode='asscii'):
     """
     Convert point cloud bin format to pcd format
     :param bin_path: bin file path
-    :param pcd_folder: pcd folder path
+    :param pcd_path: pcd file path
     :param head: {
         "FIELDS": ["x", "y", "z", "intensity"],
         "SIZE": ["4", "4", "4", "4"],
         "TYPE": ["F", "F", "F", "F"],
         "COUNT": ["1", "1", "1", "1"] }
     :return: pcd file
     """
-    try:
-        if not (os.path.isdir(pcd_path)):
-            os.makedirs(os.path.join(pcd_path))
-    except OSError as e:
-        raise
 
     if head is None:
         head = {
             "FIELDS": ["x", "y", "z", "intensity"],
             "SIZE": ["4", "4", "4", "4"],
             "TYPE": ["F", "F", "F", "F"],
             "COUNT": ["1", "1", "1", "1"]
         }
 
-    print("Converting Start!")
     points = np.fromfile(bin_path, dtype="float32").reshape((-1, len(head['FIELDS'])))
-    write_pcd(points, pcd_path, head)
+    write_pcd(points, pcd_path, head, data_mode)
 
 
 def pcd_ascii2binary(input_file, output_file):
-    start = time.time()
     point_data, headers = read_pcd(input_file)
-    end = time.time()
-    print('read time:', end - start)
     head = {
         "FIELDS": headers['FIELDS'],
         "SIZE": headers['SIZE'],
         "TYPE": headers['TYPE'],
         "COUNT": headers['COUNT']
     }
     write_pcd(point_data, output_file, head, data_mode='binary')
@@ -257,57 +267,70 @@
         mask = np.all((box_pcd_np >= -0.5 * box_size) & (box_pcd_np <= 0.5 * box_size), axis=1)
         xyz_point = xyz_point[~mask]
         intensity_point = intensity_point[~mask]
     filtered_point = np.concatenate([xyz_point, intensity_point.reshape(-1, 1)], axis=1)
     return filtered_point
 
 
-def voxel_subsample_keep_intensity(pcd_path, intensity, voxel_size, output_path='./subsampled.pcd'):
+def voxel_subsample(pcd_path, voxel_size, intensity=None, output_path='./subsampled.pcd'):
     """
     Retain points within the intensity information threshold and downsample the remaining points based on voxels
     :param pcd_path: pcd format point cloud path
     :param intensity: intensity range (list) example: [20, 200]
     :param voxel_size: voxel size
     :param output_path: Point cloud file save path
     :return:
     """
     pc_points, headers = read_pcd(pcd_path)[:, :4]
     # We default the first four columns of the point cloud to x, y, z, intensity
-    points_intensity = pc_points[(pc_points[:, 3] >= intensity[0]) & (pc_points[:, 3] <= intensity[1])]
-    points_other = pc_points[(pc_points[:, 3] < intensity[0]) | (pc_points[:, 3] > intensity[1])]
-    voxel_coords = np.floor(points_other[:, 0:3] / voxel_size).astype(np.int32)
-    voxel_indices = np.unique(voxel_coords, axis=0, return_index=True)[1]
-    downsampled_points_other = points_other[voxel_indices]
-    final_points = np.concatenate((points_intensity, downsampled_points_other), axis=0)
+    if intensity:
+        points_intensity = pc_points[(pc_points[:, 3] >= intensity[0]) & (pc_points[:, 3] <= intensity[1])]
+        points_other = pc_points[(pc_points[:, 3] < intensity[0]) | (pc_points[:, 3] > intensity[1])]
+        voxel_coords = np.floor(points_other[:, 0:3] / voxel_size).astype(np.int32)
+        voxel_indices = np.unique(voxel_coords, axis=0, return_index=True)[1]
+        downsampled_points_other = points_other[voxel_indices]
+        final_points = np.concatenate((points_intensity, downsampled_points_other), axis=0)
+    else:
+        voxel_coords = np.floor(pc_points[:, 0:3] / voxel_size).astype(np.int32)
+        voxel_indices = np.unique(voxel_coords, axis=0, return_index=True)[1]
+        final_points = pc_points[voxel_indices]
+
     head = {
         "FIELDS": headers['FIELDS'],
         "SIZE": headers['SIZE'],
         "TYPE": headers['TYPE'],
         "COUNT": headers['COUNT']
     }
     write_pcd(final_points, output_path, head)
 
 
-def random_subsample_keep_intensity(pcd_path, intensity, sampling_ratio, output_path='./subsampled.pcd'):
+def random_subsample(pcd_path, sampling_ratio, intensity, output_path='./subsampled.pcd'):
     """
     Retain points within the intensity information threshold and randomly downsample the remaining points
     :param pcd_path: pcd format point cloud path
     :param intensity: intensity range (list) example: [20, 200]
     :param sampling_ratio: downsampling rate
     :param output_path: Point cloud file save path
     :return:
     """
     pc_points, headers = read_pcd(pcd_path)[:, :4]
-    points_intensity_20_200 = pc_points[(pc_points[:, 3] >= intensity[0]) & (pc_points[:, 3] <= intensity[1])]
-    points_other = pc_points[(pc_points[:, 3] < intensity[0]) | (pc_points[:, 3] > intensity[1])]
-    num_points = points_other.shape[0]
-    num_sampled_points = int(sampling_ratio * num_points)
-    sampled_indices = np.random.choice(num_points, num_sampled_points, replace=False)
-    downsampled_points_other = points_other[sampled_indices]
-    final_points = np.concatenate((points_intensity_20_200, downsampled_points_other), axis=0)
+    if intensity:
+        points_intensity_20_200 = pc_points[(pc_points[:, 3] >= intensity[0]) & (pc_points[:, 3] <= intensity[1])]
+        points_other = pc_points[(pc_points[:, 3] < intensity[0]) | (pc_points[:, 3] > intensity[1])]
+        num_points = points_other.shape[0]
+        num_sampled_points = int(sampling_ratio * num_points)
+        sampled_indices = np.random.choice(num_points, num_sampled_points, replace=False)
+        downsampled_points_other = points_other[sampled_indices]
+        final_points = np.concatenate((points_intensity_20_200, downsampled_points_other), axis=0)
+    else:
+        num_points = pc_points.shape[0]
+        num_sampled_points = int(sampling_ratio * num_points)
+        sampled_indices = np.random.choice(num_points, num_sampled_points, replace=False)
+        final_points = pc_points[sampled_indices]
+
     head = {
         "FIELDS": headers['FIELDS'],
         "SIZE": headers['SIZE'],
         "TYPE": headers['TYPE'],
         "COUNT": headers['COUNT']
     }
     write_pcd(final_points, output_path, head)
```

## Comparing `Abava_SDK-1.0.6.dist-info/LICENSE` & `Abava_SDK-1.0.7.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `Abava_SDK-1.0.6.dist-info/METADATA` & `Abava_SDK-1.0.7.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: Abava-SDK
-Version: 1.0.6
+Version: 1.0.7
 Summary: abavaSDK
 Author: Xinjun Wu
 Author-email: wxj@molardata.com
 License-File: LICENSE
 Requires-Dist: lxml (>=4.9)
 Requires-Dist: numpy (>=1.25)
 Requires-Dist: opencv-python (>=4.8)
```

## Comparing `Abava_SDK-1.0.6.dist-info/RECORD` & `Abava_SDK-1.0.7.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-abava/__init__.py,sha256=w1Lm-Tm6UJzBc_axEIM2wr1HZRKlpU4Jrk0uqeJMzeo,6638
+abava/__init__.py,sha256=JsvcCjYOLFxhTpbk1yuJG8ZgQbQXM2D_7Pm2KMpfG4Q,6677
 abava/_version.py,sha256=9rJFz5NlwLt7-tzawXL5yzbycfzraYusTt6SLFh38Rs,76
 abava/abava_data.py,sha256=DQDyFhD_AjPtNv5YmDxM58ffTUmrZtQQczsb6C0jMW0,1851
 abava/api.py,sha256=NiMQ-h4p8jUchYfooYeZk6xcRGUPh7yxBHvm7wRKArE,837
 abava/const.py,sha256=HPMCwarZbfvjIAZjdQtWefDQCMwyJ4LCuOIGd7L5w_0,84
 abava/exception.py,sha256=8TcIE768uQcyg0u-lk2FZikj2b0bnMI7Squ75vJrH-k,1014
 abava/check/__init__.py,sha256=2qGsgv_Prl8DDPxxx5pRszYkhwl8t1p0JELdzZCiATE,23
 abava/check/check.py,sha256=rzhYEzfN7t7N_HHXKep_ezCXWoeTY0y6hNEq7isyBO0,385
@@ -25,31 +25,32 @@
 abava/export_format/voc/voc.py,sha256=868EUS38OGofHKZJxtSmgkxk2VENySnC2X7gF2j41D0,2080
 abava/export_format/yolo/__init__.py,sha256=2un8k8GFsH9dCD4_ZThEPCMZq_JL335e0Wzkk034rmo,48
 abava/export_format/yolo/export_yolo.py,sha256=JEhqrf4KTPO4z0bUmcvUvHl6-SDM8hsbXqoPL21fejM,4052
 abava/export_format/yolo/yolo.py,sha256=9de6p73ujHlKiSud3N0dd0iPj3tkOVVeDn6naKwR6as,789
 abava/factory/__init__.py,sha256=2un8k8GFsH9dCD4_ZThEPCMZq_JL335e0Wzkk034rmo,48
 abava/factory/data_factory.py,sha256=NIQ5SO5L250O1Il5FhOgOHcafaRPO2-D1GuKGvv309w,5517
 abava/post_process/__init__.py,sha256=61InPZl9jo5lBvTxEdD1Bug8B0X3vux9XblTG85C6Uo,50
-abava/post_process/process.py,sha256=FLMtjyx4l_0d9kIJz_a28GBB23oS7L-8t1xN6JCHJo0,1654
+abava/post_process/process.py,sha256=rOORn5AogDAqE2xa8AcXZ6tE4jO3b41gGRJ34KctCB4,1629
 abava/post_process/coco/__init__.py,sha256=TX_hVWaL7ZlEJDTSdLgKGojm5eLfDXtqlwnK08ShfxU,52
-abava/post_process/coco/coco_postprocess.py,sha256=v6QtFvuk91TQ8UqGZZy_lqh87FIyAcerJ9k_D2DuA3Y,4671
+abava/post_process/coco/coco_postprocess.py,sha256=McF8Sl0RumwTbCa-6fq6jIbjw_p5fndLIiOLsDmOqFw,4743
+abava/post_process/coco/test.py,sha256=O4UVeQUQYJU1uvKVGGh2xzHrgUxZmZG6l63DksN1EwE,3648
 abava/utils/__init__.py,sha256=29mYzkYNGSWT6J-GNrinnui1ZOh0ch8Q4mFIfsEq-V4,72
 abava/utils/cv_tools.py,sha256=iMqCwdEjj4MvaWcB4irlNqTFRt_qR8RfvKp82TFYvV8,23727
 abava/utils/general.py,sha256=PrNfQpqar4TgeBrSsCNV_kyhrfFhlnxET0Rri-N9GCs,6088
-abava/utils/pc_tools.py,sha256=VAT0UqbM1pd9Cm8k_zRV8xrzDyHDF58DHHzVWLiP7J8,18084
+abava/utils/pc_tools.py,sha256=OwQS6DssNEvI8QMGKUpUkYLtkddPD7DbhDc8odQzsT8,19049
 abava/visualize/__init__.py,sha256=Z0muVjdGwM_2xMkm3bE_P-qEN0awBcb9tXk9AyGJGnU,21
 abava/visualize/visual.py,sha256=SVO4h15MHOihJjiiOO1JrvfG22wO9ftfz9gMbJfW_vs,4649
 abava/visualize/coco/__init__.py,sha256=2un8k8GFsH9dCD4_ZThEPCMZq_JL335e0Wzkk034rmo,48
 abava/visualize/coco/visual_coco.py,sha256=XGamts9GIZR0I_208nXaJIxLibmcE2pmOjBUR8dJApw,1864
 abava/visualize/labelme/__init__.py,sha256=2un8k8GFsH9dCD4_ZThEPCMZq_JL335e0Wzkk034rmo,48
 abava/visualize/labelme/visual_labelme.py,sha256=tQOWLR0l2Qb222xhOqH4wmlTN26k3DPjhyTa1NID4JQ,2343
 abava/visualize/source/__init__.py,sha256=2un8k8GFsH9dCD4_ZThEPCMZq_JL335e0Wzkk034rmo,48
 abava/visualize/source/visual_source.py,sha256=2YMknJbTBaVYn25rbplcHE-TIcqVlL28ZrptUcggL30,2691
 abava/visualize/voc/__init__.py,sha256=2un8k8GFsH9dCD4_ZThEPCMZq_JL335e0Wzkk034rmo,48
 abava/visualize/voc/visual_voc.py,sha256=Hsp92kk--tSrVIcTDIR7VV-mEDvn8tyG_FMjEWBVg9k,3035
 abava/visualize/yolo/__init__.py,sha256=2un8k8GFsH9dCD4_ZThEPCMZq_JL335e0Wzkk034rmo,48
 abava/visualize/yolo/visual_yolo.py,sha256=nkrSOnQc0elJCDZUo2gJy_dVg6_6UxzkntQCoL3y9Ts,2213
-Abava_SDK-1.0.6.dist-info/LICENSE,sha256=Jwv3fsn5Tp_DrJRAfUpZCprWtVOZWtWa7IQ2WMuJMZs,1079
-Abava_SDK-1.0.6.dist-info/METADATA,sha256=CFQqKfqC47JJY3SAuZh9oZ5nLbX14MoEKWmwdXYCL5o,2059
-Abava_SDK-1.0.6.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-Abava_SDK-1.0.6.dist-info/top_level.txt,sha256=TnxFZLu-ZhQc4XpH3847ScoCc7Pzx96PZJk2EIcP6bA,6
-Abava_SDK-1.0.6.dist-info/RECORD,,
+Abava_SDK-1.0.7.dist-info/LICENSE,sha256=Jwv3fsn5Tp_DrJRAfUpZCprWtVOZWtWa7IQ2WMuJMZs,1079
+Abava_SDK-1.0.7.dist-info/METADATA,sha256=FLZAvFvd-UC8qhOQG0-7RGMU3yQgLS7HPH-rCPIpNHo,2059
+Abava_SDK-1.0.7.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+Abava_SDK-1.0.7.dist-info/top_level.txt,sha256=TnxFZLu-ZhQc4XpH3847ScoCc7Pzx96PZJk2EIcP6bA,6
+Abava_SDK-1.0.7.dist-info/RECORD,,
```

