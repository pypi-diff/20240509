# Comparing `tmp/abnativ-1.1.1-py3-none-any.whl.zip` & `tmp/abnativ-1.1.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,13 +1,13 @@
-Zip file size: 503321 bytes, number of entries: 42
+Zip file size: 503283 bytes, number of entries: 42
 -rw-r--r--  2.0 unx       75 b- defN 80-Jan-01 00:00 abnativ/__init__.py
 -rw-r--r--  2.0 unx    11743 b- defN 80-Jan-01 00:00 abnativ/__main__.py
 -rw-r--r--  2.0 unx       74 b- defN 80-Jan-01 00:00 abnativ/humanisation/__init__.py
 -rw-r--r--  2.0 unx     8451 b- defN 80-Jan-01 00:00 abnativ/humanisation/dms_utils.py
--rw-r--r--  2.0 unx    66934 b- defN 80-Jan-01 00:00 abnativ/humanisation/humanisation_utils.py
+-rw-r--r--  2.0 unx    60893 b- defN 80-Jan-01 00:00 abnativ/humanisation/humanisation_utils.py
 -rw-r--r--  2.0 unx    25160 b- defN 80-Jan-01 00:00 abnativ/humanisation/pssms/VHH_log2_pssm.npy
 -rw-r--r--  2.0 unx    25160 b- defN 80-Jan-01 00:00 abnativ/humanisation/pssms/VHH_pssm.npy
 -rw-r--r--  2.0 unx    25160 b- defN 80-Jan-01 00:00 abnativ/humanisation/pssms/VH_log2_pssm.npy
 -rw-r--r--  2.0 unx    25160 b- defN 80-Jan-01 00:00 abnativ/humanisation/pssms/VH_pssm.npy
 -rw-r--r--  2.0 unx    25160 b- defN 80-Jan-01 00:00 abnativ/humanisation/pssms/VKappa_log2_pssm.npy
 -rw-r--r--  2.0 unx    25160 b- defN 80-Jan-01 00:00 abnativ/humanisation/pssms/VKappa_pssm.npy
 -rw-r--r--  2.0 unx    25160 b- defN 80-Jan-01 00:00 abnativ/humanisation/pssms/VLambda_log2_pssm.npy
@@ -19,26 +19,26 @@
 -rw-r--r--  2.0 unx       74 b- defN 80-Jan-01 00:00 abnativ/model/alignment/__init__.py
 -rw-r--r--  2.0 unx     9654 b- defN 80-Jan-01 00:00 abnativ/model/alignment/aho_consensus.py
 -rwxr-xr-x  2.0 unx     7260 b- defN 80-Jan-01 00:00 abnativ/model/alignment/align_and_clean.py
 -rw-r--r--  2.0 unx    20256 b- defN 80-Jan-01 00:00 abnativ/model/alignment/blossum.py
 -rw-r--r--  2.0 unx   217705 b- defN 80-Jan-01 00:00 abnativ/model/alignment/csv_dict.py
 -rw-r--r--  2.0 unx     8077 b- defN 80-Jan-01 00:00 abnativ/model/alignment/liabilities.py
 -rw-r--r--  2.0 unx   282087 b- defN 80-Jan-01 00:00 abnativ/model/alignment/misc.py
--rw-r--r--  2.0 unx   250781 b- defN 80-Jan-01 00:00 abnativ/model/alignment/mybio.py
+-rw-r--r--  2.0 unx   250169 b- defN 80-Jan-01 00:00 abnativ/model/alignment/mybio.py
 -rw-r--r--  2.0 unx    15487 b- defN 80-Jan-01 00:00 abnativ/model/alignment/parse_pdb.py
 -rw-r--r--  2.0 unx   451002 b- defN 80-Jan-01 00:00 abnativ/model/alignment/plotter.py
 -rw-r--r--  2.0 unx   150143 b- defN 80-Jan-01 00:00 abnativ/model/alignment/structs.py
 -rwxr-xr-x  2.0 unx     4587 b- defN 80-Jan-01 00:00 abnativ/model/onehotencoder.py
 -rwxr-xr-x  2.0 unx    20424 b- defN 80-Jan-01 00:00 abnativ/model/scoring_functions.py
 -rw-r--r--  2.0 unx     7285 b- defN 80-Jan-01 00:00 abnativ/model/utils.py
 -rw-r--r--  2.0 unx     9812 b- defN 80-Jan-01 00:00 abnativ/model/vq.py
--rw-r--r--  2.0 unx     1490 b- defN 80-Jan-01 00:00 abnativ/scoring.py
+-rw-r--r--  2.0 unx     1488 b- defN 80-Jan-01 00:00 abnativ/scoring.py
 -rw-r--r--  2.0 unx     2845 b- defN 80-Jan-01 00:00 abnativ/training.py
 -rw-r--r--  2.0 unx     1961 b- defN 80-Jan-01 00:00 abnativ/update.py
 -rw-r--r--  2.0 unx      745 b- defN 80-Jan-01 00:00 abnativ/vh_vl_humanisation.py
 -rw-r--r--  2.0 unx     1778 b- defN 80-Jan-01 00:00 abnativ/vhh_humanisation.py
--rw-r--r--  2.0 unx    20971 b- defN 80-Jan-01 00:00 abnativ-1.1.1.dist-info/LICENSE
--rw-r--r--  2.0 unx    23599 b- defN 80-Jan-01 00:00 abnativ-1.1.1.dist-info/METADATA
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 abnativ-1.1.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       49 b- defN 80-Jan-01 00:00 abnativ-1.1.1.dist-info/entry_points.txt
-?rw-r--r--  2.0 unx     3761 b- defN 16-Jan-01 00:00 abnativ-1.1.1.dist-info/RECORD
-42 files, 1828884 bytes uncompressed, 497291 bytes compressed:  72.8%
+-rw-r--r--  2.0 unx    20971 b- defN 80-Jan-01 00:00 abnativ-1.1.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx    24117 b- defN 80-Jan-01 00:00 abnativ-1.1.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 abnativ-1.1.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx       49 b- defN 80-Jan-01 00:00 abnativ-1.1.3.dist-info/entry_points.txt
+?rw-r--r--  2.0 unx     3761 b- defN 16-Jan-01 00:00 abnativ-1.1.3.dist-info/RECORD
+42 files, 1822747 bytes uncompressed, 497253 bytes compressed:  72.7%
```

## zipnote {}

```diff
@@ -105,23 +105,23 @@
 
 Filename: abnativ/vh_vl_humanisation.py
 Comment: 
 
 Filename: abnativ/vhh_humanisation.py
 Comment: 
 
-Filename: abnativ-1.1.1.dist-info/LICENSE
+Filename: abnativ-1.1.3.dist-info/LICENSE
 Comment: 
 
-Filename: abnativ-1.1.1.dist-info/METADATA
+Filename: abnativ-1.1.3.dist-info/METADATA
 Comment: 
 
-Filename: abnativ-1.1.1.dist-info/WHEEL
+Filename: abnativ-1.1.3.dist-info/WHEEL
 Comment: 
 
-Filename: abnativ-1.1.1.dist-info/entry_points.txt
+Filename: abnativ-1.1.3.dist-info/entry_points.txt
 Comment: 
 
-Filename: abnativ-1.1.1.dist-info/RECORD
+Filename: abnativ-1.1.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## abnativ/humanisation/humanisation_utils.py

```diff
@@ -1,22 +1,15 @@
 # (c) 2023 Sormannilab and Aubin Ramon
-#     CC BY-NC-SA 4.0
 #
 # Util functions for humanisation using the AbNatiV model.
 #
 # ============================================================================
 
 from ..model.alignment.mybio import print_Alignment_pap
-from ..model.scoring_functions import (
-    abnativ_scoring,
-    cdr1_aho_indices,
-    cdr2_aho_indices,
-    cdr3_aho_indices,
-    fr_aho_indices,
-)
+from ..model.scoring_functions import abnativ_scoring, cdr1_aho_indices, cdr2_aho_indices, cdr3_aho_indices, fr_aho_indices
 from ..model.alignment.structs import Gly_X_Gly_sasa_standard_radiiMax, Term_resMax
 from ..model.alignment.mybio import ThreeToOne, renumber_Fv_pdb_file
 from ..model.onehotencoder import alphabet
 
 from .dms_utils import compute_dms_map
 
 from pkg_resources import resource_filename
@@ -39,310 +32,265 @@
 from Bio.PDB.SASA import ShrakeRupley
 
 from ImmuneBuilder import NanoBodyBuilder2, ABodyBuilder2
 
 from topmodel.check import chirality, clashes, amide_bond
 from topmodel.util.utils import ChiralCenters, AmideBonds, Clashes, CoupleIrregularity
 
-
 def check_predicted_structure(fp_pdb: str) -> Tuple[bool, list, list]:
-    """
+    '''
     Use the TopModel method to check structual liabilties in a generated model:
         - WdV clashes
         - Wrong dihedral angles
         - D-Stereoimetry
-
-    If a structure has either a WdV clash or a D-residue it will be flagged as unreliable.
+    
+    If a structure has either a WdV clash or a D-residue it will be flagged as unreliable. 
 
     Parameters
     ----------
         - fp_pdb : str
             Filepath to the structure
 
     Returns
     -------
         - is_reliable : bool
             True if reliable, False if a WdV clash or a D-residue
         - list of clashes
         - list of D-residues
+    
+    '''
 
-    """
-
-    struct = PDBParser().get_structure("WT_framework", fp_pdb)[0]
-
+    struct = PDBParser().get_structure('WT_framework', fp_pdb)[0]
+    
     ### WdV Clashes ###
     found_clashes = clashes.get_clashes(struct)
-    wdv_clashes = found_clashes[Clashes.VDW]
-
+    wdv_clashes = found_clashes[Clashes.VDW] 
+    
     # Remove disulfide bonds from clashes
     no_cc_wdv_clashes = list()
     for clash in wdv_clashes:
         if isinstance(clash, CoupleIrregularity):
             res_a_name = clash.res_a.code
             a_numb = clash.res_a.number
             res_b_name = clash.res_b.code
             b_numb = clash.res_b.number
-            if res_a_name == "C" and res_b_name == "C":
+            if res_a_name=='C' and res_b_name=='C':
                 continue
         no_cc_wdv_clashes.append(clash)
 
     ### Wrong dihedral angles (amide bond) ###
     found_amides = amide_bond.get_amide_stereo(struct)
-    cis = found_amides[AmideBonds.CIS]
+    cis =  found_amides[AmideBonds.CIS]
     cis_pro = found_amides[AmideBonds.CIS_PROLINE]
     non_plan = found_amides[AmideBonds.NON_PLANAR]
 
     ### D-Chiralities ###
     found_chiralities = chirality.get_chirality(struct)
     chiral_d = found_chiralities[ChiralCenters.D]
 
-    if len(no_cc_wdv_clashes) > 0 or len(chiral_d) > 0:
+    if len(no_cc_wdv_clashes)>0 or len(chiral_d)>0:
         is_reliable = False
     else:
         is_reliable = True
 
     return is_reliable, no_cc_wdv_clashes, chiral_d
 
 
-def predict_struct_vhh(
-    seq: str,
-    name_seq: str,
-    o_dir: str,
-    scheme: str = "AHo",
-    model_ids=[1, 2, 3, 4],
-    max_check_iter=10,
-) -> Tuple[str, str, bool, list, list, float]:
-    """Predict the structure of a nanobody sequence using the NanobodyBuilder2 software.
+def predict_struct_vhh(seq: str, name_seq: str, o_dir: str, scheme: str='AHo',
+                        model_ids=[1,2,3,4], max_check_iter=10) -> Tuple[str,str, bool, list, list, float]:
+    '''Predict the structure of a nanobody sequence using the NanobodyBuilder2 software.
 
     Parameters
     ----------
         - seq : str
-            Sequence string
-        - name seq: str
+            Sequence string 
+        - name seq: str 
         - o_dir: str
-        - scheme:
+        - scheme: 
             Alignement numbering used to annotate the residues in the prediction
         - model_ids: list of int
             The id of the model to use with ImmuneBuilder
         - max_check_iter: int
-            The maximun number of iterations to generate a structure that does not
+            The maximun number of iterations to generate a structure that does not 
             show any major liabilities (WdV clashes or D-stereoisomers)
 
     Returns
     -------
         - filepath to the predicted nanobody structure .pdb
-        - chain id
+        - chain id 
         - struct_quality_flag
         - list of no_cc_wdv_clashes
         - list of chiral_d
         - and error estimate profile from ImmuneBuilder
-    """
+    '''
+
+    
 
-    seq_dict = {"H": seq}
+    seq_dict = {'H': seq}
 
     struct_quality_flag = False
     count_check = 0
 
-    while struct_quality_flag is False and count_check < max_check_iter:
+    while struct_quality_flag is False and count_check<max_check_iter:
         predictor = NanoBodyBuilder2(model_ids=model_ids, numbering_scheme=scheme)
-
+        
         # NanoBodyBuilder2 structure prediction
         nanobody = predictor.predict(seq_dict)
-        aho_pdb_file = os.path.join(o_dir, f"{name_seq}_aho_nanobuilder.pdb")
+        aho_pdb_file = os.path.join(o_dir, f'{name_seq}_aho_nanobuilder.pdb')
         nanobody.save(aho_pdb_file)
-        ch_id = "H"
-
+        ch_id='H'
+        
         index_best_model = nanobody.error_estimates.mean(-1).argmin()
-        error_estimate_profile = np.array(nanobody.error_estimates[index_best_model])
+        error_estimate_profile = np.array(nanobody.error_estimates[index_best_model].cpu())
 
         # Check quality of the model with TopModel
-        struct_quality_flag, no_cc_wdv_clashes, chiral_d = check_predicted_structure(
-            aho_pdb_file
-        )
+        struct_quality_flag, no_cc_wdv_clashes, chiral_d = check_predicted_structure(aho_pdb_file)
         count_check += 1
 
-    if struct_quality_flag == False and count_check == max_check_iter:
-        print(
-            f"\n ATTENTION: Could not generate a structure for {name_seq} without liabilities (i.e., VdW clashes or D-stereoisomers)"
-        )
-        print(f"----> VdW clashes: {no_cc_wdv_clashes}")
-        print(f"----> D-stereo: {chiral_d}")
-
-    return (
-        aho_pdb_file,
-        ch_id,
-        struct_quality_flag,
-        no_cc_wdv_clashes,
-        chiral_d,
-        error_estimate_profile,
-    )
-
-
-def predict_struct_vh_vl(
-    vh_seq: str,
-    vl_seq: str,
-    name_seq: str,
-    o_dir: str,
-    scheme: str = "AHo",
-    model_ids=[1, 2, 3, 4],
-    max_check_iter=10,
-) -> Tuple[str, str, str, bool, list, list, float]:
-    """Predict the structure of a paired VH/VL antibody using the ABodyBuilder2 software
+    if struct_quality_flag == False and count_check==max_check_iter:
+        print(f'\n ATTENTION: Could not generate a structure for {name_seq} without liabilities (i.e., VdW clashes or D-stereoisomers)')
+        print(f'----> VdW clashes: {no_cc_wdv_clashes}')
+        print(f'----> D-stereo: {chiral_d}')
+
+    return aho_pdb_file, ch_id, struct_quality_flag, no_cc_wdv_clashes, chiral_d, error_estimate_profile
+
+def predict_struct_vh_vl(vh_seq: str, vl_seq: str, name_seq: str, o_dir: str, 
+                         scheme: str='AHo', model_ids=[1,2,3,4], max_check_iter=10) -> Tuple[str, str, str, bool, list, list, float]:
+    '''Predict the structure of a paired VH/VL antibody using the ABodyBuilder2 software
 
     Parameters
     ----------
         - vh_seq: str
             Sequence string of the Heavy chain of the paired Fvs
-        - vl_seq: str
+        - vl_seq: str 
             Sequence string of the Light chain of the paired Fvs
-        - name seq: str
+        - name seq: str 
         - o_dir: str
-        - scheme:
+        - scheme: 
             Alignement numbering used to annotate the residues in the prediction
         - model_ids: list of int
             The id of the model to use with ImmuneBuilder
         - max_check_iter: int
-            The maximun number of iterations to generate a structure that does not
+            The maximun number of iterations to generate a structure that does not 
             show any major liabilities (WdV clashes or D-stereoisomers)
 
     Returns
     -------
         - filepath to the predicted nanobody structure .pdb
         - id of heavy chain
         - id of light chain
         - struct_quality_flag
         - list of no_cc_wdv_clashes
         - list of chiral_d
         - and error estimate profile from ImmuneBuilder
-    """
+    '''
+
+    
 
-    seq_dict = {"H": vh_seq, "L": vl_seq}
+    seq_dict = {'H': vh_seq, 'L': vl_seq}
 
     struct_quality_flag = False
     count_check = 0
 
-    while struct_quality_flag is False and count_check < max_check_iter:
+    while struct_quality_flag is False and count_check<max_check_iter:
 
-        # NanoBodyBuilder2 structure prediction
-        predictor = ABodyBuilder2(model_ids=model_ids, numbering_scheme=scheme)
+        # ImmuneBuilder2 structure prediction
+        predictor = ABodyBuilder2(model_ids=model_ids,numbering_scheme=scheme)
 
-        nanobody = predictor.predict(seq_dict)
-        aho_pdb_file = os.path.join(o_dir, f"{name_seq}_aho_abodybuilder.pdb")
-        nanobody.save(aho_pdb_file)
-        ch_id_vh = "H"
-        ch_id_vl = "L"
+        antibody = predictor.predict(seq_dict)
+        aho_pdb_file = os.path.join(o_dir, f'{name_seq}_aho_abodybuilder.pdb')
+        antibody.save(aho_pdb_file)
+        ch_id_vh='H'
+        ch_id_vl='L'
 
-        index_best_model = nanobody.error_estimates.mean(-1).argmin()
-        error_estimate_profile = np.array(nanobody.error_estimates[index_best_model])
+        index_best_model = antibody.error_estimates.mean(-1).argmin()
+        error_estimate_profile = np.array(antibody.error_estimates[index_best_model].cpu())
 
         # Check quality of the model with TopModel
-        struct_quality_flag, no_cc_wdv_clashes, chiral_d = check_predicted_structure(
-            aho_pdb_file
-        )
+        struct_quality_flag, no_cc_wdv_clashes, chiral_d = check_predicted_structure(aho_pdb_file)
         count_check += 1
 
-    if struct_quality_flag == False and count_check == max_check_iter:
-        print(
-            f"\n ATTENTION: Could not generate a structure for {name_seq} without liabilities (i.e., VdW clashes or D-stereoisomers)"
-        )
-        print(f"----> VdW clashes: {no_cc_wdv_clashes}")
-        print(f"----> D-stereo: {chiral_d}")
-
-    return (
-        aho_pdb_file,
-        ch_id_vh,
-        ch_id_vl,
-        struct_quality_flag,
-        no_cc_wdv_clashes,
-        chiral_d,
-        error_estimate_profile,
-    )
-
-
-def print_chimera_mutations_code(
-    wt_seq: str, hum_seq: str, pdb_file: str, ch_id: str, output_dir: str, name_seq: str
-) -> Tuple[list, list]:
-    """
-    Print the chimera code for visualisation in .txt files. See Returns for more details.
+    if struct_quality_flag == False and count_check==max_check_iter:
+        print(f'\n ATTENTION: Could not generate a structure for {name_seq} without liabilities (i.e., VdW clashes or D-stereoisomers)')
+        print(f'----> VdW clashes: {no_cc_wdv_clashes}')
+        print(f'----> D-stereo: {chiral_d}')
+
+    return aho_pdb_file, ch_id_vh, ch_id_vl, struct_quality_flag, no_cc_wdv_clashes, chiral_d, error_estimate_profile
+
+
+def print_chimera_mutations_code(wt_seq: str, hum_seq: str, pdb_file: str, ch_id: str, 
+                                 output_dir: str, name_seq: str) -> Tuple[list,list]:
+    '''
+    Print the chimera code for visualisation in .txt files. See Returns for more details. 
 
     Parameters
     ----------
         - wt_seq: str
             WT sequence
-        - hum_seq: str
+        - hum_seq: str 
             Humanised sequence with mutations
         - pdb_file: str
-            Filepath to the pdb file to write the visualition code on.
+            Filepath to the pdb file to write the visualition code on. 
         - ch_id: str
         - o_dir: str
         - name_seq: str
 
     Returns
     -------
         - chimeratxt: list of str
             Write  the chimera code as follows 'A21.V' for an alanine at PDB position 21
             of the chain V, useful for CamSol combination
         - chimeraXtxt: list of str
-            Write chimeraX code as follows '/A:21' fon a residue at PDB position 21 which has
-            been mutated, useful for visualisation"""
+            Write chimeraX code as follows '/A:21' fon a residue at PDB position 21 which has 
+            been mutated, useful for visualisation'''
 
-    chimera_output_dir = os.path.join(output_dir, "chimera_codes")
+    chimera_output_dir = os.path.join(output_dir, 'chimera_codes')
 
     if not os.path.exists(chimera_output_dir):
         os.makedirs(chimera_output_dir)
 
     # Remove alignement if there is
-    wt_seq = wt_seq.replace("-", "")
-    hum_seq = hum_seq.replace("-", "")
+    wt_seq = wt_seq.replace('-','')
+    hum_seq = hum_seq.replace('-','')
 
     # Compare sequence with pdb sequence to extract their PDB numbering
-    chimera_codes, chimerax_codes = list(), list()
+    chimera_codes,chimerax_codes = list(), list()
     parser = PDBParser()
-    ab_chain = parser.get_structure("struct", pdb_file)[0][ch_id]
+    ab_chain = parser.get_structure('struct', pdb_file)[0][ch_id]
 
     for k, res in enumerate(ab_chain):
         resname = res.resname
         id = res.id[1]
 
-        if resname == "HOH":
-            continue
+        if resname == 'HOH': continue 
 
         resletter = ThreeToOne[resname]
 
         if resletter != wt_seq[k]:
-            raise EOFError("PDB seq does not match WT seq, BE CAREFUL")
-
-        if wt_seq[k] != hum_seq[k]:
-            chimera_codes.append(resletter + str(id) + "." + ch_id)
+            raise EOFError('PDB seq does not match WT seq, BE CAREFUL')
+        
+        if wt_seq[k]!=hum_seq[k]:
+            chimera_codes.append(resletter+str(id)+'.'+ch_id)
             chimerax_codes.append(str(id))
 
     # Save the chimera codes
-    chimera_txt = os.path.join(chimera_output_dir, f"{name_seq}_chimera_mut_codes.txt")
-    with open(chimera_txt, "w") as f:
-        f.write(",".join(chimera_codes))
-
-    chimerax_txt = os.path.join(
-        chimera_output_dir, f"{name_seq}_chimeraX_mut_codes.txt"
-    )
-    with open(chimerax_txt, "w") as f:
-        f.write("/" + ch_id + ":" + ",".join(chimerax_codes))
+    chimera_txt = os.path.join(chimera_output_dir, f'{name_seq}_chimera_mut_codes.txt')
+    with open(chimera_txt, 'w') as f: 
+        f.write(','.join(chimera_codes))
+
+    chimerax_txt = os.path.join(chimera_output_dir, f'{name_seq}_chimeraX_mut_codes.txt')
+    with open(chimerax_txt, 'w') as f: 
+        f.write('/'+ch_id+':'+','.join(chimerax_codes))
 
     return chimera_txt, chimerax_codes
 
-
-def rasa_selection_posi_to_humanise(
-    seq,
-    nat,
-    is_VHH,
-    fp_pdb_file: str = "pdb_file.pdb",
-    ch_id: str = "H",
-    threshold_rasa_score: float = 0.15,
-) -> list:
-    """Select the residues with a RASA score >= threshold_rasa_score.
-
+def rasa_selection_posi_to_humanise(seq, nat, is_VHH, fp_pdb_file: str='pdb_file.pdb', 
+                                    ch_id: str='H', threshold_rasa_score: float=0.15) -> list:
+    '''Select the residues with a RASA score >= threshold_rasa_score.
+    
     Parameters
     ----------
         - seq: str
             Unaligned string sequence
         - nat: str
             Type of AbNatiV nativeness to do the study on
         - is_VHH: bool
@@ -353,616 +301,401 @@
             Chain id of the protein in the PDB file
         - threshold_rasa_score: float
             Ratio of RASA to use as a cutoff to select residues. The higher the cutoff is
             the more solvent exposed will be the selected residues
 
     Returns
     -------
-        - List of the selected positions"""
+        - List of the selected positions'''
 
     selected_aho_positions = list()
 
-    # Compute AbNatiV profile
-    wt_vh_seq_abnativ_df, wt_vh_profile_abnativ_df = abnativ_scoring(
-        nat,
-        seq,
-        batch_size=1,
-        mean_score_only=False,
-        do_align=True,
-        is_VHH=is_VHH,
-        verbose=False,
-    )
-
-    ng_df_vh_profile = wt_vh_profile_abnativ_df.loc[
-        wt_vh_profile_abnativ_df["aa"] != "-"
-    ]
+    # Compute AbNatiV profile 
+    wt_vh_seq_abnativ_df, wt_vh_profile_abnativ_df = abnativ_scoring(nat, seq, batch_size=1,mean_score_only=False, 
+                                                                    do_align=True, is_VHH=is_VHH, verbose=False)
+
+    ng_df_vh_profile = wt_vh_profile_abnativ_df.loc[wt_vh_profile_abnativ_df['aa']!='-'] 
 
-    # Compute RASA
+    # Compute RASA 
     p = PDBParser(QUIET=1)
     sr = ShrakeRupley()
 
-    struct = p.get_structure(wt_vh_profile_abnativ_df["seq_id"][0], fp_pdb_file)
+    struct = p.get_structure(wt_vh_profile_abnativ_df['seq_id'][0], fp_pdb_file)
     sr.compute(struct, level="R")
 
     # Compute rasa of each res in the given pdb
     dict_rasa = dict()
     for res in struct[0][ch_id]:
         aho_id = res.id[1]
-        if res.resname == "HOH":
-            continue
-        if aho_id == 0 or aho_id == 149:
+        if res.resname == 'HOH': continue
+        if aho_id==0 or aho_id==149:
             norm_fact = Term_resMax[res.resname]
         else:
             norm_fact = Gly_X_Gly_sasa_standard_radiiMax[res.resname]
-        dict_rasa[aho_id] = (res.sasa / norm_fact, ThreeToOne[res.resname])
+        dict_rasa[aho_id] = (res.sasa/norm_fact, ThreeToOne[res.resname])
 
-    # RASA selection
-    for aho_posi in ng_df_vh_profile["AHo position"]:
-        abnativ_res = list(
-            ng_df_vh_profile[ng_df_vh_profile["AHo position"] == aho_posi]["aa"]
-        )[0]
+    # RASA selection 
+    for aho_posi in ng_df_vh_profile['AHo position']:
+        abnativ_res = list(ng_df_vh_profile[ng_df_vh_profile['AHo position']==aho_posi]['aa'])[0]
         pdb_res = dict_rasa[aho_posi][1]
 
         if aho_posi not in dict_rasa:
-            print(
-                "\n [ATTENTION] Given pdb structure is not complete, could not calculate the RASA of each residue.\n \
+            print('\n [ATTENTION] Given pdb structure is not complete, could not calculate the RASA of each residue.\n \
                   Missing residues were including by default. Please provide a complete structure or \
-                  use the structure prediction option.\n"
-            )
+                  use the structure prediction option.\n')    
         elif abnativ_res != pdb_res:
-            raise ValueError(
-                f"At AHo position {aho_posi}, the residue {abnativ_res} in al_wt_seq (aligned by AbNatiV) does not match the residue {pdb_res} \
-                             in the pdb file. Use the structure prediction option to avoid this problem"
-            )
+            raise ValueError(f'At AHo position {aho_posi}, the residue {abnativ_res} in al_wt_seq (aligned by AbNatiV) does not match the residue {pdb_res} \
+                             in the pdb file. Use the structure prediction option to avoid this problem')
         else:
-            if dict_rasa[aho_posi][0] >= threshold_rasa_score:
+            if dict_rasa[aho_posi][0]>=threshold_rasa_score:
                 # Framework selection
                 if aho_posi in fr_aho_indices:
                     selected_aho_positions.append(aho_posi)
 
     return selected_aho_positions
 
-
-def get_dict_pposi_allowed_muts(
-    low_frequency_cutoff: float,
-    nat_to_hum: str,
-    is_VHH: str,
-    forbidden_mut: list,
-    is_brute: bool,
-) -> dict:
-    """Returns a dict with keys will be AHo numbers, values will be allowed substitutions
+def get_dict_pposi_allowed_muts(low_frequency_cutoff:float, nat_to_hum: str, is_VHH: str, forbidden_mut: list, is_brute: bool) -> dict:
+    '''Returns a dict with keys will be AHo numbers, values will be allowed substitutions 
     according to the PSSM criteria (i.e., log>=0 and low_freq_cutoof) in both human V nat_to_hum and VHH (if is_VHH and is_brute)
-
+    
     Parameters
     ----------
         - low_frequency_cutoff: float
         - nat_to_hum: str
             Type of humanness to improve nativeness into (VH, VKappa, VLambda).
         - is_VHH: bool
             If True, considers the VHH seed for the alignment, more suitable when aligning nanobody sequences
         - forbidden_mut: list of strs
             All residues forbidden to mutate into
         - is_brute: bool
-            If False, only consider VH-PSSM allowed
+            If False, only consider VH-PSSM allowed 
             If True, also consider VHH-PSSM allowed mutation which is more restrictive (smaller space sample)
-
+            
     Returns
     -------
-        - dict with keys will be AHo numbers, values will be allowed substitutions
-            according to the PSSM criteria
-
-    """
+        - dict with keys will be AHo numbers, values will be allowed substitutions 
+            according to the PSSM criteria 
 
-    alphabet_pssm = np.array(
-        [
-            "-",
-            "A",
-            "C",
-            "D",
-            "E",
-            "F",
-            "G",
-            "H",
-            "I",
-            "K",
-            "L",
-            "M",
-            "N",
-            "P",
-            "Q",
-            "R",
-            "S",
-            "T",
-            "V",
-            "W",
-            "Y",
-        ]
-    )
+    '''
 
-    # Fetch PSSMs data to find pssm-allowed mutations at each position
+    alphabet_pssm = np.array(['-','A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 
+                              'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])
+    
+    #Fetch PSSMs data to find pssm-allowed mutations at each position
     pssms_dir = resource_filename(__name__, "pssms")
 
-    if nat_to_hum == "VH":
-        with open(os.path.join(pssms_dir, "VH_log2_pssm.npy"), "rb") as f:
-            hum_log2_pssm = np.load(f)
-        with open(os.path.join(pssms_dir, "VH_pssm.npy"), "rb") as f:
-            hum_pssm = np.load(f)
-
-    elif nat_to_hum == "VKappa":
-        with open(os.path.join(pssms_dir, "VKappa_log2_pssm.npy"), "rb") as f:
-            hum_log2_pssm = np.load(f)
-        with open(os.path.join(pssms_dir, "VKappa_pssm.npy"), "rb") as f:
-            hum_pssm = np.load(f)
-
-    elif nat_to_hum == "VLambda":
-        with open(os.path.join(pssms_dir, "VLambda_log2_pssm.npy"), "rb") as f:
-            hum_log2_pssm = np.load(f)
-        with open(os.path.join(pssms_dir, "VLambda_pssm.npy"), "rb") as f:
-            hum_pssm = np.load(f)
-
-    elif nat_to_hum == "VHH":
-        with open(os.path.join(pssms_dir, "VHH_log2_pssm.npy"), "rb") as f:
-            hum_log2_pssm = np.load(f)
-        with open(os.path.join(pssms_dir, "VHH_pssm.npy"), "rb") as f:
-            hum_pssm = np.load(f)
-
-    if is_VHH:
-        with open(os.path.join(pssms_dir, "VHH_log2_pssm.npy"), "rb") as f:
-            VHH_log2_pssm = np.load(f)
-        with open(os.path.join(pssms_dir, "VHH_pssm.npy"), "rb") as f:
-            VHH_pssm = np.load(f)
+    if nat_to_hum == 'VH':
+        with open(os.path.join(pssms_dir, 'VH_log2_pssm.npy'), 'rb') as f: hum_log2_pssm = np.load(f)
+        with open(os.path.join(pssms_dir, 'VH_pssm.npy'), 'rb') as f: hum_pssm = np.load(f)
+
+    elif nat_to_hum == 'VKappa':
+        with open(os.path.join(pssms_dir, 'VKappa_log2_pssm.npy'), 'rb') as f: hum_log2_pssm = np.load(f)
+        with open(os.path.join(pssms_dir, 'VKappa_pssm.npy'), 'rb') as f: hum_pssm = np.load(f)
+
+    elif nat_to_hum == 'VLambda':
+        with open(os.path.join(pssms_dir, 'VLambda_log2_pssm.npy'), 'rb') as f: hum_log2_pssm = np.load(f)
+        with open(os.path.join(pssms_dir, 'VLambda_pssm.npy'), 'rb') as f: hum_pssm = np.load(f)
+
+    elif nat_to_hum == 'VHH':
+        with open(os.path.join(pssms_dir, 'VHH_log2_pssm.npy'), 'rb') as f: hum_log2_pssm = np.load(f)
+        with open(os.path.join(pssms_dir, 'VHH_pssm.npy'), 'rb') as f: hum_pssm = np.load(f)
+
+    if is_VHH: 
+        with open(os.path.join(pssms_dir, 'VHH_log2_pssm.npy'), 'rb') as f: VHH_log2_pssm = np.load(f)
+        with open(os.path.join(pssms_dir, 'VHH_pssm.npy'), 'rb') as f: VHH_pssm = np.load(f)
 
     # keys will be AHo numbers, values will be allowed substitutions according to the PSSM criteria we have
-    pssm_allowed_substitutions_at_position = {}
-
-    for AHo in range(1, 150):
-        j = AHo - 1  # switch to index
-        hum_allowed = alphabet_pssm[
-            np.where(
-                (hum_log2_pssm[:, j] > 0) & (hum_pssm[:, j] > low_frequency_cutoff)
-            )[0]
-        ]
+    pssm_allowed_substitutions_at_position = {} 
+    
+    for AHo in range(1,150) :
+        j=AHo-1 # switch to index
+        hum_allowed = alphabet_pssm[np.where( (hum_log2_pssm[:,j] > 0)  & (hum_pssm[:,j] > low_frequency_cutoff))[0]]
 
         if is_brute and is_VHH:
-            VHH_allowed = alphabet_pssm[
-                np.where(
-                    (VHH_log2_pssm[:, j] > 0) & (VHH_pssm[:, j] > low_frequency_cutoff)
-                )[0]
-            ]
-            pssm_allowed_substitutions_at_position[AHo] = [
-                aa
-                for aa in hum_allowed
-                if aa in VHH_allowed and aa not in forbidden_mut
-            ]
-        else:
-            pssm_allowed_substitutions_at_position[AHo] = [
-                aa for aa in hum_allowed if aa not in forbidden_mut
-            ]
+            VHH_allowed = alphabet_pssm[np.where( (VHH_log2_pssm[:,j] > 0) & (VHH_pssm[:,j] > low_frequency_cutoff) )[0]]
+            pssm_allowed_substitutions_at_position[AHo] = [aa for aa in hum_allowed if aa in VHH_allowed and aa not in forbidden_mut]
+        else: 
+            pssm_allowed_substitutions_at_position[AHo] = [aa for aa in hum_allowed if aa not in forbidden_mut]
 
     return pssm_allowed_substitutions_at_position
 
-
-def get_cdr_aho_indices_ng(al_seq: str) -> Tuple[np.array, np.array, np.array]:
-    """Get the cdr positions (starts at 1, not 0) without the gaps of an input AHo aligned sequence"""
+def get_cdr_aho_indices_ng(al_seq: str)-> Tuple[np.array, np.array, np.array]:
+    '''Get the cdr positions (starts at 1, not 0) without the gaps of an input AHo aligned sequence'''
 
     ng_cdr1, ng_cdr2, ng_cdr3 = list(), list(), list()
 
     nb_pre_gaps = 0
     for k, res in enumerate([*al_seq]):
-        aho_k = k + 1
-        if res == "-":
-            nb_pre_gaps += 1
+        aho_k = k+1 
+        if res == '-':
+            nb_pre_gaps+=1
             continue
         elif aho_k in cdr1_aho_indices:
-            ng_cdr1.append(aho_k - nb_pre_gaps)
+            ng_cdr1.append(aho_k-nb_pre_gaps)
         elif aho_k in cdr2_aho_indices:
-            ng_cdr2.append(aho_k - nb_pre_gaps)
+            ng_cdr2.append(aho_k-nb_pre_gaps)
         elif aho_k in cdr3_aho_indices:
-            ng_cdr3.append(aho_k - nb_pre_gaps)
+            ng_cdr3.append(aho_k-nb_pre_gaps)
 
     return np.array(ng_cdr1), np.array(ng_cdr2), np.array(ng_cdr3)
 
 
 ## ENHANCED SAMPLING ##
 
-
-def humanise_enhanced_sampling(
-    wt_seq: str,
-    name_seq: str,
-    nat_to_hum: str,
-    is_VHH: str,
-    pdb_file: str,
-    ch_id: str,
-    seq_dir: str,
-    allowed_user_aho_positions: list,
-    threshold_abnativ_score: float = 0.98,
-    threshold_rasa_score: float = 0.15,
-    alphabet: list = alphabet,
-    perc_allowed_decrease_vhh: float = 1.5e-2,
-    forbidden_mut: list = ["C", "M"],
-    a: float = 0.8,
-    b: float = 0.2,
-    seq_ref: str = None,
-    name_seq_ref: str = None,
-    verbose: bool = True,
-) -> str:
-    """Humanise a full input WT sequence through AbNatiV with the Enhanced sampling stategy. It iteratively explores
+def humanise_enhanced_sampling(wt_seq:str, name_seq:str, nat_to_hum: str, is_VHH:str, pdb_file: str, ch_id: str, seq_dir:str, allowed_user_aho_positions:list,
+                          threshold_abnativ_score:float=.98,threshold_rasa_score:float=0.15, alphabet:list=alphabet, perc_allowed_decrease_vhh:float=1.5e-2,
+                          forbidden_mut: list=['C','M'], a:float=.8,b:float=.2, seq_ref: str=None, name_seq_ref: str=None, verbose: bool=True) -> str: 
+    ''' Humanise a full input WT sequence through AbNatiV with the Enhanced sampling stategy. It iteratively explores
     the mutational space aiming for rapid convergence to generate a single humanised sequence.
-
+     
         - If is_VHH == True: it employs a dual-control strategy that aims to increase the AbNatiV VH-hummanness of a sequence
     while retaining its VHH-nativeness.
 
-        - If is_VHH == False: it employs a single-control strategy that aims only to improve the AbNAtiV V{nat_to_hum}-humanness.
+        - If is_VHH == False: it employs a single-control strategy that aims only to improve the AbNAtiV V{nat_to_hum}-humanness. 
 
-    See Parameters for further details.
+    See Parameters for further details.   
 
     Parameters
     ----------
         - wt_seq: str
-            Unaligned sequence string
+            Unaligned sequence string 
         - name_seq: str
         - nat_to_hum: str
             Type of humanness to improve nativeness into (VH, VKappa, VLambda).
         - is_VHH: bool
             If True, considers the dual-control strategy for humanisation, and the VHH seed for the alignment, more suitable for nanobody sequences
         - pdb_file: str
             Filepath to the pdb file of the wt_seq
         - ch_id: str
             Chain id of the pfb pdb_file
         - seq_dir:str
             Directory where to save files
         - allowed_user_aho_positions: list of int
             List of AHo positions allowed by the user to make mutation on
-        - threshold_abnativ_score: float
+        - threshold_abnativ_score: float 
             Bellow the AbNatiV VH threshold score, a position is considered as a liability
-        - threshold_rasa_score: float
+        - threshold_rasa_score: float 
             Above this threshold, the residue is considered solvent exposed and is considered for mutation
         - alphabet: list
             A list of string with the residues composing the sequences
         - perc_allowed_decrease_vhh: float
             Maximun ΔVHH score decrease allowed for a mutation
         - forbidden_mut: list
             List of residues to ban for mutation i.e. ['C','M']
         - a: float
             Used in multi-objective selection function: aΔVH+bΔVHH
         - b: float
             Used in multi-objective selection function: aΔVH+bΔVHH
-        - seq_ref: str
+        - seq_ref: str 
             If None, does not plot any references in the profiles. If str, will plot it
         - name_seq_ref: str
         - verbose: bool
 
     Returns
     -------
         - The unaligned final humanised sequence
 
-    """
-
+    '''
+    
     # Create folder
-    dms_dir = os.path.join(seq_dir, "dms")
-    if not os.path.exists(dms_dir):
-        os.makedirs(dms_dir)
+    dms_dir = os.path.join(seq_dir, 'dms')
+    if not os.path.exists(dms_dir): os.makedirs(dms_dir)
 
     # Compute the average dependence of mutations with DMS
-    _, _, _, dms_avg_dep = compute_dms_map(
-        wt_seq, nat_to_hum, is_VHH, dms_dir, alphabet, name_seq
-    )
+    _, _, _, dms_avg_dep = compute_dms_map(wt_seq, nat_to_hum, is_VHH, dms_dir, alphabet, name_seq)    
 
     # Rasa selection of aho positions
     if threshold_rasa_score == 0:
-        allowed_rasa_aho_positions = list(range(1, 150))
+        allowed_rasa_aho_positions = list(range(1,150))
     else:
-        allowed_rasa_aho_positions = rasa_selection_posi_to_humanise(
-            wt_seq, nat_to_hum, is_VHH, pdb_file, ch_id, threshold_rasa_score
-        )
+        allowed_rasa_aho_positions = rasa_selection_posi_to_humanise(wt_seq, nat_to_hum, is_VHH, pdb_file, ch_id, threshold_rasa_score)
 
     # Intersection rasa and user allowed aho positions
-    allowed_aho_positions = list(
-        set(allowed_rasa_aho_positions) & set(allowed_user_aho_positions)
-    )
+    allowed_aho_positions = list(set(allowed_rasa_aho_positions) & set(allowed_user_aho_positions))
 
     # Humanisation
-    humanised_seq = enhanced_humanisation_of_selected_posis(
-        wt_seq,
-        nat_to_hum,
-        is_VHH,
-        name_seq,
-        threshold_abnativ_score,
-        allowed_aho_positions,
-        dms_avg_dep,
-        perc_allowed_decrease_vhh,
-        forbidden_mut,
-        a,
-        b,
-        verbose,
-    )
+    humanised_seq = enhanced_humanisation_of_selected_posis(wt_seq, nat_to_hum, is_VHH, name_seq, threshold_abnativ_score, allowed_aho_positions, 
+                                                dms_avg_dep, perc_allowed_decrease_vhh, forbidden_mut, a, b, verbose)
 
     # Plot
-    score_and_plot_abnativ_profile_with_ref(
-        wt_seq,
-        nat_to_hum,
-        humanised_seq,
-        is_VHH,
-        name_seq + f"_{nat_to_hum}",
-        seq_ref,
-        name_seq_ref,
-        seq_dir,
-    )
+    score_and_plot_abnativ_profile_with_ref(wt_seq, nat_to_hum, humanised_seq, is_VHH, name_seq+f'_{nat_to_hum}', seq_ref, name_seq_ref, seq_dir)
 
     return humanised_seq
 
-
-def enhanced_humanisation_of_selected_posis(
-    wt_seq: str,
-    nat_to_hum: str,
-    is_VHH: bool,
-    name_seq: str,
-    threshold_abnativ_score: float,
-    allowed_aho_positions: list,
-    dms_average_dependence: list,
-    perc_allowed_decrease_vhh: float,
-    forbidden_mut: list = ["C", "M"],
-    a: float = 0.8,
-    b: float = 0.2,
-    verbose: bool = False,
-) -> str:
-    """
-    Run the sampling on a set of positions allowed_aho_positions flagged for mutation.
+def enhanced_humanisation_of_selected_posis(wt_seq: str, nat_to_hum:str, is_VHH: bool, name_seq:str, threshold_abnativ_score:float, 
+                                    allowed_aho_positions: list, dms_average_dependence: list,
+                                    perc_allowed_decrease_vhh:float, forbidden_mut: list=['C','M'],
+                                    a: float=0.8, b: float=0.2, verbose: bool=False) -> str:
+    '''
+    Run the sampling on a set of positions allowed_aho_positions flagged for mutation. 
 
     Allowed mutations are PSSM-Human enriched residues.
     We mutate the positions ordered by their average dependence on other positions being mutated to
-    guide the convergence towards good mutants.
+    guide the convergence towards good mutants. 
 
     If is_VHH, employs the dual-control strategy to humanise the nanobody.
-    The final sequence generated is the one which improves the most the humanness without impacting too much the
+    The final sequence generated is the one which improves the most the humanness without impacting too much the 
     VHH-nativeness (see single_point_mutation criteria)
 
     Parameters
     ----------
         - wt_seq: str
             Unaligned string sequence
-        - nat_to_hum: str
+        - nat_to_hum: str 
             Type of humanness to improve nativeness into (VH, VKappa, VLambda).
         - is_VHH: bool
             If True, considers the dual-control strategy for humanisation, and the VHH seed for the alignment, more suitable for nanobody sequences
         - name_seq: str
         - threshold_abnativ_score: float
             Bellow the AbNatiV VH threshold score, a position is considered as a liability
         - allowed_aho_positions: list
             List of int, being the allowed AHo positions to do the mutation on
         - dms_average_dependence: list
             A list of length len(dms_map) with the average dependence for each position of dms_map
         - perc_allowed_decrease_vhh:float
-            Maximun ΔVHH score decrease allowed for a mutation
+            Maximun ΔVHH score decrease allowed for a mutation 
         - forbidden_mut: list
             List of residues to ban for mutation i.e. ['C','M']
         - a: float
             Used in multi-objective selection function: aΔVH+bΔVHH
         - b: float
             Used in multi-objective selection function: aΔVH+bΔVHH
         - verbose: bool
 
     Returns
     -------
         - str: final unaligned humanised sequence
 
-    """
+    '''
     # VH assessment
-    wt_vh_seq_abnativ_df, wt_vh_profile_abnativ_df = abnativ_scoring(
-        nat_to_hum,
-        wt_seq,
-        batch_size=1,
-        mean_score_only=False,
-        do_align=True,
-        is_VHH=is_VHH,
-        verbose=False,
-    )
-    al_wt_seq = "".join(wt_vh_seq_abnativ_df["aligned_seq"])
-    wt_vh_score = wt_vh_seq_abnativ_df[f"AbNatiV {nat_to_hum} Score"][0]
+    wt_vh_seq_abnativ_df, wt_vh_profile_abnativ_df = abnativ_scoring(nat_to_hum, wt_seq, batch_size=1,mean_score_only=False, do_align=True, is_VHH=is_VHH, verbose=False)
+    al_wt_seq = ''.join(wt_vh_seq_abnativ_df['aligned_seq'])
+    wt_vh_score = wt_vh_seq_abnativ_df[f'AbNatiV {nat_to_hum} Score'][0]
     saved_original_wt_vh_score = wt_vh_score
 
     # VHH assessement
     if is_VHH:
-        wt_vhh_seq_abnativ_df, wt_vhh_profile_abnativ_df = abnativ_scoring(
-            "VHH",
-            al_wt_seq,
-            batch_size=1,
-            mean_score_only=False,
-            do_align=False,
-            verbose=False,
-        )
-        wt_vhh_score = wt_vhh_seq_abnativ_df["AbNatiV VHH Score"][0]
-        saved_original_wt_vhh_score = wt_vhh_score
+        wt_vhh_seq_abnativ_df, wt_vhh_profile_abnativ_df = abnativ_scoring('VHH', al_wt_seq, batch_size=1,mean_score_only=False, do_align=False, verbose=False)
+        wt_vhh_score = wt_vhh_seq_abnativ_df['AbNatiV VHH Score'][0]
+        saved_original_wt_vhh_score =  wt_vhh_score
 
     saved_original_seq = wt_seq
 
     # Compute PSSM-allowed mutations at each position
-    forbidden_mut += ["-"]
+    forbidden_mut += ['-']
     low_frequency_cutoff_pssm = 0.01
-    pssm_allowed_substitutions_per_aho_position = get_dict_pposi_allowed_muts(
-        low_frequency_cutoff_pssm, nat_to_hum, is_VHH, forbidden_mut, is_brute=False
-    )
+    pssm_allowed_substitutions_per_aho_position = get_dict_pposi_allowed_muts(low_frequency_cutoff_pssm, nat_to_hum, is_VHH, forbidden_mut, is_brute=False)
 
-    # Check positions with AbNatiV-VH scores
-    to_mutate_aho_positions, to_mutate_dms_average_dependence = list(), list()
+    # Check positions with AbNatiV-VH scores  
+    to_mutate_aho_positions, to_mutate_dms_average_dependence  = list(), list()
     for aho_posi in allowed_aho_positions:
-        posi = aho_posi - 1
-        if (
-            wt_vh_profile_abnativ_df[f"AbNatiV {nat_to_hum} Residue Score"][posi]
-            <= threshold_abnativ_score
-        ):
+        posi = aho_posi-1
+        if wt_vh_profile_abnativ_df[f'AbNatiV {nat_to_hum} Residue Score'][posi] <= threshold_abnativ_score:
             to_mutate_aho_positions.append(aho_posi)
             to_mutate_dms_average_dependence.append(dms_average_dependence[posi])
 
     flag_tried_everything = False
 
-    nb_mut_accepted = 0
-    ori_nb_pois_to_mutate = len(to_mutate_aho_positions)
+    nb_mut_accepted=0
+    ori_nb_pois_to_mutate= len(to_mutate_aho_positions)
 
-    # Iterate over positions to mutate and update them when a mutation is accepted
-    while len(to_mutate_aho_positions) > 0 and not flag_tried_everything:
+    # Iterate over positions to mutate and update them when a mutation is accepted 
+    while len(to_mutate_aho_positions)>0 and not flag_tried_everything:
 
-        sorted_to_mutate_aho_positions = [
-            x
-            for _, x in sorted(
-                zip(to_mutate_dms_average_dependence, to_mutate_aho_positions)
-            )
-        ]
+        sorted_to_mutate_aho_positions = [x for _, x in sorted(zip(to_mutate_dms_average_dependence, to_mutate_aho_positions))]
 
-        if verbose:
-            print("To mutate positions ", sorted_to_mutate_aho_positions)
+        if verbose: print('To mutate positions ', sorted_to_mutate_aho_positions)
 
         # Mutate the positions ordered by their average depedence on other positions being mutated
         # if a mutation is accepted, update the sequence and starts the while loop again
         # if no mutations are accepted at one position, goes to the next one until the end of the list
         # and terminates humanisation (flag_tried_everything)
         for k, aho_posi in enumerate(sorted_to_mutate_aho_positions):
-            print(f"AHO position being mutated {aho_posi}")
+            print(f'AHO position being mutated {aho_posi}')
             if is_VHH:
-                is_posi_mutated, al_mut_seq, mut_vh_score_best, mut_vhh_score_best = (
-                    single_point_mutation(
-                        al_wt_seq,
-                        nat_to_hum,
-                        is_VHH,
-                        aho_posi,
-                        wt_vh_score,
-                        pssm_allowed_substitutions_per_aho_position,
-                        wt_vhh_score,
-                        perc_allowed_decrease_vhh,
-                        a,
-                        b,
-                        verbose,
-                    )
-                )
+                is_posi_mutated, al_mut_seq, mut_vh_score_best, mut_vhh_score_best = single_point_mutation(al_wt_seq, nat_to_hum, is_VHH, aho_posi, wt_vh_score, 
+                                                                                                            pssm_allowed_substitutions_per_aho_position, wt_vhh_score, 
+                                                                                                            perc_allowed_decrease_vhh, a, b, verbose)
             else:
-                is_posi_mutated, al_mut_seq, mut_vh_score_best, mut_vhh_score_best = (
-                    single_point_mutation(
-                        al_wt_seq,
-                        nat_to_hum,
-                        is_VHH,
-                        aho_posi,
-                        wt_vh_score,
-                        pssm_allowed_substitutions_per_aho_position,
-                        verbose=verbose,
-                    )
-                )
+                is_posi_mutated, al_mut_seq, mut_vh_score_best, mut_vhh_score_best = single_point_mutation(al_wt_seq, nat_to_hum, is_VHH, aho_posi, wt_vh_score, 
+                                                                                                            pssm_allowed_substitutions_per_aho_position, verbose=verbose)
             # Update the new WT sequence
-            al_wt_seq, wt_vh_score, wt_vhh_score = (
-                al_mut_seq,
-                mut_vh_score_best,
-                mut_vhh_score_best,
-            )
-
-            if is_posi_mutated:
-                nb_mut_accepted += 1
-                wt_vh_seq_abnativ_df, wt_vh_profile_abnativ_df = abnativ_scoring(
-                    nat_to_hum,
-                    al_wt_seq,
-                    batch_size=1,
-                    mean_score_only=False,
-                    do_align=False,
-                    verbose=False,
-                )
+            al_wt_seq, wt_vh_score, wt_vhh_score = al_mut_seq, mut_vh_score_best, mut_vhh_score_best
 
+            if is_posi_mutated: 
+                nb_mut_accepted+=1
+                wt_vh_seq_abnativ_df, wt_vh_profile_abnativ_df = abnativ_scoring(nat_to_hum, al_wt_seq, batch_size=1, mean_score_only=False, do_align=False, verbose=False)
+                
                 # Re-assess the laibles posiiotns with a low humanness score, in case a new position has now a low humanness
-                to_mutate_aho_positions, to_mutate_dms_average_dependence = (
-                    list(),
-                    list(),
-                )
+                to_mutate_aho_positions, to_mutate_dms_average_dependence  = list(), list()
                 for allowed_aho_posi in allowed_aho_positions:
-                    allowed_posi = allowed_aho_posi - 1
-                    if (
-                        wt_vh_profile_abnativ_df[f"AbNatiV {nat_to_hum} Residue Score"][
-                            allowed_posi
-                        ]
-                        <= threshold_abnativ_score
-                    ):
+                    allowed_posi = allowed_aho_posi-1 
+                    if wt_vh_profile_abnativ_df[f'AbNatiV {nat_to_hum} Residue Score'][allowed_posi] <= threshold_abnativ_score:
                         to_mutate_aho_positions.append(allowed_aho_posi)
-                        to_mutate_dms_average_dependence.append(
-                            dms_average_dependence[allowed_posi]
-                        )
-
+                        to_mutate_dms_average_dependence.append(dms_average_dependence[allowed_posi])
+            
                 break
 
-            elif k + 1 == len(sorted_to_mutate_aho_positions):
+            elif k+1 == len(sorted_to_mutate_aho_positions):
                 flag_tried_everything = True
 
-    final_seq = "".join(al_wt_seq).replace("-", "")
+    final_seq = ''.join(al_wt_seq).replace('-','')
 
     # Print results
-    if verbose:
-        print(
-            f"\n{nb_mut_accepted} position(s) mutated (Could be several time at the same position) out of {ori_nb_pois_to_mutate} positions originally"
-        )
+    if verbose: print(f'\n{nb_mut_accepted} position(s) mutated (Could be several time at the same position) out of {ori_nb_pois_to_mutate} positions originally')
 
     if is_VHH:
-        print(
-            f"\n{name_seq} WT sequence (VH: {saved_original_wt_vh_score}, VHH: {saved_original_wt_vhh_score})\n{saved_original_seq}"
-        )
-        print(
-            f"\n{name_seq} AbNatiV humanised sequence (VH: {wt_vh_score}, VHH: {wt_vhh_score})\n{final_seq}"
-        )
+        print(f'\n{name_seq} WT sequence (VH: {saved_original_wt_vh_score}, VHH: {saved_original_wt_vhh_score})\n{saved_original_seq}')
+        print(f'\n{name_seq} AbNatiV humanised sequence (VH: {wt_vh_score}, VHH: {wt_vhh_score})\n{final_seq}')
     else:
-        print(
-            f"\n{name_seq} WT sequence ({nat_to_hum}: {saved_original_wt_vh_score})\n{saved_original_seq}"
-        )
-        print(f"\n{name_seq} AbNatiV humanised sequence ({nat_to_hum}: {wt_vh_score})")
+        print(f'\n{name_seq} WT sequence ({nat_to_hum}: {saved_original_wt_vh_score})\n{saved_original_seq}')
+        print(f'\n{name_seq} AbNatiV humanised sequence ({nat_to_hum}: {wt_vh_score})')
 
     count_mut = sum(1 for a, b in zip(saved_original_seq, final_seq) if a != b)
 
-    print(f"=====> Final number of mutations {count_mut}")
+    print(f'=====> Final number of mutations {count_mut}')
 
     return final_seq
 
-
-def single_point_mutation(
-    al_wt_seq: str,
-    nat_to_mut: str,
-    is_VHH: str,
-    aho_posi: int,
-    wt_vh_score: float,
-    allowed_substitutions_per_aho_position: dict,
-    wt_vhh_score: float = None,
-    perc_allowed_decrease_vhh: float = 1.5e-2,
-    a: float = 0.8,
-    b: float = 0.2,
-    verbose: bool = True,
-) -> Tuple[bool, str, float, float]:
-    """Single point mutation of a aligned sequence scoring the AbNatiV VHH and VH nativeness.
-
-    We select the mutant which increases the most the VH-AbNatiV score (i.e., highest ΔVH)
-    and which does not decrease the VHH-AbNatiV score by more than perc_allowed_decrease_vhh decrease
-    (i.e., perc_allowed_decrease_vhh decrease tolerance of ΔVHH) when is_VHH is True.
-
+def single_point_mutation(al_wt_seq:str, nat_to_mut: str, is_VHH:str, aho_posi:int, wt_vh_score:float,  
+                          allowed_substitutions_per_aho_position: dict, 
+                          wt_vhh_score:float = None, perc_allowed_decrease_vhh:float = 1.5e-2, 
+                            a:float = 0.8, b:float = 0.2, verbose: bool = True) -> Tuple[bool, str, float, float]:
+    '''Single point mutation of a aligned sequence scoring the AbNatiV VHH and VH nativeness. 
+
+    We select the mutant which increases the most the VH-AbNatiV score (i.e., highest ΔVH) 
+    and which does not decrease the VHH-AbNatiV score by more than perc_allowed_decrease_vhh decrease 
+    (i.e., perc_allowed_decrease_vhh decrease tolerance of ΔVHH) when is_VHH is True. 
+    
     This selection process is governed by the multi-objective function: aΔVH+bΔVHH when is_VHH is True.
+    
+    If no better mutant is found, the procedure continues to the next position. 
 
-    If no better mutant is found, the procedure continues to the next position.
-
-    If a mutant is found, the sequence is updated and the process of selecting positions for mutation
-    recommences to ensure no over positions are depreciated by this new mutation.
+    If a mutant is found, the sequence is updated and the process of selecting positions for mutation 
+    recommences to ensure no over positions are depreciated by this new mutation. 
 
     Parameters
     ----------
         - al_wt_seq: str
             Aligned string sequence
         - nat_to_hum: str
             Type of humanness to improve nativeness into (VH, VKappa, VLambda).
         - is_VHH: bool
             If True, considers the dual-control strategy for humanisation, and the VHH seed for the alignment, more suitable for nanobody sequences
         - aho_posi: int
             AHo position to try mutations on
         - wt_vh_score: float
             AbNatiV VH sequence score, for comparison at the very end
         - allowed_substitutions_per_aho_position: dict
-            Dict with keys will be AHo numbers, values will be allowed substitutions
-            according to the PSSM criteria
+            Dict with keys will be AHo numbers, values will be allowed substitutions 
+            according to the PSSM criteria 
         - wt_vhh_score: float
             AbNatiV VHH sequence score, for comparison at the very end
         - perc_allowed_decrease_vhh:float
-            Maximun ΔVHH score decrease allowed for a mutation
+            Maximun ΔVHH score decrease allowed for a mutation 
         - a: float
             Used in multi-objective selection function: aΔVH+bΔVHH
         - b: float
             Used in multi-objective selection function: aΔVH+bΔVHH
         - verbose: bool
 
     Returns
@@ -972,151 +705,95 @@
         - al_mut_seq: str
             Aligned sequence of the new mutant sequence
         - mut_vh_score_best: float
             New AbNatiV VH score
         - mut_vhh_score_best: float
             New AbNatiV VHH score, it's None if is_VHH is True
 
-    """
+    '''
 
-    posi = aho_posi - 1
+    posi = aho_posi-1
     res_wt = [*al_wt_seq][posi]
 
-    # Init with the WT as best
-    res_best, ΔVH_best, ΔVHH_best, mut_vh_score_best, mut_vhh_score_best = (
-        res_wt,
-        0,
-        0,
-        wt_vh_score,
-        wt_vhh_score,
-    )
+    #Init with the WT as best
+    res_best, ΔVH_best, ΔVHH_best, mut_vh_score_best, mut_vhh_score_best = res_wt, 0, 0, wt_vh_score, wt_vhh_score 
 
-    for mut in allowed_substitutions_per_aho_position[aho_posi]:
-        if res_wt == mut:
-            continue
+    for mut in allowed_substitutions_per_aho_position[aho_posi] :
+        if res_wt == mut: continue
 
         mut_seq = [*al_wt_seq]
         mut_seq[posi] = mut
-        mut_seq = "".join(mut_seq)
+        mut_seq = ''.join(mut_seq)
 
-        # Score mutant
-        mut_vh_seq_abnativ_df, mut_vh_profile_abnativ_df = abnativ_scoring(
-            nat_to_mut,
-            mut_seq,
-            batch_size=1,
-            mean_score_only=True,
-            do_align=False,
-            verbose=False,
-        )
-        mut_vh_score = mut_vh_seq_abnativ_df[f"AbNatiV {nat_to_mut} Score"][0]
+        #Score mutant
+        mut_vh_seq_abnativ_df, mut_vh_profile_abnativ_df = abnativ_scoring(nat_to_mut, mut_seq, batch_size=1,mean_score_only=True, do_align=False, verbose=False)
+        mut_vh_score = mut_vh_seq_abnativ_df[f'AbNatiV {nat_to_mut} Score'][0]
         ΔVH = mut_vh_score - wt_vh_score
-        if verbose:
-            print(f"\n{res_wt} (WT) at posi {posi} tries {mut}, gives ΔVH: {ΔVH}")
+        if verbose: print(f'\n{res_wt} (WT) at posi {posi} tries {mut}, gives ΔVH: {ΔVH}')
 
         if is_VHH:
-            mut_vhh_seq_abnativ_df, mut_vhh_profile_abnativ_df = abnativ_scoring(
-                "VHH",
-                mut_seq,
-                batch_size=1,
-                mean_score_only=True,
-                do_align=False,
-                verbose=False,
-            )
-            mut_vhh_score = mut_vhh_seq_abnativ_df["AbNatiV VHH Score"][0]
+            mut_vhh_seq_abnativ_df, mut_vhh_profile_abnativ_df = abnativ_scoring('VHH', mut_seq, batch_size=1,mean_score_only=True, do_align=False, verbose=False)
+            mut_vhh_score = mut_vhh_seq_abnativ_df['AbNatiV VHH Score'][0]
             ΔVHH = mut_vhh_score - wt_vhh_score
-            if verbose:
-                print(f'\nand gives ΔVHH" {ΔVHH}')
+            if verbose: print(f'\nand gives ΔVHH" {ΔVHH}')
 
-        # Condition acceptance the mutation
-        if ΔVH >= 0:
-            # Metropolis acceptance criterion
+        #Condition acceptance the mutation
+        if ΔVH>=0:
+            # Metropolis acceptance criterion 
             if is_VHH:
-                if ΔVHH >= 0:
+                if ΔVHH >=0:
                     if ΔVH >= ΔVH_best:
-                        (
-                            res_best,
-                            ΔVH_best,
-                            ΔVHH_best,
-                            mut_vh_score_best,
-                            mut_vhh_score_best,
-                        ) = (mut, ΔVH, ΔVHH, mut_vh_score, mut_vhh_score)
+                        res_best, ΔVH_best, ΔVHH_best, mut_vh_score_best, mut_vhh_score_best = mut, ΔVH, ΔVHH, mut_vh_score, mut_vhh_score
                 else:
-                    if abs(ΔVHH) <= perc_allowed_decrease_vhh * wt_vhh_score:
+                    if abs(ΔVHH) <= perc_allowed_decrease_vhh*wt_vhh_score:
                         # Select best mutation
-                        if a * ΔVH + b * ΔVHH >= a * ΔVH_best + b * ΔVHH_best:
-                            (
-                                res_best,
-                                ΔVH_best,
-                                ΔVHH_best,
-                                mut_vh_score_best,
-                                mut_vhh_score_best,
-                            ) = (mut, ΔVH, ΔVHH, mut_vh_score, mut_vhh_score)
+                        if a*ΔVH + b*ΔVHH >= a*ΔVH_best + b*ΔVHH_best:
+                            res_best, ΔVH_best, ΔVHH_best, mut_vh_score_best, mut_vhh_score_best = mut, ΔVH, ΔVHH, mut_vh_score, mut_vhh_score
             else:
                 if ΔVH >= ΔVH_best:
-                    res_best, ΔVH_best, mut_vh_score_best = mut, ΔVH, mut_vh_score
+                        res_best, ΔVH_best, mut_vh_score_best  = mut, ΔVH, mut_vh_score
 
     # Print results
     if res_best == res_wt:
-        if verbose:
-            print(
-                f"\n=> For AHo-position {posi+1}, can not find a better res with a higher humanness score than {res_wt}"
-            )
+        if verbose: print(f'\n=> For AHo-position {posi+1}, can not find a better res with a higher humanness score than {res_wt}')
         is_posi_mutated = False
     else:
-        if verbose:
-            print(
-                f"\n=> Mutation at AHo-position {posi+1} accepted from {res_wt} to {res_best} accepted"
-            )
+        if verbose: print(f'\n=> Mutation at AHo-position {posi+1} accepted from {res_wt} to {res_best} accepted')
         is_posi_mutated = True
-
+        
     # Update sequence
     al_mut_seq = [*al_wt_seq]
     al_mut_seq[posi] = res_best
-    al_mut_seq = "".join(al_mut_seq)
+    al_mut_seq = ''.join(al_mut_seq)
 
     return is_posi_mutated, al_mut_seq, mut_vh_score_best, mut_vhh_score_best
 
 
-## EXHAUSTIVE SAMPLING ##
 
+## EXHAUSTIVE SAMPLING ##
 
-def humanise_exhaustive_sampling(
-    wt_seq: str,
-    nat_to_hum: str,
-    is_VHH: bool,
-    name_seq: str,
-    pdb_file: str,
-    ch_id: str,
-    output_dir: str,
-    pdb_dir: str,
-    allowed_user_aho_positions: list,
-    threshold_abnativ_score: float,
-    threshold_rasa_score: float,
-    perc_allowed_decrease_vhh: float = 1.5e-2,
-    forbidden_mut: list = ["C", "M"],
-    seq_ref: str = None,
-    name_seq_ref: str = None,
-    verbose: bool = True,
-) -> None:
-    """
-    Humanise a full input WT sequence through AbNatiV with the Exhaustive sampling stategy. It assesses all mutation combinations within
+def humanise_exhaustive_sampling(wt_seq:str, nat_to_hum:str, is_VHH:bool, name_seq:str,  pdb_file: str, ch_id: str, output_dir:str, pdb_dir:str, allowed_user_aho_positions: list,
+                             threshold_abnativ_score: float, threshold_rasa_score: float, perc_allowed_decrease_vhh: float=1.5e-2, forbidden_mut: list=['C','M'],
+                            seq_ref: str=None, name_seq_ref: str=None,  verbose: bool=True) -> None: 
+    '''
+    Humanise a full input WT sequence through AbNatiV with the Exhaustive sampling stategy. It assesses all mutation combinations within 
     the available mutational space (PSSM-allowed mutations) and selects the best sequences (Pareto Front). It is much more computationally demanding than the
     Enhanced method. It is not recommended to use if it as to explore a lot of liable positions.
 
         - If is_VHH == True: it employs a dual-control strategy that aims to increase the AbNatiV VH-hummanness of a sequence
     while retaining its VHH-nativeness.
 
         - If is_VHH == False: it employs a single-control strategy that aims only to improve the AbNAtiV V{nat_to_hum}-humanness.
 
-    See Parameters for further details.
-
+    See Parameters for further details.   
+    
     Parameters
     ----------
         - wt_seq: str
-            Unaligned sequence string
+            Unaligned sequence string 
         - nat_to_hum: str
             Type of humanness to improve nativeness into (VH, VKappa, VLambda).
         - is_VHH: bool
             If True, considers the dual-control strategy for humanisation, and the VHH seed for the alignment, more suitable for nanobody sequences
         - name_seq: str
         - pdb_file: str
             Filepath to the pdb file of the wt_seq
@@ -1124,188 +801,112 @@
             Chain id of the pfb pdb_file
         - output_dir:str
             Directory where to save files
         - pdb_dir: str
             Directory where to save predicted pdb structures
         - allowed_user_aho_positions: list of int
             List of AHo positions allowed by the user to make mutation on
-        - threshold_abnativ_score: float
+        - threshold_abnativ_score: float 
             Bellow the AbNatiV VH threshold score, a position is considered as a liability
-        - threshold_rasa_score: float
+        - threshold_rasa_score: float 
             Above this threshold, the residue is considered solvent exposed and is considered for mutation
         - alphabet: list
             A list of string with the residues composing the sequences
         - perc_allowed_decrease_vhh: float
-            Maximun ΔVHH score decrease allowed for a mutation
+            Maximun ΔVHH score decrease allowed for a mutation 
         - forbidden_mut: list
             List of residues to ban for mutation i.e. ['C','M']
-        - seq_ref: str
+        - seq_ref: str 
             If None, does not plot any references in the profiles. If str, will plot it
         - name_seq_ref: str
         - verbose: bool
-
+            
     Returns
     -------
-        - dict with keys will be AHo numbers, values will be allowed substitutions
-            according to the PSSM criteria
-    """
+        - dict with keys will be AHo numbers, values will be allowed substitutions 
+            according to the PSSM criteria 
+    '''
 
     # Create folder
-    fig_fp_save = os.path.join(output_dir, "profiles")
-    if not os.path.exists(fig_fp_save):
-        os.makedirs(fig_fp_save)
+    fig_fp_save = os.path.join(output_dir, 'profiles')
+    if not os.path.exists(fig_fp_save): os.makedirs(fig_fp_save)
 
     # Get allowed mutations at every AHo number
-    forbidden_mut += ["-"]
+    forbidden_mut += ['-']
     low_frequency_cutoff_pssm = 0.01
-    pssm_allowed_substitutions_at_position = get_dict_pposi_allowed_muts(
-        low_frequency_cutoff_pssm, nat_to_hum, is_VHH, forbidden_mut, is_brute=True
-    )
+    pssm_allowed_substitutions_at_position = get_dict_pposi_allowed_muts(low_frequency_cutoff_pssm, nat_to_hum, is_VHH, forbidden_mut, is_brute=True)
 
     # Score WT
-    wt_vh_seq_abnativ_df, wt_vh_profile_abnativ_df = abnativ_scoring(
-        nat_to_hum,
-        wt_seq,
-        batch_size=1,
-        mean_score_only=False,
-        do_align=True,
-        is_VHH=is_VHH,
-        verbose=False,
-    )
-    al_wt_seq = "".join(wt_vh_seq_abnativ_df["aligned_seq"])
+    wt_vh_seq_abnativ_df, wt_vh_profile_abnativ_df = abnativ_scoring(nat_to_hum, wt_seq, batch_size=1,mean_score_only=False, do_align=True, is_VHH=is_VHH, verbose=False)
+    al_wt_seq = ''.join(wt_vh_seq_abnativ_df['aligned_seq'])
 
     if is_VHH:
-        wt_vhh_seq_abnativ_df, wt_vhh_profile_abnativ_df = abnativ_scoring(
-            "VHH",
-            wt_seq,
-            batch_size=1,
-            mean_score_only=False,
-            do_align=True,
-            is_VHH=is_VHH,
-            verbose=False,
-        )
+        wt_vhh_seq_abnativ_df, wt_vhh_profile_abnativ_df = abnativ_scoring('VHH', wt_seq, batch_size=1,mean_score_only=False, do_align=True, is_VHH=is_VHH, verbose=False)
     else:
         wt_vhh_seq_abnativ_df, wt_vhh_profile_abnativ_df = None, None
-
+    
     # Rasa selection of aho positions
     if threshold_rasa_score == 0:
-        allowed_rasa_aho_positions = list(range(1, 150))
+        allowed_rasa_aho_positions = list(range(1,150))
     else:
-        allowed_rasa_aho_positions = rasa_selection_posi_to_humanise(
-            wt_seq, nat_to_hum, is_VHH, pdb_file, ch_id, threshold_rasa_score
-        )
+        allowed_rasa_aho_positions = rasa_selection_posi_to_humanise(wt_seq, nat_to_hum, is_VHH, pdb_file, ch_id, threshold_rasa_score)
 
     # Intersection rasa and user allowed aho positions
-    allowed_aho_positions = list(
-        set(allowed_rasa_aho_positions) & set(allowed_user_aho_positions)
-    )
+    allowed_aho_positions = list(set(allowed_rasa_aho_positions) & set(allowed_user_aho_positions))
 
     # Look at all the mutants possible based on PSSM and abnativ liabilities (VH and VHH)
-    allowed_mutations_pposi = exhaustive_selection_mutation_pposi_to_humanise(
-        pssm_allowed_substitutions_at_position,
-        al_wt_seq,
-        nat_to_hum,
-        is_VHH,
-        wt_vh_profile_abnativ_df,
-        wt_vhh_profile_abnativ_df,
-        threshold_abnativ_score,
-        allowed_aho_positions,
-        verbose,
-    )
-
-    inds_that_may_have_mutaitons = [
-        j for j, muts in enumerate(allowed_mutations_pposi) if len(muts) > 1
-    ]
-    options_at_inds = [allowed_mutations_pposi[j] for j in inds_that_may_have_mutaitons]
-
-    generator = xuniqueCombinationsPOSITIONALstr(
-        options_at_inds
-    )  # the only drawback of the generator approach is that it does not tell you directly how many mutations away from wt each generated sequence is
-    all_seqs = [
-        full_sequence_from_mut_option(muts, al_wt_seq, inds_that_may_have_mutaitons)
-        for muts in generator
-    ]
+    allowed_mutations_pposi = exhaustive_selection_mutation_pposi_to_humanise(pssm_allowed_substitutions_at_position, al_wt_seq,
+                                                                         nat_to_hum, is_VHH,
+                                                                        wt_vh_profile_abnativ_df, wt_vhh_profile_abnativ_df, 
+                                                                        threshold_abnativ_score, allowed_aho_positions, verbose)
+
+    inds_that_may_have_mutaitons=[j for j,muts in enumerate(allowed_mutations_pposi) if len(muts)>1 ]
+    options_at_inds = [allowed_mutations_pposi[j] for j in inds_that_may_have_mutaitons ]
 
-    fp_all_mut = os.path.join(output_dir, f"{name_seq}_all_mut_brute.fa")
-    with open(fp_all_mut, "w") as f:
+    generator = xuniqueCombinationsPOSITIONALstr(options_at_inds) # the only drawback of the generator approach is that it does not tell you directly how many mutations away from wt each generated sequence is 
+    all_seqs = [full_sequence_from_mut_option(muts, al_wt_seq, inds_that_may_have_mutaitons) for muts in generator]
+
+    fp_all_mut = os.path.join(output_dir, f'{name_seq}_all_mut_brute.fa')
+    with open(fp_all_mut, 'w') as f:
         for k, seq in enumerate(all_seqs):
             seq = seq[0]
-            f.write(f">{k}_brute_mut\n")
-            f.write(seq + "\n")
+            f.write(f'>{k}_brute_mut\n')
+            f.write(seq+'\n')
 
     try:
-        brute_vh_seq_abnativ_df, brute_vh_profile_abnativ_df = abnativ_scoring(
-            nat_to_hum,
-            fp_all_mut,
-            batch_size=128,
-            mean_score_only=True,
-            do_align=False,
-            is_VHH=is_VHH,
-            verbose=True,
-        )
+        brute_vh_seq_abnativ_df, brute_vh_profile_abnativ_df = abnativ_scoring(nat_to_hum, fp_all_mut, batch_size=128,mean_score_only=True, do_align=False, is_VHH=is_VHH, verbose=True)
 
         if is_VHH:
-            brute_vhh_seq_abnativ_df, brute_vhh_profile_abnativ_df = abnativ_scoring(
-                "VHH",
-                fp_all_mut,
-                batch_size=128,
-                mean_score_only=True,
-                do_align=False,
-                is_VHH=is_VHH,
-                verbose=True,
-            )
-        else:
+            brute_vhh_seq_abnativ_df, brute_vhh_profile_abnativ_df = abnativ_scoring('VHH', fp_all_mut, batch_size=128,mean_score_only=True, do_align=False, is_VHH=is_VHH, verbose=True)
+        else: 
             brute_vhh_seq_abnativ_df, brute_vhh_profile_abnativ_df = None, None
 
-        # Pareto select and plot all mutants
-        exhaustive_selection_of_mutants(
-            name_seq,
-            nat_to_hum,
-            is_VHH,
-            wt_vh_seq_abnativ_df,
-            wt_vhh_seq_abnativ_df,
-            brute_vh_seq_abnativ_df,
-            brute_vhh_seq_abnativ_df,
-            perc_allowed_decrease_vhh,
-            output_dir,
-            pdb_dir,
-            fig_fp_save,
-            pdb_file,
-            ch_id,
-            seq_ref,
-            name_seq_ref,
-            verbose,
-        )
-
+        #Pareto select and plot all mutants
+        exhaustive_selection_of_mutants(name_seq, nat_to_hum, is_VHH, wt_vh_seq_abnativ_df, wt_vhh_seq_abnativ_df, brute_vh_seq_abnativ_df, brute_vhh_seq_abnativ_df,
+                                perc_allowed_decrease_vhh, output_dir, pdb_dir, fig_fp_save, pdb_file, ch_id,
+                                    seq_ref, name_seq_ref,verbose)
+        
     finally:
         os.remove(fp_all_mut)
-    return
+    return 
 
-
-def exhaustive_selection_mutation_pposi_to_humanise(
-    pssm_allowed_substitutions_at_position: dict,
-    al_wt_seq: str,
-    nat_to_hum: str,
-    is_VHH: bool,
-    df_vh_abnativ_profile: pd.DataFrame,
-    df_vhh_abnativ_profile: pd.DataFrame,
-    threshold_abnativ_score: float,
-    allowed_aho_positions: list = fr_aho_indices,
-    verbose: bool = True,
-) -> list:
+def exhaustive_selection_mutation_pposi_to_humanise(pssm_allowed_substitutions_at_position:dict, al_wt_seq:str, nat_to_hum:str, is_VHH:bool,
+                                               df_vh_abnativ_profile: pd.DataFrame, df_vhh_abnativ_profile: pd.DataFrame, threshold_abnativ_score:float, allowed_aho_positions: list= fr_aho_indices,
+                                               verbose: bool= True) -> list:
+    
     """Returns a list of str of mutation options for each position on AbNatiV liabilities (VHH and VH) and PSMM (VHH and VH)
     allowed mutations is is_VHH. It works only on v{nat_to_hum} for other versions.
 
-    At least return the WT res for each position. If there are potential mutants, for a position it will
+    At least return the WT res for each position. If there are potential mutants, for a position it will 
     be for instance : 'AGV'.
-
+    
     Parameters
     ----------
         - pssm_allowed_substitutions_at_position: dict
-            dict with keys will be AHo numbers, values will be allowed substitutions
+            dict with keys will be AHo numbers, values will be allowed substitutions 
             according to the PSSM criteria
         - al_wt_seq: str
             Aligned WT sequence
         - nat_to_hum: str
             Type of humanness to improve nativeness into (VH, VKappa, VLambda).
         - is_VHH: bool
             If True, considers the VHH seed for the alignment, more suitable when aligning nanobody sequences
@@ -1314,130 +915,75 @@
         - df_vhh_abnativ_profile: pd.DataFrame
             AbNatiV dataframe profile of VHH-assessment
         -threshold_abnativ_score: float
             Bellow the AbNatiV VH threshold score, a position is considered as a liability
         - allowed_aho_positions: list
             List of int, being the allowed AHo positions to do the mutation on
         - verbose: bool
-
+            
     Returns
     -------
-        - dict with keys will be AHo numbers, values will be allowed substitutions
-            according to the PSSM criteria
+        - dict with keys will be AHo numbers, values will be allowed substitutions 
+            according to the PSSM criteria 
     """
 
-    # get available mutations
-    absolute_max_number_of_seqs = 1e5
+    # get available mutations 
+    absolute_max_number_of_seqs=1e5
 
     mutation_options = []
-    combinatorial_space_size = 1
+    combinatorial_space_size=1
     liabilities_skipped_no_pssm_options = []
-    flagged_as_AbNativ_liability, flagged_as_pssm_liability = [], []
-
-    not_aho_posi = 0
-    for AHo in range(1, 150):
-        j = AHo - 1  # switch to index
-        wt_res = al_wt_seq[j]
-        mutation_options += [
-            wt_res
-        ]  # this is done so that at the very least there is the WT sequence as an option
-        if wt_res == "-":
+    flagged_as_AbNativ_liability, flagged_as_pssm_liability=[],[]
+    
+    not_aho_posi=0
+    for AHo in range(1,150) :
+        j=AHo-1 # switch to index
+        wt_res=al_wt_seq[j]
+        mutation_options+= [wt_res] # this is done so that at the very least there is the WT sequence as an option
+        if wt_res=='-':
             continue
         not_aho_posi += 1
-        if AHo not in allowed_aho_positions:
-            continue
+        if AHo not in allowed_aho_positions: continue
         # potentially mutable because exposed and poor humanness/nativeness
 
-        if is_VHH:
-            condition = (
-                df_vh_abnativ_profile[f"AbNatiV {nat_to_hum} Residue Score"][j]
-                < threshold_abnativ_score
-                or df_vhh_abnativ_profile["AbNatiV VHH Residue Score"][j]
-                < threshold_abnativ_score
-            )
-        else:
-            condition = (
-                df_vh_abnativ_profile[f"AbNatiV {nat_to_hum} Residue Score"][j]
-                < threshold_abnativ_score
-            )
+        if is_VHH: condition = (df_vh_abnativ_profile[f'AbNatiV {nat_to_hum} Residue Score'][j] < threshold_abnativ_score or df_vhh_abnativ_profile['AbNatiV VHH Residue Score'][j] < threshold_abnativ_score) 
+        else: condition = df_vh_abnativ_profile[f'AbNatiV {nat_to_hum} Residue Score'][j] < threshold_abnativ_score
 
         if condition:
             flagged_as_AbNativ_liability += [AHo]
             # at least one pssm allowed mutation at this liability site
-            if len(pssm_allowed_substitutions_at_position[AHo]) > 0:
-                mutation_options[-1] += "".join(
-                    [
-                        aa
-                        for aa in pssm_allowed_substitutions_at_position[AHo]
-                        if aa != al_wt_seq[j]
-                    ]
-                )  # add those that are different from WT residues
-                combinatorial_space_size *= len(mutation_options[-1])
-            else:
-                liabilities_skipped_no_pssm_options += [AHo]
+            if len(pssm_allowed_substitutions_at_position[AHo])>0 : 
+                mutation_options[-1] += ''.join([ aa for aa in pssm_allowed_substitutions_at_position[AHo] if aa!=al_wt_seq[j]]) # add those that are different from WT residues
+                combinatorial_space_size*=len(mutation_options[-1])
+            else :
+                liabilities_skipped_no_pssm_options +=[ AHo ]
         # not a CDR position, where WT residue is not among pssm-allowed
-        elif (
-            len(pssm_allowed_substitutions_at_position[AHo]) > 0
-            and AHo in fr_aho_indices
-            and al_wt_seq[j] not in pssm_allowed_substitutions_at_position[AHo]
-        ):
-            flagged_as_pssm_liability += [AHo]
-            mutation_options[-1] += "".join(
-                [
-                    aa
-                    for aa in pssm_allowed_substitutions_at_position[AHo]
-                    if aa != al_wt_seq[j]
-                ]
-            )  # add those that are different from WT residues
-            combinatorial_space_size *= len(mutation_options[-1])
-    if verbose:
-        print("Mutation options per posi:\n", mutation_options)
-        print("\ncombinatorial_space_size = %d\n" % (combinatorial_space_size))
-        print(
-            "flagged_as_AbNativ_liability",
-            len(flagged_as_AbNativ_liability),
-            flagged_as_AbNativ_liability,
-        )
-        print(
-            "flagged_as_pssm_liability",
-            len(flagged_as_pssm_liability),
-            flagged_as_pssm_liability,
-        )
-
-    if combinatorial_space_size > absolute_max_number_of_seqs:
-        print(
-            "Exceeded absolute_max_number_of_seqs=%d recommend to abort with control+C"
-            % (absolute_max_number_of_seqs)
-        )
+        elif len(pssm_allowed_substitutions_at_position[AHo])>0 and AHo in fr_aho_indices and al_wt_seq[j] not in pssm_allowed_substitutions_at_position[AHo] :
+            flagged_as_pssm_liability+=[ AHo ]
+            mutation_options[-1] += ''.join([ aa for aa in pssm_allowed_substitutions_at_position[AHo] if aa!=al_wt_seq[j]]) # add those that are different from WT residues
+            combinatorial_space_size*=len(mutation_options[-1])
+    if verbose: 
+        print('Mutation options per posi:\n', mutation_options)
+        print("\ncombinatorial_space_size = %d\n"%(combinatorial_space_size))
+        print('flagged_as_AbNativ_liability', len(flagged_as_AbNativ_liability), flagged_as_AbNativ_liability)
+        print('flagged_as_pssm_liability', len(flagged_as_pssm_liability), flagged_as_pssm_liability)
 
-    return mutation_options
+    if combinatorial_space_size > absolute_max_number_of_seqs :
+        print("Exceeded absolute_max_number_of_seqs=%d recommend to abort with control+C"%(absolute_max_number_of_seqs))
 
+    return mutation_options
 
-def exhaustive_selection_of_mutants(
-    name_seq: str,
-    nat_to_hum: str,
-    is_VHH: bool,
-    wt_vh_seq_abnativ_df: pd.DataFrame,
-    wt_vhh_seq_abnativ_df: pd.DataFrame,
-    brute_vh_seq_abnativ_df: pd.DataFrame,
-    brute_vhh_seq_abnativ_df: pd.DataFrame,
-    perc_allowed_decrease_vhh: float,
-    output_dir: str,
-    pdb_dir: str,
-    fig_fp_save: str,
-    pdb_file: str,
-    ch_id: str,
-    seq_ref: str = None,
-    name_seq_ref: str = None,
-    verbose: bool = True,
-):
-    """Select the best sequences trhough a Pareto front search that seeks to increase the V{nat_to_hum}-humanness and to
-    decrease the number of mutations.
+def exhaustive_selection_of_mutants(name_seq: str, nat_to_hum:str, is_VHH:bool, wt_vh_seq_abnativ_df: pd.DataFrame, wt_vhh_seq_abnativ_df:pd.DataFrame, 
+                               brute_vh_seq_abnativ_df: pd.DataFrame, brute_vhh_seq_abnativ_df: pd.DataFrame,
+                               perc_allowed_decrease_vhh: float, output_dir: str, pdb_dir: str, fig_fp_save: str, 
+                               pdb_file: str, ch_id: str, seq_ref: str=None, name_seq_ref: str=None, verbose: bool=True):
+    '''Select the best sequences trhough a Pareto front search that seeks to increase the V{nat_to_hum}-humanness and to
+    decrease the number of mutations. 
 
-    It plots the Pareto Front results.
+    It plots the Pareto Front results. 
 
     If is_VHH, will apply a hard cut-off and remove all mutants with a ΔVHH decrease >= perc_allowed_decrease_vhh*VHH(WT).
 
     Parameters
     ----------
         - name_seq: str
         - nat_to_hum: str
@@ -1449,192 +995,127 @@
         - wt_vhh_seq_abnativ_df: pd.DataFrame
             AbNatiV dataframe profile of VHH-assessmentv of WT
         - brute_vh_seq_abnativ_df: pd.DataFrame
             AbNatiV dataframe profile of VH-assessment of all brute sequences
         - brute_vhh_seq_abnativ_df: pd.DataFrame
             AbNatiV dataframe profile of VHH-assessmentv of all brute sequences
         - perc_allowed_decrease_vhh: float
-            Maximun ΔVHH score decrease allowed for a mutation
+            Maximun ΔVHH score decrease allowed for a mutation 
         - output_dir:str
             Directory where to save files
         - pdb_dir: str
             Directory where to save predicted pdb structures
         -fig_fp_save: str
             Folder where to save the profiles of pareto set
         - pdb_file: str
             Filepath to the pdb file of the wt_seq
         - ch_id: str
             Chain id of the pfb pdb_file
-        - seq_ref: str
+        - seq_ref: str 
             If None, does not plot any references in the profiles. If str, will plot it
         - name_seq_ref: str
         - verbose: bool
-
+            
     Returns
     -------
-        - dict with keys will be AHo numbers, values will be allowed substitutions
-            according to the PSSM criteria
-    """
-
-    wt_seq = wt_vh_seq_abnativ_df["aligned_seq"][0]
-    wt_vh_score = wt_vh_seq_abnativ_df[f"AbNatiV {nat_to_hum} Score"][0]
-    if is_VHH:
-        wt_vhh_score = wt_vhh_seq_abnativ_df["AbNatiV VHH Score"][0]
+        - dict with keys will be AHo numbers, values will be allowed substitutions 
+            according to the PSSM criteria 
+    '''
+    
+    wt_seq = wt_vh_seq_abnativ_df['aligned_seq'][0]
+    wt_vh_score = wt_vh_seq_abnativ_df[f'AbNatiV {nat_to_hum} Score'][0]
+    if is_VHH: 
+        wt_vhh_score = wt_vhh_seq_abnativ_df['AbNatiV VHH Score'][0]
 
     allowed_hum_seqs = defaultdict(list)
-    for k, id in enumerate(brute_vh_seq_abnativ_df["seq_id"]):
+    for k, id in enumerate(brute_vh_seq_abnativ_df['seq_id']):
 
-        ΔVH = round(
-            brute_vh_seq_abnativ_df[f"AbNatiV {nat_to_hum} Score"][k] - wt_vh_score, 5
-        )
-        if is_VHH:
-            ΔVHH = round(
-                brute_vhh_seq_abnativ_df["AbNatiV VHH Score"][k] - wt_vhh_score, 5
-            )
-
-        count_mut = sum(
-            1 for a, b in zip(wt_seq, brute_vh_seq_abnativ_df["input_seq"][k]) if a != b
-        )
+        ΔVH = round(brute_vh_seq_abnativ_df[f'AbNatiV {nat_to_hum} Score'][k] - wt_vh_score,5)
+        if is_VHH: 
+            ΔVHH = round(brute_vhh_seq_abnativ_df['AbNatiV VHH Score'][k] - wt_vhh_score,5)
 
-        if is_VHH and id != brute_vhh_seq_abnativ_df["seq_id"][k]:
-            raise Exception("ID VH not the same than ID VHH")
+        count_mut = sum(1 for a, b in zip(wt_seq, brute_vh_seq_abnativ_df['input_seq'][k]) if a != b)
+
+        if is_VHH and id!=brute_vhh_seq_abnativ_df['seq_id'][k]:
+                raise Exception('ID VH not the same than ID VHH')
 
         if ΔVH >= 0:
             if is_VHH:
-                if ΔVHH >= -perc_allowed_decrease_vhh * wt_vhh_score:
-                    allowed_hum_seqs["seq_id"].extend([id])
-                    allowed_hum_seqs["VH"].extend(
-                        [brute_vh_seq_abnativ_df["AbNatiV VH Score"][k]]
-                    )
-                    allowed_hum_seqs["VHH"].extend(
-                        [brute_vhh_seq_abnativ_df["AbNatiV VHH Score"][k]]
-                    )
-                    allowed_hum_seqs["Diff-VH"].extend([ΔVH])
-                    allowed_hum_seqs["Diff-VHH"].extend([ΔVHH])
-                    allowed_hum_seqs["count_mut"].extend([count_mut])
-                    allowed_hum_seqs["aligned_seq"].extend(
-                        [brute_vh_seq_abnativ_df["input_seq"][k]]
-                    )
-            else:
-                allowed_hum_seqs["seq_id"].extend([id])
-                allowed_hum_seqs[f"{nat_to_hum}"].extend(
-                    [brute_vh_seq_abnativ_df[f"AbNatiV {nat_to_hum} Score"][k]]
-                )
-                allowed_hum_seqs[f"Diff-{nat_to_hum}"].extend([ΔVH])
-                allowed_hum_seqs["count_mut"].extend([count_mut])
-                allowed_hum_seqs["aligned_seq"].extend(
-                    [brute_vh_seq_abnativ_df["input_seq"][k]]
-                )
+                if ΔVHH >= - perc_allowed_decrease_vhh*wt_vhh_score:
+                    allowed_hum_seqs['seq_id'].extend([id])
+                    allowed_hum_seqs['VH'].extend([brute_vh_seq_abnativ_df['AbNatiV VH Score'][k]])
+                    allowed_hum_seqs['VHH'].extend([brute_vhh_seq_abnativ_df['AbNatiV VHH Score'][k]])
+                    allowed_hum_seqs['Diff-VH'].extend([ΔVH])
+                    allowed_hum_seqs['Diff-VHH'].extend([ΔVHH])
+                    allowed_hum_seqs['count_mut'].extend([count_mut])
+                    allowed_hum_seqs['aligned_seq'].extend([brute_vh_seq_abnativ_df['input_seq'][k]])
+            else: 
+                allowed_hum_seqs['seq_id'].extend([id])
+                allowed_hum_seqs[f'{nat_to_hum}'].extend([brute_vh_seq_abnativ_df[f'AbNatiV {nat_to_hum} Score'][k]])
+                allowed_hum_seqs[f'Diff-{nat_to_hum}'].extend([ΔVH])
+                allowed_hum_seqs['count_mut'].extend([count_mut])
+                allowed_hum_seqs['aligned_seq'].extend([brute_vh_seq_abnativ_df['input_seq'][k]])
 
     df_allowed_hum_seqs = pd.DataFrame.from_dict(allowed_hum_seqs)
 
-    # paretoplot
-    df_pareto = df_allowed_hum_seqs[[f"Diff-{nat_to_hum}", "count_mut"]]
+    #paretoplot
+    df_pareto = df_allowed_hum_seqs[[f'Diff-{nat_to_hum}','count_mut']]
     mask = paretoset(df_pareto, sense=["max", "min"])
 
     df_pareto = df_allowed_hum_seqs
     df_pareto_set = df_allowed_hum_seqs[mask]
-    df_pareto_set.to_csv(os.path.join(output_dir, f"{name_seq}_pareto_set.csv"))
-    df_pareto_set = df_pareto_set.sort_values("count_mut")
+    df_pareto_set.to_csv(os.path.join(output_dir, f'{name_seq}_pareto_set.csv'))
+    df_pareto_set = df_pareto_set.sort_values('count_mut')
 
-    # Print all png and structures of the pareto set
-    dict_all_seqs = {name_seq + "_wt": wt_seq}
+    #Print all png and structures of the pareto set 
+    dict_all_seqs={name_seq+'_wt':wt_seq}
     if is_VHH:
-        predict_struct_vhh(wt_seq.replace("-", ""), name_seq + "_wt", pdb_dir)
+        predict_struct_vhh(wt_seq.replace('-',''), name_seq+'_wt', pdb_dir)
 
-    fp_pareto_mut = os.path.join(output_dir, f"{name_seq}_pareto_mut_brute.fa")
-    if verbose:
-        print(
-            f"\n> Select best mutant from the pareto set on ΔVH and nb of mutations space and save them in {fp_pareto_mut}.\n"
-        )
-    with open(fp_pareto_mut, "w") as f:
-        for k, hum_seq in enumerate(df_pareto_set["aligned_seq"]):
-            f.write(f">{name_seq} +{str(k)}\n")
-            f.write(hum_seq.replace("-", "") + "\n")
+    fp_pareto_mut = os.path.join(output_dir, f'{name_seq}_pareto_mut_brute.fa')
+    if verbose: print(f'\n> Select best mutant from the pareto set on ΔVH and nb of mutations space and save them in {fp_pareto_mut}.\n')
+    with open(fp_pareto_mut, 'w') as f:
+        for k, hum_seq in enumerate(df_pareto_set['aligned_seq']):
+            f.write(f'>{name_seq} +{str(k)}\n')
+            f.write(hum_seq.replace('-','')+'\n')
 
-            dict_all_seqs[name_seq + f"_{count_mut}"] = hum_seq
-            count_mut = list(df_pareto_set["count_mut"])[k]
+            dict_all_seqs[name_seq + f'_{count_mut}']=hum_seq
+            count_mut = list(df_pareto_set['count_mut'])[k]
             if is_VHH:
-                predict_struct_vhh(
-                    hum_seq.replace("-", ""),
-                    name_seq + "_exhaustive_" + str(k),
-                    pdb_dir,
-                )
-            print_chimera_mutations_code(
-                wt_seq, hum_seq, pdb_file, ch_id, pdb_dir, name_seq + str(k)
-            )
-            score_and_plot_abnativ_profile_with_ref(
-                wt_seq.replace("-", ""),
-                nat_to_hum,
-                hum_seq.replace("-", ""),
-                is_VHH,
-                name_seq + nat_to_hum + str(count_mut),
-                seq_ref,
-                name_seq_ref,
-                fig_fp_save,
-            )
+                predict_struct_vhh(hum_seq.replace('-',''), name_seq+'_exhaustive_' + str(k), pdb_dir)
+            print_chimera_mutations_code(wt_seq, hum_seq, pdb_file, ch_id, pdb_dir, name_seq + str(k))
+            score_and_plot_abnativ_profile_with_ref(wt_seq.replace('-',''), nat_to_hum, hum_seq.replace('-',''), is_VHH, name_seq + nat_to_hum+ str(count_mut), seq_ref, name_seq_ref, fig_fp_save)
 
     sns.set_theme()
-    sns.set(font_scale=1)
-    sns.set_style(
-        "white",
-        {
-            "axes.spines.right": True,
-            "axes.spines.top": True,
-            "xtick.bottom": True,
-            "ytick.left": True,
-        },
-    )
-    fig, ax = plt.subplots(figsize=(6, 6))
-
-    cm = plt.cm.get_cmap("RdYlBu")
-    plt.scatter(
-        df_pareto_set["count_mut"],
-        df_pareto_set[f"Diff-{nat_to_hum}"],
-        color="black",
-        s=80,
-        label="Pareto set",
-    )
+    sns.set(font_scale = 1)
+    sns.set_style('white', {'axes.spines.right':True, 'axes.spines.top':True,
+                        'xtick.bottom': True, 'ytick.left': True})
+    fig, ax = plt.subplots(figsize=(6,6))
+
+    cm = plt.cm.get_cmap('RdYlBu')
+    plt.scatter(df_pareto_set['count_mut'], df_pareto_set[f'Diff-{nat_to_hum}'], color='black', s=80, label='Pareto set')
+    
+
+    if is_VHH: 
+        sc = plt.scatter(df_pareto['count_mut'], df_pareto[f'Diff-{nat_to_hum}'], c=df_pareto['Diff-VHH'], s=20,cmap=cm)
+        plt.colorbar(sc, label='ΔVHH AbNatiV')
 
-    if is_VHH:
-        sc = plt.scatter(
-            df_pareto["count_mut"],
-            df_pareto[f"Diff-{nat_to_hum}"],
-            c=df_pareto["Diff-VHH"],
-            s=20,
-            cmap=cm,
-        )
-        plt.colorbar(sc, label="ΔVHH AbNatiV")
-
-    else:
-        sc = plt.scatter(
-            df_pareto["count_mut"],
-            df_pareto[f"Diff-{nat_to_hum}"],
-            c="darkorange",
-            alpha=0.7,
-            s=20,
-        )
+    else: 
+        sc = plt.scatter(df_pareto['count_mut'], df_pareto[f'Diff-{nat_to_hum}'], c='darkorange', alpha=.7, s=20)
 
-    plt.xlabel("Number of mutations")
-    plt.ylabel(f"ΔV{nat_to_hum} AbNatiV")
+    plt.xlabel('Number of mutations')
+    plt.ylabel(f'ΔV{nat_to_hum} AbNatiV')
     plt.legend()
-    plt.title(f"Pareto optimal set {name_seq}")
-    plt.savefig(
-        os.path.join(fig_fp_save, f"{name_seq}_brute_pareto_plot.png"),
-        dpi=800,
-        bbox_inches="tight",
-    )
-
-    # Print pap file
-    print_Alignment_pap(
-        dict_all_seqs, os.path.join(output_dir, f"{name_seq}_hums.pap"), nchar_id=18
-    )
+    plt.title(f'Pareto optimal set {name_seq}')
+    plt.savefig(os.path.join(fig_fp_save, f'{name_seq}_brute_pareto_plot.png'), dpi=800, bbox_inches='tight')
 
-    return
+    # Print pap file 
+    print_Alignment_pap(dict_all_seqs, os.path.join(output_dir,f'{name_seq}_hums.pap'), nchar_id=18)
 
+    return
 
 def xuniqueCombinationsPOSITIONALstr(list_of_strings, n=None):
     """
     xuniqueCombinationsPOSITIONALstr(['abc', 'AB'] ) -->['aA', 'bA', 'cA', 'aB', 'bB', 'cB']
     N= product([len(l) for l in list_of_list_items ])
     """
     if n is None:
@@ -1645,335 +1126,177 @@
         for i in range(len(list_of_strings)):
             for cc in xuniqueCombinationsPOSITIONALstr(list_of_strings[i + 1 :], n - 1):
                 j = -1
                 while j < len(list_of_strings[i]) - 1:
                     j += 1
                     yield list_of_strings[i][j] + cc
 
-
-def full_sequence_from_mut_option(mut_option, wt_seq, inds_of_mutations):
-    """
+def full_sequence_from_mut_option(mut_option, wt_seq, inds_of_mutations ) :
+    '''
     # generate the full sequence for an option like 'KENAFQREVNQ', 'QENAFQREVNQ', 'KQNAFQREVNQ', ...
     # and also return number of mutations from wt
     assumes inds_of_mutations is sorted (which it should be)
     probably would be faster with a Biopython mutable_str or a list, but this should be good enough
-    """
-    seq = ""
-    oldj = 0
-    nmuts = 0
-    for i, j in enumerate(inds_of_mutations):
-        seq += (
-            wt_seq[oldj:j] + mut_option[i]
-        )  # replace in str of WT the mutation at the index
-        oldj = j + 1
-        if mut_option[i] != wt_seq[j]:
-            nmuts += 1
-    seq += wt_seq[oldj:]
-    return seq, nmuts
-
-
-## PLOTTING ##
-
-
-def score_and_subplot_profile_with_ref(
-    axs: plt.axes,
-    id_subplot: int,
-    seq_wt: str,
-    seq_hum: str,
-    seq_ref: str,
-    model_type: str,
-    name_seq: str,
-    name_seq_ref: str,
-    is_VHH: bool,
-    keep_gaps: bool = False,
-    legend_type: str = "HUM",
-) -> None:
-    """
+    '''
+    seq=''
+    oldj=0
+    nmuts=0
+    for i,j in enumerate(inds_of_mutations) :
+        seq+=wt_seq[oldj:j]+mut_option[i] # replace in str of WT the mutation at the index
+        oldj=j+1
+        if mut_option[i]!=wt_seq[j] :nmuts+=1
+    seq+=wt_seq[oldj:]
+    return seq,nmuts
+
+
+## PLOTTING ## 
+
+def score_and_subplot_profile_with_ref(axs: plt.axes, id_subplot: int, seq_wt: str, seq_hum: str, seq_ref: str, 
+                                       model_type: str, name_seq: str, name_seq_ref: str, is_VHH: bool,keep_gaps:bool=False,
+                                       legend_type:str='HUM')-> None:
+    '''
     Plot for given plt.axes, the AbNatiV VHH and VH profiles of the WT and humanised sequences
     along with a ref sequence if provided.
-    """
-
-    df_mean_wt, df_profile_wt = abnativ_scoring(
-        model_type,
-        seq_wt,
-        batch_size=1,
-        mean_score_only=False,
-        do_align=True,
-        is_VHH=is_VHH,
-        verbose=False,
-    )
-    df_mean_hum, df_profile_hum = abnativ_scoring(
-        model_type,
-        seq_hum,
-        batch_size=1,
-        mean_score_only=False,
-        do_align=True,
-        is_VHH=is_VHH,
-        verbose=False,
-    )
-
-    seq_wt = "".join(df_profile_wt["aa"].tolist())
-    seq_hum = "".join(df_profile_hum["aa"].tolist())
-
-    scores_wt = df_profile_wt[f"AbNatiV {model_type} Residue Score"].tolist()
-    scores_hum = df_profile_hum[f"AbNatiV {model_type} Residue Score"].tolist()
-
+    '''
+    
+    df_mean_wt, df_profile_wt = abnativ_scoring(model_type, seq_wt, batch_size=1,mean_score_only=False, do_align=True, is_VHH=is_VHH, verbose=False)
+    df_mean_hum, df_profile_hum = abnativ_scoring(model_type, seq_hum, batch_size=1,mean_score_only=False, do_align=True, is_VHH=is_VHH, verbose=False)
+
+    seq_wt = ''.join(df_profile_wt['aa'].tolist())
+    seq_hum = ''.join(df_profile_hum['aa'].tolist())
+    
+    scores_wt = df_profile_wt[f'AbNatiV {model_type} Residue Score'].tolist()
+    scores_hum = df_profile_hum[f'AbNatiV {model_type} Residue Score'].tolist()
+    
     # Add CDRs
     if not keep_gaps:
         cdr1, cdr2, cdr3 = get_cdr_aho_indices_ng(seq_wt)
-
-        seq_wt = seq_wt.replace("-", "")
-        seq_hum = seq_hum.replace("-", "")
-
-        scores_wt = df_profile_wt[df_profile_wt["aa"] != "-"][
-            f"AbNatiV {model_type} Residue Score"
-        ].tolist()
-        scores_hum = df_profile_hum[df_profile_hum["aa"] != "-"][
-            f"AbNatiV {model_type} Residue Score"
-        ].tolist()
-    else:
+    
+        seq_wt = seq_wt.replace('-','')
+        seq_hum = seq_hum.replace('-','')
+
+        scores_wt = df_profile_wt[df_profile_wt['aa'] != '-'][f'AbNatiV {model_type} Residue Score'].tolist()
+        scores_hum = df_profile_hum[df_profile_hum['aa'] != '-'][f'AbNatiV {model_type} Residue Score'].tolist()
+    else: 
         cdr1, cdr2, cdr3 = cdr1_aho_indices, cdr2_aho_indices, cdr3_aho_indices
+    
+    axs[id_subplot].axvspan(cdr1[0]-1,cdr1[-1]-1, alpha=0.06, color='forestgreen')
+    axs[id_subplot].axvspan(cdr2[0]-1,cdr2[-1]-1, alpha=0.06, color='forestgreen')
+    axs[id_subplot].axvspan(cdr3[0]-1,cdr3[-1]-1, alpha=0.06, color='forestgreen')
 
-    axs[id_subplot].axvspan(cdr1[0] - 1, cdr1[-1] - 1, alpha=0.06, color="forestgreen")
-    axs[id_subplot].axvspan(cdr2[0] - 1, cdr2[-1] - 1, alpha=0.06, color="forestgreen")
-    axs[id_subplot].axvspan(cdr3[0] - 1, cdr3[-1] - 1, alpha=0.06, color="forestgreen")
 
-    wt_nativ = round(df_mean_wt[f"AbNatiV {model_type} Score"][0], 3)
-    hum_nativ = round(df_mean_hum[f"AbNatiV {model_type} Score"][0], 3)
-
-    # Add Ref sequence if required
+    wt_nativ = round(df_mean_wt[f'AbNatiV {model_type} Score'][0],3)
+    hum_nativ = round(df_mean_hum[f'AbNatiV {model_type} Score'][0],3)
+    
+    # Add Ref sequence if required 
     if seq_ref is not None:
-        df_mean_ref, df_profile_ref = abnativ_scoring(
-            model_type,
-            seq_ref,
-            batch_size=1,
-            mean_score_only=False,
-            do_align=True,
-            is_VHH=is_VHH,
-            verbose=False,
-        )
-        seq_ref = "".join(df_profile_ref["aa"].tolist())
-        scores_ref = df_profile_ref[f"AbNatiV {model_type} Residue Score"].tolist()
-
-        ref_nativ = round(df_mean_ref[f"AbNatiV {model_type} Score"][0], 3)
-        axs[id_subplot].plot(
-            scores_ref,
-            linestyle="-.",
-            linewidth=5,
-            alpha=0.15,
-            color="black",
-            label=f"{name_seq_ref} (REF): {ref_nativ} AbNatiV",
-        )
-        axs[id_subplot].annotate(
-            "$\\bf{REF~-}$",
-            xy=(-4.64, 1.0175),
-            xycoords="data",
-            annotation_clip=False,
-            fontsize=18,
-        )
-
-    axs[id_subplot].plot(
-        scores_wt,
-        linewidth=5,
-        alpha=0.65,
-        color="darkorange",
-        label=f"Precursor (WT): {wt_nativ} AbNatiV",
-    )
-    name_legend = "Humanised"
-    if legend_type == "GFT":
-        name_legend = "Grafted"
-    axs[id_subplot].plot(
-        scores_hum,
-        linestyle="--",
-        linewidth=5,
-        alpha=0.95,
-        color="mediumpurple",
-        label=f"{name_legend} ({legend_type}): {hum_nativ} AbNatiV",
-    )
-
+        df_mean_ref, df_profile_ref = abnativ_scoring(model_type, seq_ref, batch_size=1,mean_score_only=False, do_align=True, is_VHH=is_VHH, verbose=False)
+        seq_ref = ''.join(df_profile_ref['aa'].tolist())
+        scores_ref = df_profile_ref[f'AbNatiV {model_type} Residue Score'].tolist()
+
+        ref_nativ = round(df_mean_ref[f'AbNatiV {model_type} Score'][0],3)
+        axs[id_subplot].plot(scores_ref, linestyle ='-.', linewidth = 5, alpha=0.15, color='black', label=f'{name_seq_ref} (REF): {ref_nativ} AbNatiV')
+        axs[id_subplot].annotate('$\\bf{REF~-}$', xy=(-4.64, 1.0175), xycoords='data', annotation_clip=False, fontsize=18) 
+
+
+    axs[id_subplot].plot(scores_wt, linewidth = 5, alpha=0.65, color='darkorange', label=f'Precursor (WT): {wt_nativ} AbNatiV')
+    name_legend = 'Humanised'
+    if legend_type=='GFT':
+        name_legend='Grafted'
+    axs[id_subplot].plot(scores_hum, linestyle ='--', linewidth = 5, alpha=0.95, color='mediumpurple', label=f'{name_legend} ({legend_type}): {hum_nativ} AbNatiV')
+    
+    
     # Bold selected residues mutated
     axs[id_subplot].xaxis.set_ticks(np.arange(0, len(seq_wt), 1.0))
     both_seqs = list()
     if seq_ref is not None:
         for i, res in enumerate(seq_wt):
-            if res != seq_hum[i]:
-                both_seqs.append(
-                    seq_ref[i]
-                    + "\n$\\bf{"
-                    + seq_wt[i]
-                    + "}$"
-                    + "\n"
-                    + "$\\bf{"
-                    + seq_hum[i]
-                    + "}$"
-                )
-            else:
-                both_seqs.append(seq_ref[i] + "\n" + seq_wt[i] + "\n" + seq_hum[i])
-    else:
+                if res != seq_hum[i]:
+                    both_seqs.append( seq_ref[i] + '\n$\\bf{' + seq_wt[i] + '}$'+ '\n'+ '$\\bf{' + seq_hum[i] + '}$')
+                else:
+                    both_seqs.append( seq_ref[i] + '\n'+ seq_wt[i] + '\n' + seq_hum[i])
+    else: 
         for i, res in enumerate(seq_wt):
-            if res != seq_hum[i]:
-                both_seqs.append(
-                    "$\\bf{" + seq_wt[i] + "}$" + "\n" + "$\\bf{" + seq_hum[i] + "}$"
-                )
-            else:
-                both_seqs.append(seq_wt[i] + "\n" + seq_hum[i])
+                if res != seq_hum[i]:
+                    both_seqs.append('$\\bf{' + seq_wt[i] + '}$'+ '\n'+ '$\\bf{' + seq_hum[i] + '}$')
+                else:
+                    both_seqs.append(seq_wt[i] + '\n' + seq_hum[i])
 
     axs[id_subplot].set_xticklabels(both_seqs, fontsize=18)
-    axs[id_subplot].tick_params(axis="x", which="major", pad=3)
-    axs[id_subplot].xaxis.set_label_position("top")
-    axs[id_subplot].set_ylabel(
-        f"AbNatiV {model_type}\nResidue Score", fontsize=25, labelpad=14
-    )
-    axs[id_subplot].set_xlabel("Sequence", fontsize=25, labelpad=14)
-
+    axs[id_subplot].tick_params(axis='x', which='major', pad=3)
+    axs[id_subplot].xaxis.set_label_position('top')
+    axs[id_subplot].set_ylabel(f'AbNatiV {model_type}\nResidue Score', fontsize = 25, labelpad =14)
+    axs[id_subplot].set_xlabel('Sequence', fontsize = 25, labelpad =14)
+    
     axs[id_subplot].xaxis.tick_top()
 
-    # Add WT/HUM annotation
-    axs[id_subplot].annotate(
-        "$\\bf{(WT)}$",
-        xy=(-4.65, 1.01215),
-        xycoords="data",
-        annotation_clip=False,
-        fontsize=18,
-    )
-    axs[id_subplot].annotate(
-        "$\\bf{(" + legend_type + ")}$",
-        xy=(-5.2, 1.0075),
-        xycoords="data",
-        annotation_clip=False,
-        fontsize=18,
-    )
-
-    if model_type == "VHH":
-        subtitle = "$\\bf{VHH-Nativeness~Profiles}$"
-    else:
-        subtitle = "$\\bf{Humanness~Profiles}$"
-
+    # Add WT/HUM annotation 
+    axs[id_subplot].annotate('$\\bf{(WT)}$', xy=(-4.65, 1.01215), xycoords='data', annotation_clip=False, fontsize=18) 
+    axs[id_subplot].annotate('$\\bf{(' + legend_type + ')}$', xy=(-5.2, 1.0075), xycoords='data', annotation_clip=False, fontsize=18) 
+
+    if model_type == 'VHH':
+        subtitle = '$\\bf{VHH-Nativeness~Profiles}$'
+    else: 
+        subtitle = '$\\bf{Humanness~Profiles}$'
+    
     axs[id_subplot].set_title(subtitle, fontsize=28, pad=10)
 
     leg = axs[id_subplot].legend()
     for legobj in leg.legendHandles:
         legobj.set_linewidth(8.0)
-    title_legend = "$\\bf{" + name_seq.replace("_", "~") + "}$"
-    axs[id_subplot].legend(
-        loc="lower left",
-        title=title_legend,
-        frameon=True,
-        edgecolor="w",
-        title_fontsize=28,
-        fontsize=25,
-        framealpha=0.5,
-    )
-
-
-def score_and_plot_abnativ_profile_with_ref(
-    seq_wt: str,
-    nat_to_hum: str,
-    seq_hum: str,
-    is_VHH: bool,
-    name_seq: str,
-    seq_ref: str,
-    name_seq_ref: str,
-    folder_save: str,
-    keep_gaps: bool = False,
-    legend_type: str = "HUM",
-) -> None:
-    """
+    title_legend = '$\\bf{' + name_seq.replace('_','~') + '}$'
+    axs[id_subplot].legend(loc='lower left', title=title_legend, frameon=True,  edgecolor = 'w', 
+                           title_fontsize=28, fontsize=25, framealpha=0.5)
+
+def score_and_plot_abnativ_profile_with_ref(seq_wt: str, nat_to_hum:str, seq_hum: str, is_VHH: bool, name_seq: str, 
+                                            seq_ref: str, name_seq_ref: str, folder_save: str, keep_gaps:bool=False,
+                                            legend_type:str ='HUM') -> None:
+    '''
     Plot the AbNatiV VHH and VH profiles of the WT and humanised sequences
     along with a ref sequence if provided.
 
     Parameters
     ----------
         - seq_wt: str
             Unaligned WT sequence
         - nat_to_hum: str
             Type of humanness to improve nativeness into (VH, VKappa, VLambda).
         - seq_hum: str
             Unaligned humanised sequence
         - is_VHH: bool
             If True, considers the VHH seed for the alignment, more suitable when aligning nanobody sequences
         - name_seq: str
-        - seq_ref: str
+        - seq_ref: str 
             If None, does not plot any references in the profiles. If str, will plot it
         - name_seq_ref: str
         - folder_save: str
             Directory where to save the figures
         - keep_gaps: bool
             If True will plot with gaps
         - legend_type: str
-            'HUM' or 'GRAFT'
-
-    """
-
-    sns.set(font_scale=2.2)
-    sns.set_style(
-        "white",
-        {
-            "axes.spines.right": False,
-            "axes.spines.top": True,
-            "axes.spines.bottom": False,
-            "xtick.bottom": False,
-            "xtick.top": True,
-            "ytick.left": True,
-            "xtick.labeltop": True,
-        },
-    )
-
-    nb_plots = 1
-    if is_VHH:
-        nb_plots = 2
-    if seq_ref is not None:
-        fig, axs = plt.subplots(nb_plots, figsize=(40, 8 * nb_plots))
-    else:
-        l = len(seq_wt.replace("-", ""))
-        length = 40 * l / 149
-        fig, axs = plt.subplots(nb_plots, figsize=(length, 8 * nb_plots))
+            'HUM' or 'GRAFT' 
+            
+    '''
+    
+
+    sns.set(font_scale = 2.2)
+    sns.set_style('white', {'axes.spines.right':False, 'axes.spines.top': True, 'axes.spines.bottom': False,
+                                    'xtick.bottom': False,'xtick.top': True, 'ytick.left': True, 'xtick.labeltop':True})
+    
+    nb_plots=1
+    if is_VHH: nb_plots=2
+    if seq_ref is not None : fig, axs = plt.subplots(nb_plots, figsize=(40,8*nb_plots))
+    else :
+        l = len(seq_wt.replace('-',''))
+        length = 40*l/149
+        fig, axs = plt.subplots(nb_plots, figsize=(length, 8*nb_plots))
 
     # Plot VH
     if not is_VHH:
-        score_and_subplot_profile_with_ref(
-            [axs],
-            0,
-            seq_wt,
-            seq_hum,
-            seq_ref,
-            nat_to_hum,
-            name_seq,
-            name_seq_ref,
-            is_VHH,
-            keep_gaps=keep_gaps,
-            legend_type=legend_type,
-        )
+        score_and_subplot_profile_with_ref([axs], 0, seq_wt, seq_hum, seq_ref, nat_to_hum, name_seq, name_seq_ref, is_VHH,keep_gaps=keep_gaps,legend_type=legend_type)
     else:
-        score_and_subplot_profile_with_ref(
-            axs,
-            0,
-            seq_wt,
-            seq_hum,
-            seq_ref,
-            nat_to_hum,
-            name_seq,
-            name_seq_ref,
-            is_VHH,
-            keep_gaps=keep_gaps,
-            legend_type=legend_type,
-        )
-        score_and_subplot_profile_with_ref(
-            axs,
-            1,
-            seq_wt,
-            seq_hum,
-            seq_ref,
-            "VHH",
-            name_seq,
-            name_seq_ref,
-            is_VHH,
-            keep_gaps=keep_gaps,
-            legend_type=legend_type,
-        )
+        score_and_subplot_profile_with_ref(axs, 0, seq_wt, seq_hum, seq_ref, nat_to_hum, name_seq, name_seq_ref, is_VHH,keep_gaps=keep_gaps,legend_type=legend_type)
+        score_and_subplot_profile_with_ref(axs, 1, seq_wt, seq_hum, seq_ref, 'VHH', name_seq, name_seq_ref, is_VHH,keep_gaps=keep_gaps,legend_type=legend_type)
 
     plt.tight_layout()
-    plt.savefig(
-        os.path.join(folder_save, f"abnativ_hum_profiles_{name_seq}.png"),
-        dpi=250,
-        bbox_inches="tight",
-    )
+    plt.savefig(os.path.join(folder_save, f'abnativ_hum_profiles_{name_seq}.png'), dpi=250, bbox_inches='tight')
+
```

## abnativ/model/alignment/mybio.py

```diff
@@ -1,11 +1,13 @@
 """
  Copyright 2023. Aubin Ramon and Pietro Sormanni. CC BY-NC-SA 4.0
 """
 
+# (c) 2023 Sormannilab
+
 from Bio import SeqIO
 from Bio.SeqIO import FastaIO
 from Bio.Seq import Seq
 from Bio.SeqRecord import SeqRecord
 
 from Bio import Align
 from Bio.Align import substitution_matrices  # substitution_matrices.load("BLOSUM62")
@@ -18,23 +20,25 @@
 import time
 from numpy import sqrt
 import numpy
 import multiprocessing
 
 from typing import Tuple
 
+from scipy.spatial.distance import squareform
+
 from . import misc, csv_dict, plotter, structs
 from .aho_consensus import VH_consensus_no_gaps, VH_conservation_index, VL_consensus_no_gaps, VL_conservation_index, VKappa_consensus_no_gaps
 from .aho_consensus import VKappa_conservation_index, VLambda_consensus_no_gaps, VLambda_conservation_index, VHH_consensus_no_gaps, VHH_conservation_index
 from .blossum import blosum_alph, distance_blosum_normalised, distance_blosum_normalised_gap_to_zero
 from .liabilities import chemical_liabilities, cdr_liabilities
 
 from traceback import print_exc
 
-
+from tqdm import tqdm
 
 def subset_sum(numbers, target, partial=[], partial_sum=0):
     """
     find all combinations of numbers that sum to target
     """
     if partial_sum == target:
         yield partial
@@ -308,15 +312,15 @@
             if len(sequence1) > len(sequence2):
                 alignments = aligner(sequence1, sequence2)
                 alignment = sorted(alignments)[0]
             else:
                 alignments = aligner(sequence2, sequence1)
                 alignment = sorted(alignments)[0]
         else:
-
+    
             aligner = Align.PairwiseAligner()
             aligner.mode = 'global'
             aligner.substitution_matrix = matrix_or_function
             aligner.open_gap_score = penalty_to_open_gap
             aligner.extend_gap_score = penalty_to_extend_gap
             alignments = aligner(sequence1, sequence2)
             alignment = sorted(alignments)[0]
@@ -463,64 +467,64 @@
        distance matrix  computed for 1001 vs. 2000000 sequences [only_first_N_sequences=False], took 2078.2 s
        on circe single core (took 20% of RAM):
        distance matrix  computed for 10000 vs. 2000000 sequences [only_first_N_sequences=False], took 28674.9 s (about 8 hours)
        distance matrix  computed for 10000 vs. 2000000 sequences [only_first_N_sequences=False], took 22688.9 s
     '''
     sta=time.time()
     # first covert aligned sequence into indeces of matrix_for_distance, which will be much faster in double loop.
-    mat_inds1 = _alinged_seqs_to_mat_inds(aligned_seqs1, RegionOfInterest_indices=RegionOfInterest_indices,alphabet_dict=alphabet_dict)
-    mat_inds2 = _alinged_seqs_to_mat_inds(aligned_seqs2, RegionOfInterest_indices=RegionOfInterest_indices,alphabet_dict=alphabet_dict)
-
+    mat_inds1 = _alinged_seqs_to_mat_inds(aligned_seqs1, RegionOfInterest_indices=RegionOfInterest_indices,alphabet_dict=alphabet_dict) 
+    mat_inds2 = _alinged_seqs_to_mat_inds(aligned_seqs2, RegionOfInterest_indices=RegionOfInterest_indices,alphabet_dict=alphabet_dict) 
+    
     N1 = len(mat_inds1)
     N2 = len(mat_inds2)
     distance_similarity = numpy.empty((N1,N2))  # allocate your memory
     if not return_number_of_mutations:
         #distance_similarity = (matrix_for_distance[ mat_inds1, mat_inds2 ]) # doen't work unless you amplify mat_inds2 to len of mat_inds1 but perhaps RAM cost outweighs CPU gain
         for j in range(N1):
             distance_similarity[j] = (matrix_for_distance[ mat_inds1[j], mat_inds2 ]).sum(
                 axis=1
             )  # mat.shape[1] is the length of the aligned sequences, so we sum over all residue distances (will be 0 for identical residues as that is no distance)
-
+            
             if only_first_N_sequences and j + 1 >= only_first_N_sequences:
                 distance_similarity = distance_similarity[: j+1]
                 break
         if not quiet:
             print(
                 "distance matrix  computed for %d vs. %d sequences [only_first_N_sequences=%s], took %.1lf s"
                 % (
                     N1,N2,
                     str(only_first_N_sequences),
                     (time.time() - sta),
                 )
-            )
+            ) 
         sys.stdout.flush()
         return distance_similarity
     else : #if return_number_of_mutations:
         # initialise the matrix for the distance of N mutations
         distance_n_mutations = numpy.empty((N1,N2))
         # get the corresponding matrix distance
-        number_of_mutations = _get_matrix_distance_number_of_mutations(alphabet_dict)
+        number_of_mutations = _get_matrix_distance_number_of_mutations(alphabet_dict) 
         for j in range(N1):
-            distance_similarity[j] = (matrix_for_distance[ mat_inds1[j], mat_inds2 ]).sum(axis=1)
+            distance_similarity[j] = (matrix_for_distance[ mat_inds1[j], mat_inds2 ]).sum(axis=1)  
             # mat.shape[1] is the length of the aligned sequences, so we sum over all residue distances (will be 0 for identical residues as that is no distance)
-
+           
             distance_n_mutations[j] = (number_of_mutations[mat_inds1[j], mat_inds2 ]).sum(axis=1)
             if only_first_N_sequences and j + 1 >= only_first_N_sequences:
                 distance_similarity = distance_similarity[: j+1]
                 distance_n_mutations = distance_n_mutations[: j+1]
                 break
         if not quiet:
             print(
                 "distance matrix and distance_n_mutations computed for %d vs. %d sequences [only_first_N_sequences=%s], took %.1lf s"
                 % (
                     N1,N2,
                     str(only_first_N_sequences),
                     (time.time() - sta),
                 )
-            )
+            ) 
         sys.stdout.flush()
         return distance_similarity, distance_n_mutations
 
 def _get_matrix_distance_number_of_mutations(alphabet_dict=blosum_alph):
     '''
     return number_of_mutations   a squared distance of shape (len(alphabet_dict),len(alphabet_dict))
     that can be used for the calculation of the number of mutation between two sequences
@@ -1918,21 +1922,21 @@
 
     al_seq = [*al_seq]
     idx_consv_serine = 25
 
     if al_seq[idx_consv_serine] != '-':
         if verbose: print(f'No need to realign, there is a {al_seq[idx_consv_serine]} at {idx_consv_serine}')
         return ''.join(al_seq), True
-
+    
     for k, res in enumerate(al_seq[idx_consv_serine+1:]):
         posi = k+idx_consv_serine+1
 
-        if posi >= idx_consv_serine + 3:
+        if posi >= idx_consv_serine + 3: 
             return ''.join(al_seq), False
-
+        
         if res != '-':
             al_seq[idx_consv_serine] = res
             al_seq[posi] = '-'
             return ''.join(al_seq), True
 
 
 def clean_anarci_alignment(al, cons_cys_HD=[23,106], del_cyst_misalign=False, add_Nterm_missing='q',try_to_realign_misalignedCys=True, add_C_term_missing='SS',isVHH=False,cons_index_cutoff=0.9 ,consensus_and_consindex_tuple_for_clean=None, warn=True, debug=False, nb_N_gaps=None, check_duplicates=True, verbose=True) :
@@ -1941,27 +1945,28 @@
      or misaligned key Cys residues. (delete those only if del_cyst_misalign=True)
     in AHo numbering at least, VL and VH have both Cys at numbers 23 and 106
      However, for VL one should change the default add_Nterm_missing and add_C_term_missing
     set add_Nterm_missing to None to skip the check of the N terminus
     same for add_C_term_missing
     consensus_seq_for_clean can be a consensus sequence given to clean the alignment,
      if not given it will first use the al.chain_type to use hard-coded one (from AHo scheme)
-    '''
+    '''     
     if len(al)==0 :
         sys.stdout.write("EMPTY alignment in clean_anarci_alignment() nothing to clean\n")
         return al
     consensus_al=None
+    
     if consensus_and_consindex_tuple_for_clean is not None :
         consensus_seq_for_clean, conservation_index= consensus_and_consindex_tuple_for_clean
         if len(consensus_seq_for_clean)!=len(conservation_index) :
             sys.stderr.write("\n***ERROR*** in clean_anarci_alignment provided consensus_and_consindex_tuple_for_clean but len(consensus)=%d while len(cons_index)=%d\n"%(len(consensus_seq_for_clean),len(conservation_index)))
         if len(consensus_seq_for_clean)==len(al.HD) :
             consensus_al=consensus_seq_for_clean
         else :
-            sys.stderr.write("\n***ERROR*** in clean_anarci_alignment provided consensus_and_consindex_tuple_for_clean with consensus of length %d but alignment has legnth of %d. IGNORING consensus_seq_for_clean and trying to go for hardcoded or compute from alignment itself\n\n" % (len(consensus_seq_for_clean),len(al.HD)))
+            sys.stderr.write("\n***ERROR*** in clean_anarci_alignment provided consensus_and_consindex_tuple_for_clean with consensus of length %d but alignment has legnth of %d. IGNORING consensus_seq_for_clean and trying to go for hardcoded or compute from alignment itself\n\n" % (len(consensus_seq_for_clean),len(al.HD)))      
     elif al.chain_type=='H' :
         if len(VH_consensus_no_gaps)== len(al.HD) : # go for the hard-coded
             if isVHH : # not too different but a bit
                 consensus_al=VHH_consensus_no_gaps
                 conservation_index=VHH_conservation_index
             else :
                 consensus_al=VH_consensus_no_gaps
@@ -1974,143 +1979,140 @@
         if len(VLambda_consensus_no_gaps)== len(al.HD) : # go for the hard-coded
             consensus_al=VLambda_consensus_no_gaps
             conservation_index=VLambda_conservation_index
     elif try_to_realign_misalignedCys :
         sys.stderr.write("***WARNING*** clean_anarci_alignment() Alignment chain_type %s not recognised setting try_to_realign_misalignedCys=False\n"%(str(al.chain_type)))
         try_to_realign_misalignedCys=False
     if al.chain_type in ['L','K'] :
-        if add_C_term_missing is not None and 'S' in add_C_term_missing :
+        if add_C_term_missing is not None and 'S' in add_C_term_missing : 
             sys.stderr.write("***WARNING*** clean_anarci_alignment() Alignment chain_type %s LIKELY NOT COMPATIBLE with input add_C_term_missing=%s\n"%(str(al.chain_type),str(add_C_term_missing)))
-        if add_Nterm_missing is not None and 'q' in add_Nterm_missing.lower() :
+        if add_Nterm_missing is not None and 'q' in add_Nterm_missing.lower() : 
             sys.stderr.write("***WARNING*** clean_anarci_alignment() Alignment chain_type %s LIKELY NOT COMPATIBLE with input add_Nterm_missing=%s\n"%(str(al.chain_type),str(add_Nterm_missing)))
     if try_to_realign_misalignedCys and consensus_al is None : # determine consensus of this alignment under scrutiny
         tmp_clea = clean_anarci_alignment(al, cons_cys_HD=cons_cys_HD, add_Nterm_missing=add_Nterm_missing,try_to_realign_misalignedCys=False, add_C_term_missing=add_C_term_missing, warn=False)
         pssm, log2_pssm, conservation_index ,consensus, aa_freq, aa_freq_ignoring_gaps, gap_freqs=tmp_clea.get_pssm(plot=False)
         alph=numpy.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])
         consensus_al=''.join(alph[pssm[1:,].argmax(axis=0)]) # don't consider gaps so don't use consensus from above
-
+    
     al_clean=al.copy(deep=False)
     cys=numpy.array([ al.HD.index(c) if c in al.HD else al.HD.index(str(c)) for c in cons_cys_HD ])
     wrong_cys=0
     fixed_wrong_cys=0
     missing_N_term=0
     fixed_missing_N_term=0
     missing_C_term=0
     fixed_missing_C_term=0
     fixed_wrong_stem_fr1_kappa=0
-    duplicates=0
+    duplicates=0 
     key_indeces=None # in this way we calcualte key_indices only once rather than every time
-    if key_indeces is None :
+    if key_indeces is None : 
         key_indeces=numpy.where(numpy.array(conservation_index) >= cons_index_cutoff)[0]
         key_indeces=numpy.unique(list(key_indeces)+list(cys)) # Returns the sorted unique elements
     nodupli_seqs = set()
     for k in al :
         al_clean[k]=al[k][:]
-        # check duplicates
+        # check duplicates 
         seq = "".join(al_clean[k])
         if check_duplicates:
             if seq not in nodupli_seqs:
                 nodupli_seqs.add(seq)
             else:
                 del al_clean[k]
                 duplicates +=1
                 if warn: sys.stderr.write('Skipping %s as duplicate \n'%(seq))
-
-
                 continue
-
+        
         # check Cysteines
         if len(cons_cys_HD):
             if any(numpy.array(al[k])[cys]!='C') :
                 wrong_cys+=1
                 if try_to_realign_misalignedCys :
                     re_aligned_seq , success = fix_anarci_misaligned_seq( al[k], consensus_al, conservation_index ,seq_name=k,key_indeces=key_indeces,cons_index_cutoff=cons_index_cutoff, debug=debug)
                     if success and all(numpy.array(re_aligned_seq)[cys]=='C'):
                         al_clean[k]=list(re_aligned_seq)[:]
                         fixed_wrong_cys+=1
                     else :
-                        #print("THIS ONE ABOVE\n\n")
-                        if warn :
-                            sys.stderr.write('Attention %s as no cysteines %s at indices %s =IgG numbering %s\n'%(k,str(numpy.array(al[k])[cys]),str(cys),str(cons_cys_HD)))
+                        if warn : 
+                            sys.stderr.write('Attention %s has no cysteines %s at indices %s =IgG numbering %s\n'%(k,str(numpy.array(al[k])[cys]),str(cys),str(cons_cys_HD)))
                             sys.stderr.write('If the position has not been mutated on purpose, consider that the alignment is erroneous which will impact the whole AbNatiV profile.\n')
                         if del_cyst_misalign: del al_clean[k]
-                        continue
+                        continue                    
                 else :
-                    if warn :
-                        sys.stderr.write('Attention %s as no cysteines %s at indices %s =IgG numbering %s\n'%(k,str(numpy.array(al[k])[cys]),str(cys),str(cons_cys_HD)))
+                    if warn : 
+                        sys.stderr.write('Attention %s has no cysteines %s at indices %s =IgG numbering %s\n'%(k,str(numpy.array(al[k])[cys]),str(cys),str(cons_cys_HD)))
                         sys.stderr.write('If the position has not been mutated on purpose, consider that the alignment is erroneous which will impact the whole AbNatiV profile.\n')
                     if del_cyst_misalign: del al_clean[k]
                     continue
 
         # [VKAPPA] Check for last FR1 misalignement with CDR1
         if al.chain_type=='K':
             if al_clean[k][25] == '-':
                 fixed_al_seq, success = fix_anarci_misaligned_serine_kappa(al_clean[k], verbose=True)
                 if success:
                     al_clean[k] = [*fixed_al_seq]
                     fixed_wrong_stem_fr1_kappa+=1
                 else:
-                    if warn:
+                    if warn: 
                         sys.stderr.write('Attention %s as misalignement at the end of FR1 (posi 25), could do realign it. Sequqence is discarded.')
                     del al_clean[k]
 
 
         # check N terminus
         if add_Nterm_missing is not None :
-            n=0
+            n=0 
             while al[k][n]=='-' and n<len(al[k]) : n+=1
-            if n>0 :
+            if n>0 : 
                 missing_N_term+=1
                 if n<=len(add_Nterm_missing) :
-                    n1=0
-                    while al[k][n1]=='-' and n1<len(al[k]) :
+                    n1=0 
+                    while al[k][n1]=='-' and n1<len(al[k]) : 
                         al_clean[k][n1]=add_Nterm_missing[n1]
                         n1+=1
                     if warn : sys.stderr.write('Added %s at N terminus of %s to replace %d gaps\n'%(add_Nterm_missing[:n1],k,n))
                     fixed_missing_N_term+=1
                 else :
                     if warn : sys.stderr.write('Skipping %s as too many (%d) gaps at N-terminus %s\n'%(k,n,str(al[k][:n+2])))
                     del al_clean[k]
                     continue
 
         if add_Nterm_missing is None and type(nb_N_gaps)==int:
-            n = 0
+            n = 0 
             while al[k][n]=='-' and n<len(al[k]) : n+=1
             if n >nb_N_gaps:
                 if warn : sys.stderr.write('Skipping %s as too many (%d) gaps at N-terminus %s\n'%(k,n,str(al[k][:n+2])))
                 del al_clean[k]
                 missing_N_term=+1
                 continue
 
         # check C terminus
         if add_C_term_missing and al.chain_type in ['H']:
-            n=0
+            n=0 
             while al[k][-1-n]=='-' and n<len(al[k]) : n+=1
-            if n>0 :
+            if n>0 : 
                 missing_C_term+=1
                 if n<=len(add_C_term_missing) :
-                    n1=0
-                    while al[k][-1-n1]=='-' and n1<len(al[k]) :
+                    n1=0 
+                    while al[k][-1-n1]=='-' and n1<len(al[k]) : 
                         al_clean[k][-1-n1]=add_C_term_missing[-1-n1]
                         n1+=1
                     if warn : sys.stderr.write('Added %s at C terminus of %s to replace %d gaps\n'%(add_C_term_missing[-n1:],k,n))
                     fixed_missing_C_term+=1
                 else :
                     if warn : sys.stderr.write('Skipping %s as too many (%d) gaps at C-terminus %s\n'%(k,n,str(al[k][-n-2:])))
                     del al_clean[k]
                     continue
         if add_C_term_missing and al.chain_type in ['L','K']:
-            n=0
-            while al[k][-2-n]=='-' and n<len(al[k])-1 :
+            n=0 
+            while al[k][-2-n]=='-' and n<len(al[k])-1 : 
                 n+=1 #always have a missing C-term residue
-            if n>0 :
+            if n>0 : 
                 missing_C_term+=1
                 if n<=len(add_C_term_missing) :
-                    n1=0
-                    while al[k][-2-n1]=='-' and n1<len(al[k])-1:
+                    n1=0 
+                    while al[k][-2-n1]=='-' and n1<len(al[k])-1: 
                         al_clean[k][-2-n1]=add_C_term_missing[-1-n1]
                         n1+=1
                     if warn : sys.stderr.write('Added %s at C terminus of %s to replace %d gaps\n'%(add_C_term_missing[-n1:],k,n))
                     fixed_missing_C_term+=1
                 else :
                     if warn : sys.stderr.write('Skipping %s as too many (%d) gaps at C-terminus %s\n'%(k,n,str(al[k][-n-2:])))
                     del al_clean[k]
@@ -2122,15 +2124,15 @@
         sys.stdout.write(" missing_N_term=%d (%.1lf%% of total), fixed %d of them (%.1lf%% of wrong ones)\n" % (missing_N_term,100.*missing_N_term/len(al),fixed_missing_N_term, 100.*fixed_missing_N_term/missing_N_term ))
     if add_C_term_missing is not None  and missing_C_term > 0 and warn :
         sys.stdout.write(" missing_C_term=%d (%.1lf%% of total), fixed %d of them (%.1lf%% of wrong ones)\n" % (missing_C_term,100.*missing_C_term/len(al),fixed_missing_C_term, 100.*fixed_missing_C_term/missing_C_term ))
     if duplicates > 0 and warn :
         sys.stdout.write(" duplicates=%d (%.1lf%% of total))\n" % (duplicates,100.*duplicates/len(al)))
     if fixed_wrong_stem_fr1_kappa > 0 and warn :
         sys.stdout.write("(VKappa only) fixed_wrong_stem_fr1_kappa=%d (%.1lf%% of total)\n" % (fixed_wrong_stem_fr1_kappa,100.*fixed_wrong_stem_fr1_kappa/len(al)))
-
+    
     return al_clean
 
 
 
 
 
 class Anarci_alignment(OrderedDict):
@@ -2206,19 +2208,19 @@
                     self.consensus= VH_consensus_no_gaps
                     self.conservation_index=VH_conservation_index
             elif self.chain_type =='K' :
                 self.consensus= VKappa_consensus_no_gaps
                 self.conservation_index=VKappa_conservation_index
             elif self.chain_type =='L' :
                 self.consensus= VLambda_consensus_no_gaps
-                self.conservation_index=VLambda_conservation_index
-
+                self.conservation_index=VLambda_conservation_index 
+    
     def shape(self):
         return (len(self), len(self.HD))
-
+    
     def warn(self, message, out=sys.stderr):
         """
         print warning message'
         """
         out.write("*WARNING* " + message)
         out.flush()
 
@@ -2975,15 +2977,15 @@
                     sys.stderr.write("*Potential Warning* to fix_anarci_misaligned_seq setting consensus to hard-coded VH_consensus_no_gaps and VH_conservation_index which are human, for chain_type H (note VHH may have different consensus!)\n")
                 elif self.chain_type =='K' :
                     self.consensus= VKappa_consensus_no_gaps
                     self.conservation_index=VKappa_conservation_index
                     sys.stderr.write("*Potential Warning* to fix_anarci_misaligned_seq setting consensus to hard-coded VKappa_consensus_no_gaps and VKappa_onservation_index which are human for chain_type K (kappa light)\n")
                 elif self.chain_type =='L' :
                     self.consensus= VLambda_consensus_no_gaps
-                    self.conservation_index=VLambda_conservation_index
+                    self.conservation_index=VLambda_conservation_index 
                     sys.stderr.write("*Potential Warning* to fix_anarci_misaligned_seq setting consensus to hard-coded VLambda_consensus_no_gaps and VLambda_onservation_index which are human for chain_type L (lambda light)\n")
             if self.consensus is not None:
                 if self.conservation_index is not None and len(
                     self.conservation_index
                 ) == len(self.consensus) == len(self.HD):
                     re_aligned_seq, success = fix_anarci_misaligned_seq(
                         self[name],
@@ -3020,15 +3022,15 @@
         if return_seqind_to_schemnum:
             return added, seqind_to_schemnum
         if return_seqind_regions:
             return added, seqind_regions
         if len(self.HD) != len(self.seq_region):
             self.HD_to_seq_region()
         return added
-
+    
     def numpymat(self, only_k=None, remove_cdr_if_separate=True):
         """
         saves in self.mat the numpy matrix corresponding to the whole aligment (no HD)
         """
         if only_k is None:
             only_k = self
         self.mat = numpy.array([self[k] for k in only_k])
@@ -3519,26 +3521,26 @@
         if pssm_file_save_k is not None :
             other_columns={ 'Fv_region':self.seq_region, self.scheme+' numbering':self.HD}
             if key_seq_name is None :
                 key_seq_name='Consensus'
                 key_seq=consensus
             else :
                 other_columns['Consensus']=consensus
-            print_pssm( pssm,
+            print_pssm( pssm, 
                         pssm_file_save_k+'PWM_frequencypssm.txt',
                         sequence_numbering=None,
                         sequence_numbering_name=None,
                         key_sequence=key_seq,
                         key_seq_name=key_seq_name,
                         conservation_index=conservation_index,
                         Nseqs=len(self),
                         other_columns=other_columns,
                         round_to=5,
                     )
-            print_pssm( log2_pssm,
+            print_pssm( log2_pssm, 
                         pssm_file_save_k+'PSSM_loglikelihoodpssm.txt',
                         sequence_numbering=None,
                         sequence_numbering_name=None,
                         key_sequence=key_seq,
                         key_seq_name=key_seq_name,
                         conservation_index=conservation_index,
                         Nseqs=len(self),
@@ -4295,16 +4297,16 @@
     query_key_seq can be given in case not all of it aligned to generate a pssm (i.e. local alignment)
        in which case the returned aa_frequency_by_index will be a list of the same length as query_key_seq with None
        in those positions where no pssm info was available.
     return aa_frequency_by_index   a list of OreredDicts where index in the list are index in the alignment/key_seq
      keys of individual amino acids are residue type (as in  pssm_rows) while values are frequencies from the pssm
      if sort it sorts these OrderedDict from most abundant to less abundant residue (or gap)
     if return_retained_pssm_inds (works only wiht both query_key_seq and aligned_key_seq given)
-    return aa_frequency_by_index,retained_pssm_inds  where retained_pssm_inds is a list as long as
-     query_key_seq (that is as long as aa_frequency_by_index) containing either None for positions with no PSSM
+    return aa_frequency_by_index,retained_pssm_inds  where retained_pssm_inds is a list as long as 
+     query_key_seq (that is as long as aa_frequency_by_index) containing either None for positions with no PSSM 
      info or j where j is the index of the PSSM that correspond to that position.
      useful when there are other pssm-shaped array to process, such as antibody numbering or similar
     """
     # print("DEB: aligned_key_seq=",aligned_key_seq)
     # print("DEB: query_key_seq  =",query_key_seq,aligned_key_seq==query_key_seq, len(aligned_key_seq),pssm.shape)
     if pssm_rows is None:
         if pssm.shape[0] == 21:
@@ -4597,15 +4599,15 @@
         sequence_numbering_name = ""
     if Nseqs is not None :
         key_column_hd_name='Nseqs=%d|%s'%(Nseqs,sequence_numbering_name)
     else :
         key_column_hd_name=sequence_numbering_name
         if key_column_hd_name=='' :
             key_column_hd_name='Position'
-
+    
     if type(out_file) is str:
         out_file = open(out_file, "w")
         closeit = True
     if key_sequence is not None and len(key_sequence) != pssm.shape[1]:
         sys.stderr.write(
             "**ERROR** in print_pssm given key_sequence of length %d but pssm has shape %s, not printing key_sequence column\n"
             % (len(key_sequence), str(pssm.shape))
@@ -4968,15 +4970,15 @@
     return log2likelihood_pssm
     pssm is a matrix of frequencies, also called PWM
     pssm sholud not have a row for gaps and ideally should not contain any 0 (made with pseudocounts)
     background=='auto' gets background frequency as overall frequency in alignment background=pssm.mean(axis=1).
     background=='uniform' assign uniform frequency
     background== np.array of length of pssm (usually 21) for custom background frequencies (must be in same order as pssm)
     note that consensus sequence may differ if calculated from frequencies or log-likelihood
-    value_for_zerofreqs=True will set the log-likelihood of items with zero frequency to
+    value_for_zerofreqs=True will set the log-likelihood of items with zero frequency to 
       2 int units below the minimum value from non-zero (e.g. if min o observed is -11.68 -> then non observed get -13)
     """
     if background is None:
         background = "auto"
     if type(background) is str:
         if background == "auto":
             background = pssm.mean(axis=1)  # mean frequency in alignment
@@ -5164,20 +5166,20 @@
 
 
 
 def renumber_Fv_pdb_file(pdbf, H, L ,scheme='IMGT',outfilename=None) :
     '''
     If L=None, will only consider the Heavy chain (suitable for VHHs)
     return outfilename,pdb_respos_to_IMGT,IMGT_to_pdb_respos,Chains_Fv_res,original_polymer
-    H and L are ids of heavy and light (only one heavy and one light currently supported
+    H and L are ids of heavy and light (only one heavy and one light currently supported 
         (can run multiple time by giving output file as input and new set of H and L))
     uses anarci
     create scheme (e.g. IMGT) numbered file printed to outfilename
     when it's None outfilename = pdb_id+scheme+'.pdb'
-    Note IMGT is a bit weird (check on structure downloaded from SabDab) for
+    Note IMGT is a bit weird (check on structure downloaded from SabDab) for 
      example it has 111, 111A, 112A, 112 (in this order, see 7so5)
     '''
     if outfilename is None :
         outfilename=pdb_id+scheme+'.pdb'
     patht,pdb_id,ext=misc.get_file_path_and_extension(pdbf)
 
     polymer,sruct, ovmap = structs.pdb_to_polymer(pdbf)
@@ -5216,20 +5218,20 @@
                         out.write('REMARK   6 Fv residues renumbered with %s numbering scheme [%s:%s and %s:%s]\n'%(scheme,H,chty,L,chtyL))
                     else:
                         out.write('REMARK   6 Fv residues renumbered with %s numbering scheme [%s:%s]\n'%(scheme,H,chty))
                     remark_added=True
                 ch=line[21]
                 if ch in Chains_Fv_res : # chain ID
                     respos = line[22:27].strip()
-                    try :
+                    try : 
                         respos = int(respos)
                         if ch not in max_respos :
                             max_respos[ch]=respos
                             delta=None
-                    except Exception :
+                    except Exception : 
                         pass
                     if respos in pdb_respos_to_IMGT[ch] :
                         pdb_respos_to_IMGT[ch][respos]
                         new_respos = '%5s'%(str(pdb_respos_to_IMGT[ch][respos][0])+str(pdb_respos_to_IMGT[ch][respos][1])) # second most often ' '
                         line = line[:22]+new_respos+line[27:]
                         if type(respos) is int and respos > max_respos[ch] :
                             max_respos[ch]=respos
@@ -5307,64 +5309,64 @@
 
 
 
 def get_sequence_liabilities(sequence, cdr_index_range=None, return_summary_line=True,summary_line_score_cutoff=0. , chemical_liabilities=chemical_liabilities, cdr_liabilities=cdr_liabilities):
     '''
     cdr_index_range can be a dict whose keys are various cdr names ("CDRH3",...) and values are list with  range e.g. [12, 30] in sequence indices included.
        or it can be just a list with one range.
-       if you use it to process a cdr you could give cdr_index_range='all'
+       if you use it to process a cdr you could give cdr_index_range='all' 
     return found_liabilities,found_CDR_liabilities,summary_line,overall_liability_score if return_summary_line=True
     return found_liabilities,found_CDR_liabilities   both are csv_dict.Data classes
     can merge by
     found_liabilities.vstack(found_CDR_liabilities)
     and filter most relevnat by
     liabilities= found_liabilities.group_by_condition('liability score',lambda x : x>=1)
-
+    
     summary_line example: (number are index in sequence)
     '31DT:isomerization;55NG:deamidation;62DS:isomerization;73DT:isomerization;77NT:deamidation;84NS:deamidation;90DT:isomerization;90n1C:disulphide-dimerisation;90n2G:poor-specificity;134NH:deamidation;134NxS:N-glycosylation;'
     '''
     HD=['location (sequence index)','sequence motif', 'liability', 'specific location','location type',  'liability score' , 'incremental liability score', 'reference']
     found_liabilities=csv_dict.Data()
     found_liabilities.hd=csv_dict.list_to_dictionary(HD)
     n=0
     for j, aa in enumerate(sequence) :
-        if j>1 :
+        if j>1 : 
             k1=sequence[j-2]+'x'+aa
-            if k1 in chemical_liabilities :
+            if k1 in chemical_liabilities : 
                 found_liabilities[n]=[j-2, k1]+chemical_liabilities[k1]
                 n+=1
             k1=sequence[j-2:j+1]
-            if k1 in chemical_liabilities :
+            if k1 in chemical_liabilities : 
                 found_liabilities[n]=[j-1, k1]+chemical_liabilities[k1]
                 n+=1
         if j>0 :
             k1=sequence[j-1:j+1]
-            if k1 in chemical_liabilities :
+            if k1 in chemical_liabilities : 
                 found_liabilities[n]=[j-1, k1]+chemical_liabilities[k1]
                 n+=1
     found_CDR_liabilities=csv_dict.Data() # keep same n as keys so can readily vstack
     if cdr_index_range is not None :
         found_CDR_liabilities.hd=csv_dict.list_to_dictionary(HD)
         if cdr_index_range=='all' : cdr_index_range= {'any': [0, len(sequence)]}
         elif not hasattr(cdr_index_range, 'keys') : cdr_index_range={'CDR': cdr_index_range }
         for cdk in cdr_index_range :
-            cdr = sequence[ cdr_index_range[cdk][0] : cdr_index_range[cdk][-1]+1 ] # include end
+            cdr = sequence[ cdr_index_range[cdk][0] : cdr_index_range[cdk][-1]+1 ] # include end 
             spec_loc=False # check if specific CDR names are given, in which case add only CDR-relevant liabilities
             if '1' in cdk or '2' in cdk or '3' in cdk : spec_loc=True
             for k in cdr_liabilities :
-                if spec_loc and not (('H' in cdk or 'L' in cdk) and cdr_liabilities[k][1][3:].upper()==cdk[3:].upper()) and not (('H' not in cdk and 'L' not in cdk and ('1' in cdk or '2' in cdk or '3' in cdk)) and cdr_liabilities[k][1][4:].upper()==cdk[3:].upper() ) :
+                if spec_loc and not (('H' in cdk or 'L' in cdk) and cdr_liabilities[k][1][3:].upper()==cdk[3:].upper()) and not (('H' not in cdk and 'L' not in cdk and ('1' in cdk or '2' in cdk or '3' in cdk)) and cdr_liabilities[k][1][4:].upper()==cdk[3:].upper() ) : 
                     continue
                 f= cdr.find(k)
                 if f>0  :
                     found_CDR_liabilities[n]=[ cdr_index_range[cdk][0]+f, k ] + cdr_liabilities[k]
                     found_CDR_liabilities[n][found_CDR_liabilities.hd['location type']]=cdk
                     n+=1
                 if k[0]=='n' :
                     N=cdr.count(k[2:])
-                    if N >= int(k[1]) :
+                    if N >= int(k[1]) : 
                         found_CDR_liabilities[n]=[ cdr_index_range[cdk][0] , ('n%d'%(N))+k[2:] ] + cdr_liabilities[k] # set to start of range
                         found_CDR_liabilities[n][found_CDR_liabilities.hd['location type']]=cdk
                         found_CDR_liabilities[n][found_CDR_liabilities.hd['incremental liability score']]*=(N-int(k[1]) +1) # increment score according to number of liabilities found in this CDR
                         n+=1
     if return_summary_line :
         summary_line=''
         overall_liability_score=0
@@ -5381,75 +5383,84 @@
 def distance_matrix_from_aligned_sequences(aligned_seqs, matrix_for_distance=distance_blosum_normalised, alphabet_dict=blosum_alph, RegionOfInterest_indices=None,only_first_N_sequences=False, return_number_of_mutations=False, quiet=False ):
     '''
     this function calculate a distance matrix in a way rather optimised for speed
      returned only as flattend triangle of symmetrix matrix, if needed one can use squareform
             (from scipy.spatial.distance import squareform) to make it square
     if only_first_N_sequences is given it will return a non-square matrix of shape
         (only_first_N_sequences,len(aligned_seqs))
-    matrix_for_distance: the calculation is based on the inter-residue distances given in matrix_for_distance
+    matrix_for_distance: the calculation is based on the inter-residue distances given in matrix_for_distance 
     note that the expectation is that diagonal elements are 0 (0 distance among identical residues).
     alphabet_dict is like {'A':0,'C':1,...} with the indices of each amino acid in the given matrix_for_distance
-    RegionOfInterest_indices can be given as those indices corresponding to the alignment column to be used for distance calculations
+    RegionOfInterest_indices can be given as those indices corresponding to the alignment column to be used for distance calculations 
      (this is useful to calculate the distance excluding specific regions, e.g. CDR3 of antibodies that are anyway highly diverse)
     return distance_similarity
+    or if return_number_of_mutations=True
     return_number_of_mutations return a 'distance' that is the total number of mutations
-        return distance_similarity,distance_n_mutations
     '''
     sta=time.time()
-    mat_inds=[] # first covert aligned sequence into indeces of matrix_for_distance, which will be much faster in double loop.
-    if RegionOfInterest_indices is not None :
+
+    # First covert aligned sequence into indeces of matrix_for_distance, which will be much faster in double loop.
+    mat_inds=[]
+    if RegionOfInterest_indices is not None : 
         RegionOfInterest_indices=numpy.array(RegionOfInterest_indices)
         for r in aligned_seqs :
             mat_inds+=[ [ alphabet_dict[aa.upper()] for aa in numpy.array(list(r))[RegionOfInterest_indices] ] ] # NOTE .upper() for q and other possible guesses
     else :
         for r in aligned_seqs :
             mat_inds+=[ [ alphabet_dict[aa.upper()] for aa in r ] ] # NOTE .upper() for q and other possible guesses
-    N=len(mat_inds)
-    distance_similarity = numpy.empty((N*(N-1))//2) # allocate your memory
-    n=0
+    
+    N = len(mat_inds)
+
+    # Allocate minimum required memory
+    if only_first_N_sequences and only_first_N_sequences < N:
+        max_pairs = only_first_N_sequences * N #All N-first-pairs
+    else:
+        max_pairs = N * (N - 1) // 2 #Flatenned upper triangle
+    
+    distance_similarity = numpy.empty(max_pairs, dtype=numpy.float16) 
+    
     if return_number_of_mutations :
-        distance_n_mutations= numpy.empty((N*(N-1))//2)
-        number_of_mutations=numpy.ones(matrix_for_distance.shape)
+        number_of_mutations=numpy.ones(matrix_for_distance.shape, dtype=numpy.float16)
         numpy.fill_diagonal(number_of_mutations,0)
-        if 'X' in alphabet_dict :
+        if 'X' in alphabet_dict : 
             number_of_mutations[alphabet_dict['X'],alphabet_dict['X']]=1
-        for j in range(len(mat_inds)-1) :
-            distance_similarity[ n:n+N-1-j] = (matrix_for_distance[ mat_inds[j+1:], mat_inds[j]] ).sum(axis=1) # mat.shape[1] is the length of the aligned sequences
-            distance_n_mutations[ n:n+N-1-j] = (number_of_mutations[ mat_inds[j+1:], mat_inds[j]] ).sum(axis=1)
-            if only_first_N_sequences and j+1 >= only_first_N_sequences :
-                distance_similarity=distance_similarity[:n+N-1-j]
-                distance_n_mutations=distance_n_mutations[:n+N-1-j]
-                Md=numpy.zeros((only_first_N_sequences,N))
-                Mn=numpy.zeros((only_first_N_sequences,N))
-                ns=0
-                for js in range(only_first_N_sequences) :
-                    Md[js][js+1:]=  distance_similarity[ns:ns+N-1-js]
-                    Mn[js][js+1:]=  distance_n_mutations[ns:ns+N-1-js]
-                    ns+= N-1-js
-                Md[:,:only_first_N_sequences]+=Md[:,:only_first_N_sequences].T
-                Mn[:,:only_first_N_sequences]+=Mn[:,:only_first_N_sequences].T
-                distance_similarity=Md
-                distance_n_mutations=Mn
-                break
-            n+=N-1-j
-        if not quiet :
-            print("distance matrix and distance_n_mutations computed for %d sequences [only_first_N_sequences=%s], took %s s"%(len(aligned_seqs),str(only_first_N_sequences),str(time.time()-sta))) # for 4-mer 1.6*10^5 sequences took 388.76327562332153 on circe
-        return distance_similarity,distance_n_mutations
-    else :
-        for j in range(len(mat_inds)-1) :
-            distance_similarity[ n:n+N-1-j] = (matrix_for_distance[ mat_inds[j+1:], mat_inds[j]] ).sum(axis=1) # mat.shape[1] is the length of the aligned sequences
-            if only_first_N_sequences and j+1 >= only_first_N_sequences :
-                distance_similarity=distance_similarity[:n+N-1-j]
-                Md=numpy.zeros((only_first_N_sequences,N))
-                ns=0
-                for js in range(only_first_N_sequences) :
-                    Md[js][js+1:]=  distance_similarity[ns:ns+N-1-js]
-                    ns+= N-1-js
-                Md[:,:only_first_N_sequences]+=Md[:,:only_first_N_sequences].T
-                distance_similarity=Md
-                break
-            n+=N-1-j
-        if not quiet :
-            print("distance matrix computed for %d sequences [only_first_N_sequences=%s], took %s s"%(len(aligned_seqs),str(only_first_N_sequences),str(time.time()-sta)))  # for 4-mer 1.6*10^5 sequences took 388.76327562332153 on circe
-        return distance_similarity
+
+    # Adjust the upper limit of the loop to handle only_first_N_sequences
+    max_j = only_first_N_sequences if only_first_N_sequences and only_first_N_sequences < N else N - 1
+    n=0
+
+    print('\n\n->>> Compute distance')
+    for j in tqdm(range(max_j)):
+        indices = slice(n, n+N-1-j)
+
+        if return_number_of_mutations:
+            distance_similarity[indices] = number_of_mutations[mat_inds[j+1:], mat_inds[j]].sum(axis=1)
+        else: 
+            distance_similarity[indices] = matrix_for_distance[mat_inds[j+1:], mat_inds[j]].sum(axis=1) #Sum over each residue
+        
+        n += N-1-j # Triangular search
+        
+    if only_first_N_sequences and only_first_N_sequences < N: 
+        Md = numpy.zeros((only_first_N_sequences, N))
+        # Fill the full matrices for the first N sequences
+        index = 0
+        for i in range(only_first_N_sequences):
+            for k in range(i + 1, N):
+                Md[i, k] = distance_similarity[index]
+                index += 1
+        
+        # Transform it into full matrix
+        Md[:,:only_first_N_sequences] += Md[:,:only_first_N_sequences].T
+
+    else:
+        print('->Doing distance squareform')
+        Md = squareform(distance_similarity)
+
+    if not quiet :
+            print("distance matrix and distance_n_mutations computed for %d sequences [only_first_N_sequences=%s], took %s s"%(len(aligned_seqs),str(only_first_N_sequences),str(time.time()-sta))) # for 4-mer 1.6*10^5 sequences took 388.76327562332153 on circe 
+    
+    return Md
+    
+
+
```

## abnativ/scoring.py

```diff
@@ -21,20 +21,20 @@
 
     ## DATA SCORING ##
     batch_size = 128
     abnativ_df_mean, abnativ_df_profile = abnativ_scoring(args.nativeness_type, args.input_filepath_or_seq, batch_size, args.mean_score_only,
                                                           args.do_align, args.is_VHH, args.is_plotting_profiles, args.output_directory, args.output_id)
 
     ## DATA SAVING ##
-    #print(f'\n-> Scores being saved in {args.output_directory}\n')
+    print(f'\n-> Scores being saved in {args.output_directory}\n')
     abnativ_df_mean.to_csv(os.path.join(args.output_directory, f'{args.output_id}_abnativ_seq_scores.csv'))
     if not args.mean_score_only:
         abnativ_df_profile.to_csv(os.path.join(args.output_directory, f'{args.output_id}_abnativ_res_scores.csv'))
     if args.is_plotting_profiles:
         save_profile_fp = os.path.join(args.output_directory, f'{args.output_id}_profiles')
-        #print(f'\n-> Profile plots saved in {save_profile_fp}\n')
+        print(f'\n-> Profile plots saved in {save_profile_fp}\n')
```

## Comparing `abnativ-1.1.1.dist-info/LICENSE` & `abnativ-1.1.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `abnativ-1.1.1.dist-info/METADATA` & `abnativ-1.1.3.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 Metadata-Version: 2.1
 Name: abnativ
-Version: 1.1.1
+Version: 1.1.3
 Summary: AbNatiV: a VQ-VAE-based assessment of the nativeness of antibodies.
 Home-page: https://gitlab.developers.cam.ac.uk/ch/sormanni/abnativ
 License: CC BY-NC-SA 4.0
 Author: Aubin Ramon
 Requires-Python: >=3.8.0,<3.9.0
 Classifier: License :: Other/Proprietary License
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
-Requires-Dist: biopython (>=1.79,<2.0)
+Requires-Dist: biopython (>=1.81,<2.0)
 Requires-Dist: brewer2mpl (>=1.4.1,<2.0.0)
 Requires-Dist: chroma (>=0.2.0,<0.3.0)
 Requires-Dist: einops (>=0.7.0,<0.8.0)
 Requires-Dist: freesasa (>=2.2.1,<3.0.0)
 Requires-Dist: immunebuilder (>=1.0.1,<2.0.0)
 Requires-Dist: matplotlib (>=3.7.0,<3.8.0)
+Requires-Dist: mendeleev (==0.12.1)
 Requires-Dist: mlflow (>=2.11.3,<3.0.0)
 Requires-Dist: numpy (>=1.24.0,<1.25.0)
 Requires-Dist: pandas (>=1.0,<2.0)
 Requires-Dist: paretoset (>=1.2.3,<2.0.0)
 Requires-Dist: protein-topmodel (==1.0.1)
 Requires-Dist: python-parallel (>=0.9.1,<0.10.0)
 Requires-Dist: pytorch-lightning (>=2.0,<3.0)
@@ -27,36 +28,45 @@
 Requires-Dist: scipy (>=1.9.0,<1.10.0)
 Requires-Dist: seaborn (>=0.13.2,<0.14.0)
 Requires-Dist: torch (==2.2.2)
 Requires-Dist: tqdm (>=4.66.2,<5.0.0)
 Project-URL: Repository, https://gitlab.developers.cam.ac.uk/ch/sormanni/abnativ
 Description-Content-Type: text/markdown
 
-<div align="center">
-
 # AbNatiV: VQ-VAE-based assessment of antibody and nanobody nativeness for hit selection, humanisation, and engineering
 
 </div>
 
+## License
+
+Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) (see License file).
+This software is not to be used for commerical purposes.
+
 ## Reference
 
-> Original publication: <https://www.nature.com/articles/s42256-023-00778-3>
->
-> bioRxiv preprint at <https://www.biorxiv.org/content/10.1101/2023.04.28.538712v2>
+> Original publication: 
+https://www.nature.com/articles/s42256-023-00778-3
+
 
 ## Presentation
 
 AbNatiV is a deep-learning tool for assessing the nativeness of antibodies and nanobodies,
 i.e., their likelihood of belonging to the distribution
 of immune-system derived human antibodies or camelid nanobodies,
+AbNatiV is a deep-learning tool for assessing the nativeness of antibodies and nanobodies,
+i.e., their likelihood of belonging to the distribution
+of immune-system derived human antibodies or camelid nanobodies,
 which can be exploited to guide antibody engineering and humanisation.
 
 The model is a vector-quantized variational auto-encoder (VQ-VAE) that generates
 an interpretable nativeness score
 and a residue-level nativeness profile for a given input sequence.
+The model is a vector-quantized variational auto-encoder (VQ-VAE) that generates
+an interpretable nativeness score
+and a residue-level nativeness profile for a given input sequence.
 
 * AbNatiV provides a <strong><ins>nativeness score</ins></strong> for each of its 4 default training datasets:\
    &emsp;&emsp; 1. `VH`: human immune-system derived heavy chains,\
    &emsp;&emsp; 2. `VKappa`: human immune-system derived kappa light chains,\
    &emsp;&emsp; 3. `VLambda`: human immune-system derived lambda light chains,\
    &emsp;&emsp; 4. `VHH`: camelid immune-system derived single-domain antibody sequences.
 
@@ -64,67 +74,70 @@
   &emsp;&emsp; 1. <strong>nanobodies</strong>: it employs a dual-control strategy aiming to increase the humanness of the sequence without decreasing its initial VHH-nativenees,\
   &emsp;&emsp; 2. <strong>paired VH/VL</strong>: it directly increases the VH-humanness and VL-humanneess of both sequences.
 
 <strong>A web server for scoring is available at https://www-cohsoftware.ch.cam.ac.uk/index.php/abnativ</strong>
 
 ## Setup AbNatiV
 
+### Automatic conda environment creation (recommended)
+
+The following will create a new conda environment will all of the required packages installed. This option is best for use when AbNatiV is going to be used in a standalone fashion.
+
+```bash
+git clone https://gitlab.developers.cam.ac.uk/ch/sormanni/abnativ.git
+cd abnativ
+
+# This will automatically create the conda environment and install AbNatiV
+./setup_env.sh
+```
+
+If a more complex environment is required, manual installation should be preferred as the automation script may lead to some issues.
+
 
 ### Installation from PyPI (manual)
-Ensure that you have the correct dependancies already installed before installing from the PyPI repository. For `x86_64` is pretty straight forward since all the packages are on `conda`, however, for `arm64`/`Apple Silicon (M1/2/3)` it requires a few extra steps since the packages are not on conda.
+
+> :warning:  **python 3.8 is required**  :warning:
+
+Ensure that you have the correct dependancies already installed before installing from the PyPI repository. For `x86_64 (Step 1a)` is pretty straight forward since all the packages are on `conda`, however, for `arm64`/`Apple Silicon (M1/2/3) (Step 1b)` it requires a few extra steps since the packages are not on conda.
 
 
 The following non-PyPI packages are required:
 
 * `pdbfixer` - availible from `conda-forge`
 * `ANARCI` - availible from `conda-forge/x86_64` 
 
-**x86_64**
+**Step 1a. x86_64**
 ```bash
 # Ensure that conda dependancies are installed
 conda install -c conda-forge pdbfixer
 conda install -c bioconda anarci
 ```
 
-**Apple Silicon**
+**Step 1b. Apple Silicon**
 
 Hmmer and ANARCI need to be installed manually. The easiest way to do this is to use `brew` for `hmmer` and manually installing from github for `ANACRI`. It is also possible to manually install `hmmer` from source if needed. If `hmmer` is already installed, ensure that the `hmmer` binary directory is in `PATH` so that the build tools can find it.
 
 ```bash
 brew install hmmer # Hmmer is not availible on conda for arm64 - use brew instead
 
 conda install -c conda-forge pdbfixer
 conda install -c conda-forge biopython">=1.79.0,<1.80.0" -y
 git clone https://github.com/oxpig/ANARCI.git
 cd ANARCI
 python setup.py install
 ```
 
-**Install AbNatiV**
+**Step 2. Install AbNatiV**
 ```
-# Install from our GitLab PyPI repository
-pip install abnativ --index-url https://gitlab.developers.cam.ac.uk/api/v4/projects/6948/packages/pypi/simple
+# Install from PyPI
+pip install abnativ
+# Download the pretrained models
+abnativ update
 ```
 
-
-### Automatic conda environment creation (recommended)
-
-The following will create a new conda environment will all of the required packages installed. This option is best for use when AbNatiV is going to be used in a standalone fashion.
-
-```bash
-git clone https://gitlab.developers.cam.ac.uk/ch/sormanni/abnativ.git
-cd abnativ
-
-# This will automatically create the conda environment and install AbNatiV
-./setup_env.sh
-```
-
-If a more complex environment is required, manual installation should be preferred as the automation script may lead to some issues.
-
-
 ## AbNatiV command-line interface
 
 ### 1 - Antibody nativeness scoring
 
 To score input antibody sequences, use the `abnativ score` command line. You can plot nativeness profiles using the `-plot` option.
 
 AbNatiV provides an interpretable overall nativeness score, which approaches 1 for highly native sequences and where 0.8 represents the threshold that best separates native from non-native sequences. This score is computed for the whole Fv sequence, but can also be computed for individual CDRs or framework region (closest to 1, highest nativeness).
@@ -273,15 +286,14 @@
 A single-control strategy only is applied. It aims to increase the AbNatiV VH- and VL- hummanness of each sequence separately.
 
 Two sampling methods are available:\
 &emsp; 1. <strong>Enhanced sampling</strong> (default): iteratively explores the mutational space aiming for rapid convergence to generate a single humanised sequence,\
 &emsp; 2. <strong>Exhaustive sampling</strong> (if `-isExhaustive`): assesses all mutation combinations within the available mutational space (PSSM-allowed mutations) and selects the best sequences (Pareto Front). It returns a variant with the highest VH-humanness for each number of mutations that are beneficial to the VH-humanness (i.e., when increasing the number of mutations only increases the humanness).
 
 A `-rasa` of 0 will consider every framework residue for mutation. A `-rasa` of 0.15 will considered only solvent-exposed framework residues (as defined in our paper).
-
 NB: a crystal structure (pdb format) can be included (via the filepath `-pdb`, and the chain IDs `-ch_vh` and `-ch_vl`) to better assess the solvent-exposed surface of the paired chains. If `None`, ABodyBuilder2 will predict the structure to work on. Only cleaned pdb files will be tolerated. If there is an error to process your pdb file, it is recommended to use the ABodyBuilder2 option.
 
 <details>
     <summary>See <strong>abnativ hum_vh_vl</strong> command line description</summary>
 
 ```
 abnativ hum_vh_vl [-h] [-i_vh INPUT_SEQ_VH] [-i_vl INPUT_SEQ_VL] [-odir OUTPUT_DIRECTORY] [-oid OUTPUT_ID] [-VHscore THRESHOLD_ABNATIV_SCORE]
@@ -382,15 +394,15 @@
 Every epoch of the training will be saved in `./checkpoints/<model_name>` and the logs in `./mlruns`.
 The Lightning Pytorch logging can be monitored running `mlflow ui` (see MLflow documentation: https://mlflow.org/docs/1.0.0/tracking.html).
 
 ## Issues
 
 - The installation of OpenMM might create troubles with your device. If you have an `import error` with `lib glibxx_3.4.30`, you could solve it with `export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH`.
 
-If you experience any issues please add an issue to the [Gitlab](https://gitlab.developers.cam.ac.uk/ch/sormanni/abnativ)
+If you experience any issues please add an issue to the [Gitlab](https://gitlab.developers.cam.ac.uk/ch/sormanni/abnativ).
 
 ## Contact
 
 Please contact ar2033@cam.ac.uk to report issues of for any questions.
 
 ## Acknowledgements
```

## Comparing `abnativ-1.1.1.dist-info/RECORD` & `abnativ-1.1.3.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 abnativ/__init__.py,sha256=LNIShNI0NKH3sLGkz9KLmWdEjPK54FdGD14__YOaJu0,75
 abnativ/__main__.py,sha256=oMM7ssXCPSJyVBdq0jvvTnqQNxvkeqLXNUgXCXzCSQA,11743
 abnativ/humanisation/__init__.py,sha256=r7e0M_m6aJjfoiZPoQAULFBybqS2LkaGWfCtkRh1mm8,74
 abnativ/humanisation/dms_utils.py,sha256=Yzg8X490jg4IQu9aXzMpfVE2mGu8ZaNO7IUx5knLs8s,8451
-abnativ/humanisation/humanisation_utils.py,sha256=cMGcO3uefOcUR08uT-xCT2xx9PncgFIhbX606GzR6O8,66934
+abnativ/humanisation/humanisation_utils.py,sha256=Mg9ys23F0q59VVFyA36JyZXg6bb7jmU-rHIBceX2-HQ,60893
 abnativ/humanisation/pssms/VHH_log2_pssm.npy,sha256=dbbuxj-Ygh30tfVe-hH99mMsJzWlVYBSVcPbzpe0b_Q,25160
 abnativ/humanisation/pssms/VHH_pssm.npy,sha256=dbbuxj-Ygh30tfVe-hH99mMsJzWlVYBSVcPbzpe0b_Q,25160
 abnativ/humanisation/pssms/VH_log2_pssm.npy,sha256=6KiCcs7DaOUkeRR-aXqXQTYimOkQemJ9zOm5CI-IsK8,25160
 abnativ/humanisation/pssms/VH_pssm.npy,sha256=6KiCcs7DaOUkeRR-aXqXQTYimOkQemJ9zOm5CI-IsK8,25160
 abnativ/humanisation/pssms/VKappa_log2_pssm.npy,sha256=_xoiZmEPB5267hZihO5ErQDK5aadLReFd0MQ-aIrUTo,25160
 abnativ/humanisation/pssms/VKappa_pssm.npy,sha256=_xoiZmEPB5267hZihO5ErQDK5aadLReFd0MQ-aIrUTo,25160
 abnativ/humanisation/pssms/VLambda_log2_pssm.npy,sha256=GepnMQdaZn_0DgFd0bicRfL7Mg4Cr5BLB9sbdnIJeKw,25160
@@ -18,25 +18,25 @@
 abnativ/model/alignment/__init__.py,sha256=r7e0M_m6aJjfoiZPoQAULFBybqS2LkaGWfCtkRh1mm8,74
 abnativ/model/alignment/aho_consensus.py,sha256=geQrCaREjoKC_8yYxjMoGQBQnanZSxnLbPzzwFSj8QA,9654
 abnativ/model/alignment/align_and_clean.py,sha256=FLJiM_1VbqmOyCTA4qfcsuT0rPAZqRV6dxcQh4MWBdM,7260
 abnativ/model/alignment/blossum.py,sha256=QgbW0MW_sZ6U-NRKYT89seMjvo8I40zXy4Tt4fdQR3w,20256
 abnativ/model/alignment/csv_dict.py,sha256=ie0IQ-fJvS5cqpBht0wsw1TS-STfjf02lOc76E_r4oE,217705
 abnativ/model/alignment/liabilities.py,sha256=cfD5o8NocI4wp-oK1zPO_ZH4BOu-5O_rdORIq6T6Euw,8077
 abnativ/model/alignment/misc.py,sha256=9PJ2XDfK_CF9DYicNzLzXKGH4SfA3sSTWLN3j5_Ge9s,282087
-abnativ/model/alignment/mybio.py,sha256=Tu6vpgkECeZu23Wp6kCgT4Cqo1YAp24pwbqfuWDUt0E,250781
+abnativ/model/alignment/mybio.py,sha256=QsJ9b-r5FqTueRlamiZKywwxzfZ9QPeU78zL3T-28j8,250169
 abnativ/model/alignment/parse_pdb.py,sha256=UKo-akzv5Rx_z8Ltj3SI71yKIUwEVYTbKK5L5LcxTXw,15487
 abnativ/model/alignment/plotter.py,sha256=GXV4uAGccMguPhGz8D5uKfg3QUYjwDcHdhjTD63nWO4,451002
 abnativ/model/alignment/structs.py,sha256=cwVulvwXqOmbd9CnrE3gjwvUFRw4nUv8Tic3Iw88Evs,150143
 abnativ/model/onehotencoder.py,sha256=2-4MH6H6hEZzBwQe_X1QAR6ROjb3CociHvAm16Ub2X0,4587
 abnativ/model/scoring_functions.py,sha256=4X6u9_QXrb6a3fmRH6BitZpsu6HQwapb1srgQWWuPPo,20424
 abnativ/model/utils.py,sha256=l6KDIXbckUCHOYepXlrJFrHPkXOmxBRDoK7y94InMvo,7285
 abnativ/model/vq.py,sha256=rgY05vitaT0JNeT4szdL4L4w5NwcmszPisKPkZ9hYIk,9812
-abnativ/scoring.py,sha256=KmGBh8qPR6yYJfFGNXAGLkCMWfXmMUNtl_0gEj6maaY,1490
+abnativ/scoring.py,sha256=m_zidOffpCiUV3FeO_NnNzHanMwPtjfrabzHfX6_HFw,1488
 abnativ/training.py,sha256=JNOnGe8nGjBJwV8995NXUN-hOBH6n447oOMu8-jNdA0,2845
 abnativ/update.py,sha256=kgrkXFavY9LPSNDEq1mFnsqRsZKggs72gArHczBOj20,1961
 abnativ/vh_vl_humanisation.py,sha256=kwAeZptOmV2yU4n4qGpaX_Nnf87kdckmYZ3LA9nA7Ac,745
 abnativ/vhh_humanisation.py,sha256=i3p6Fue6N6is7FjzLA_SQnKyWUxT0IKVnKDXaWdW-_c,1778
-abnativ-1.1.1.dist-info/LICENSE,sha256=wr5zmFzXyIzY_EIgV4vjiS8mV53TkNMp62WDXE3CwY0,20971
-abnativ-1.1.1.dist-info/METADATA,sha256=bcRVYr_Kvj4BIe2Z7nU-kiwyheAtluWqj1lNoa78ty0,23599
-abnativ-1.1.1.dist-info/WHEEL,sha256=sP946D7jFCHeNz5Iq4fL4Lu-PrWrFsgfLXbbkciIZwg,88
-abnativ-1.1.1.dist-info/entry_points.txt,sha256=E9N9-wD2bnLBchRTpk5Hnp0ZL4LKf5XVKfuzscKVMRw,49
-abnativ-1.1.1.dist-info/RECORD,,
+abnativ-1.1.3.dist-info/LICENSE,sha256=wr5zmFzXyIzY_EIgV4vjiS8mV53TkNMp62WDXE3CwY0,20971
+abnativ-1.1.3.dist-info/METADATA,sha256=FGsd7kaikbw3psruhvlhNYMmXCEL5ZsihhuGV3ZwvRA,24117
+abnativ-1.1.3.dist-info/WHEEL,sha256=sP946D7jFCHeNz5Iq4fL4Lu-PrWrFsgfLXbbkciIZwg,88
+abnativ-1.1.3.dist-info/entry_points.txt,sha256=E9N9-wD2bnLBchRTpk5Hnp0ZL4LKf5XVKfuzscKVMRw,49
+abnativ-1.1.3.dist-info/RECORD,,
```

