# Comparing `tmp/switch_api-0.5.4b2.tar.gz` & `tmp/switch_api-0.5.4b3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "switch_api-0.5.4b2.tar", last modified: Wed Apr 10 03:52:03 2024, max compression
+gzip compressed data, was "switch_api-0.5.4b3.tar", last modified: Fri Apr 19 04:46:31 2024, max compression
```

## Comparing `switch_api-0.5.4b2.tar` & `switch_api-0.5.4b3.tar`

### file list

```diff
@@ -1,79 +1,79 @@
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.906225 switch_api-0.5.4b2/
--rw-rw-rw-   0        0        0      258 2022-12-01 22:51:03.000000 switch_api-0.5.4b2/AUTHORS.rst
--rw-rw-rw-   0        0        0    36354 2024-04-10 03:50:02.000000 switch_api-0.5.4b2/HISTORY.md
--rw-rw-rw-   0        0        0     1113 2021-09-08 01:10:53.000000 switch_api-0.5.4b2/LICENCE
--rw-rw-rw-   0        0        0       79 2021-09-08 01:10:53.000000 switch_api-0.5.4b2/MANIFEST.in
--rw-rw-rw-   0        0        0    37670 2024-04-10 03:52:03.907223 switch_api-0.5.4b2/PKG-INFO
--rw-rw-rw-   0        0        0      606 2021-09-08 01:10:53.000000 switch_api-0.5.4b2/README.md
--rw-rw-rw-   0        0        0      118 2021-10-25 06:49:21.000000 switch_api-0.5.4b2/pyproject.toml
--rw-rw-rw-   0        0        0       69 2024-04-10 03:52:03.912210 switch_api-0.5.4b2/setup.cfg
--rw-rw-rw-   0        0        0     1483 2024-04-10 03:50:02.000000 switch_api-0.5.4b2/setup.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.522615 switch_api-0.5.4b2/switch_api/
--rw-rw-rw-   0        0        0     1552 2024-04-10 03:50:02.000000 switch_api-0.5.4b2/switch_api/__init__.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.554518 switch_api-0.5.4b2/switch_api/_authentication/
--rw-rw-rw-   0        0        0      399 2021-10-29 06:41:48.000000 switch_api-0.5.4b2/switch_api/_authentication/__init__.py
--rw-rw-rw-   0        0        0     8110 2021-10-29 06:41:48.000000 switch_api-0.5.4b2/switch_api/_authentication/_authentication.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.569512 switch_api-0.5.4b2/switch_api/_authentication/_credentials_store/
--rw-rw-rw-   0        0        0      401 2021-10-29 06:41:48.000000 switch_api-0.5.4b2/switch_api/_authentication/_credentials_store/__init__.py
--rw-rw-rw-   0        0        0     6033 2021-10-29 06:41:48.000000 switch_api-0.5.4b2/switch_api/_authentication/_credentials_store/_credentials_store.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.590017 switch_api-0.5.4b2/switch_api/_authentication/_msal/
--rw-rw-rw-   0        0        0      369 2021-10-29 06:41:48.000000 switch_api-0.5.4b2/switch_api/_authentication/_msal/__init__.py
--rw-rw-rw-   0        0        0     8925 2021-10-29 06:41:48.000000 switch_api-0.5.4b2/switch_api/_authentication/_msal/_custom_application.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.611958 switch_api-0.5.4b2/switch_api/_guide/
--rw-rw-rw-   0        0        0      466 2024-02-15 06:17:47.000000 switch_api-0.5.4b2/switch_api/_guide/__init__.py
--rw-rw-rw-   0        0        0     3204 2024-02-15 08:27:33.000000 switch_api-0.5.4b2/switch_api/_guide/main.py
--rw-rw-rw-   0        0        0     2155 2024-02-15 06:17:47.000000 switch_api-0.5.4b2/switch_api/_guide/processor.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.637176 switch_api-0.5.4b2/switch_api/_utils/
--rw-rw-rw-   0        0        0      474 2021-09-08 01:10:54.000000 switch_api-0.5.4b2/switch_api/_utils/__init__.py
--rw-rw-rw-   0        0        0     4293 2024-04-10 00:48:44.000000 switch_api-0.5.4b2/switch_api/_utils/_constants.py
--rw-rw-rw-   0        0        0     6098 2024-02-20 03:37:51.000000 switch_api-0.5.4b2/switch_api/_utils/_marketplace.py
--rw-rw-rw-   0        0        0    15358 2024-02-20 00:50:07.000000 switch_api-0.5.4b2/switch_api/_utils/_platform.py
--rw-rw-rw-   0        0        0    24013 2024-04-10 02:51:56.000000 switch_api-0.5.4b2/switch_api/_utils/_utils.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.657156 switch_api-0.5.4b2/switch_api/analytics/
--rw-rw-rw-   0        0        0      654 2023-11-02 08:33:44.000000 switch_api-0.5.4b2/switch_api/analytics/__init__.py
--rw-rw-rw-   0        0        0    15018 2024-02-15 03:55:50.000000 switch_api-0.5.4b2/switch_api/analytics/analytics.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.673114 switch_api-0.5.4b2/switch_api/cache/
--rw-rw-rw-   0        0        0      491 2022-04-13 07:59:49.000000 switch_api-0.5.4b2/switch_api/cache/__init__.py
--rw-rw-rw-   0        0        0     7164 2022-04-13 07:59:49.000000 switch_api-0.5.4b2/switch_api/cache/cache.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.718644 switch_api-0.5.4b2/switch_api/controls/
--rw-rw-rw-   0        0        0      451 2023-11-02 23:12:31.000000 switch_api-0.5.4b2/switch_api/controls/__init__.py
--rw-rw-rw-   0        0        0      824 2023-11-02 07:09:52.000000 switch_api-0.5.4b2/switch_api/controls/_constants.py
--rw-rw-rw-   0        0        0     9625 2024-01-23 00:15:45.000000 switch_api-0.5.4b2/switch_api/controls/_mqtt.py
--rw-rw-rw-   0        0        0      336 2023-11-02 23:11:50.000000 switch_api-0.5.4b2/switch_api/controls/constants.py
--rw-rw-rw-   0        0        0    10625 2024-01-23 00:15:45.000000 switch_api-0.5.4b2/switch_api/controls/controls.py
--rw-rw-rw-   0        0        0     8686 2024-02-19 03:00:10.000000 switch_api-0.5.4b2/switch_api/controls/mqtt.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.732615 switch_api-0.5.4b2/switch_api/dataset/
--rw-rw-rw-   0        0        0      570 2021-12-06 00:54:20.000000 switch_api-0.5.4b2/switch_api/dataset/__init__.py
--rw-rw-rw-   0        0        0     7919 2022-09-29 01:50:44.000000 switch_api-0.5.4b2/switch_api/dataset/dataset.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.749537 switch_api-0.5.4b2/switch_api/email/
--rw-rw-rw-   0        0        0      483 2023-09-13 00:25:51.000000 switch_api-0.5.4b2/switch_api/email/__init__.py
--rw-rw-rw-   0        0        0     5071 2024-02-20 02:53:27.000000 switch_api-0.5.4b2/switch_api/email/email_sender.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.774469 switch_api-0.5.4b2/switch_api/error_handlers/
--rw-rw-rw-   0        0        0     2102 2023-09-21 04:02:56.000000 switch_api-0.5.4b2/switch_api/error_handlers/__init__.py
--rw-rw-rw-   0        0        0    16085 2023-10-13 00:49:52.000000 switch_api-0.5.4b2/switch_api/error_handlers/error_handlers.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.818385 switch_api-0.5.4b2/switch_api/extensions/
--rw-rw-rw-   0        0        0     1060 2023-09-13 00:25:51.000000 switch_api-0.5.4b2/switch_api/extensions/__init__.py
--rw-rw-rw-   0        0        0     7755 2023-09-13 00:25:51.000000 switch_api-0.5.4b2/switch_api/extensions/extensions.py
--rw-rw-rw-   0        0        0     1707 2023-09-13 00:25:51.000000 switch_api-0.5.4b2/switch_api/extensions/field_meta.py
--rw-rw-rw-   0        0        0     3090 2023-09-13 00:25:51.000000 switch_api-0.5.4b2/switch_api/extensions/helpers.py
--rw-rw-rw-   0        0        0     1456 2023-09-13 00:25:51.000000 switch_api-0.5.4b2/switch_api/extensions/pipeline.py
--rw-rw-rw-   0        0        0     3288 2023-11-02 07:08:42.000000 switch_api-0.5.4b2/switch_api/initialize.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.847273 switch_api-0.5.4b2/switch_api/integration/
--rw-rw-rw-   0        0        0     1842 2024-04-08 23:10:05.000000 switch_api-0.5.4b2/switch_api/integration/__init__.py
--rw-rw-rw-   0        0        0    13286 2023-11-02 06:21:30.000000 switch_api-0.5.4b2/switch_api/integration/_utils.py
--rw-rw-rw-   0        0        0    37613 2024-02-15 03:39:58.000000 switch_api-0.5.4b2/switch_api/integration/helpers.py
--rw-rw-rw-   0        0        0   115980 2024-04-10 03:50:02.000000 switch_api-0.5.4b2/switch_api/integration/integration.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.892289 switch_api-0.5.4b2/switch_api/pipeline/
--rw-rw-rw-   0        0        0     3446 2024-02-15 08:27:33.000000 switch_api-0.5.4b2/switch_api/pipeline/__init__.py
--rw-rw-rw-   0        0        0    90145 2024-02-20 03:15:24.000000 switch_api-0.5.4b2/switch_api/pipeline/automation.py
--rw-rw-rw-   0        0        0    36702 2021-11-22 06:41:28.000000 switch_api-0.5.4b2/switch_api/pipeline/definitions.py
--rw-rw-rw-   0        0        0    30347 2024-02-20 03:03:23.000000 switch_api-0.5.4b2/switch_api/pipeline/pipeline.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.906225 switch_api-0.5.4b2/switch_api/platform_insights/
--rw-rw-rw-   0        0        0      554 2021-12-06 00:54:20.000000 switch_api-0.5.4b2/switch_api/platform_insights/__init__.py
--rw-rw-rw-   0        0        0     2276 2021-12-06 00:54:20.000000 switch_api-0.5.4b2/switch_api/platform_insights/platform_insights.py
-drwxrwxrwx   0        0        0        0 2024-04-10 03:52:03.540557 switch_api-0.5.4b2/switch_api.egg-info/
--rw-rw-rw-   0        0        0    37670 2024-04-10 03:52:03.000000 switch_api-0.5.4b2/switch_api.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     1909 2024-04-10 03:52:03.000000 switch_api-0.5.4b2/switch_api.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2024-04-10 03:52:03.000000 switch_api-0.5.4b2/switch_api.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0      148 2024-04-10 03:52:03.000000 switch_api-0.5.4b2/switch_api.egg-info/requires.txt
--rw-rw-rw-   0        0        0       11 2024-04-10 03:52:03.000000 switch_api-0.5.4b2/switch_api.egg-info/top_level.txt
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.067991 switch_api-0.5.4b3/
+-rw-r--r--   0 vsts      (1001) docker     (127)      249 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/AUTHORS.rst
+-rw-r--r--   0 vsts      (1001) docker     (127)    34677 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/HISTORY.md
+-rw-r--r--   0 vsts      (1001) docker     (127)     1093 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/LICENCE
+-rw-r--r--   0 vsts      (1001) docker     (127)       74 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/MANIFEST.in
+-rw-r--r--   0 vsts      (1001) docker     (127)    35953 2024-04-19 04:46:31.067991 switch_api-0.5.4b3/PKG-INFO
+-rw-r--r--   0 vsts      (1001) docker     (127)      590 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/README.md
+-rw-r--r--   0 vsts      (1001) docker     (127)      113 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/pyproject.toml
+-rw-r--r--   0 vsts      (1001) docker     (127)       62 2024-04-19 04:46:31.067991 switch_api-0.5.4b3/setup.cfg
+-rw-r--r--   0 vsts      (1001) docker     (127)     1450 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/setup.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.059991 switch_api-0.5.4b3/switch_api/
+-rw-r--r--   0 vsts      (1001) docker     (127)     1510 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/__init__.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/_authentication/
+-rw-r--r--   0 vsts      (1001) docker     (127)      391 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_authentication/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     7908 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_authentication/_authentication.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/_authentication/_credentials_store/
+-rw-r--r--   0 vsts      (1001) docker     (127)      393 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_authentication/_credentials_store/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     5851 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_authentication/_credentials_store/_credentials_store.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/_authentication/_msal/
+-rw-r--r--   0 vsts      (1001) docker     (127)      361 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_authentication/_msal/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     8752 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_authentication/_msal/_custom_application.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/_guide/
+-rw-r--r--   0 vsts      (1001) docker     (127)      455 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_guide/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     3108 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_guide/main.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     2093 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_guide/processor.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/_utils/
+-rw-r--r--   0 vsts      (1001) docker     (127)      462 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_utils/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     4042 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_utils/_constants.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     5952 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_utils/_marketplace.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    14920 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_utils/_platform.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    20863 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/_utils/_utils.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/analytics/
+-rw-r--r--   0 vsts      (1001) docker     (127)      642 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/analytics/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    14655 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/analytics/analytics.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/cache/
+-rw-r--r--   0 vsts      (1001) docker     (127)      480 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/cache/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     6985 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/cache/cache.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/controls/
+-rw-r--r--   0 vsts      (1001) docker     (127)      439 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/controls/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)      803 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/controls/_constants.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     9365 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/controls/_mqtt.py
+-rw-r--r--   0 vsts      (1001) docker     (127)      325 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/controls/constants.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    10360 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/controls/controls.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     8458 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/controls/mqtt.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/dataset/
+-rw-r--r--   0 vsts      (1001) docker     (127)      558 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/dataset/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     7724 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/dataset/dataset.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/email/
+-rw-r--r--   0 vsts      (1001) docker     (127)      472 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/email/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     4935 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/email/email_sender.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/error_handlers/
+-rw-r--r--   0 vsts      (1001) docker     (127)     2066 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/error_handlers/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    15754 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/error_handlers/error_handlers.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.063991 switch_api-0.5.4b3/switch_api/extensions/
+-rw-r--r--   0 vsts      (1001) docker     (127)     1038 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/extensions/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     7519 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/extensions/extensions.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     1652 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/extensions/field_meta.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     2970 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/extensions/helpers.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     1406 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/extensions/pipeline.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     3242 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/initialize.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.067991 switch_api-0.5.4b3/switch_api/integration/
+-rw-r--r--   0 vsts      (1001) docker     (127)     1802 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/integration/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    12939 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/integration/_utils.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    36653 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/integration/helpers.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    99132 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/integration/integration.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.067991 switch_api-0.5.4b3/switch_api/pipeline/
+-rw-r--r--   0 vsts      (1001) docker     (127)     3359 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/pipeline/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    88340 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/pipeline/automation.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    35931 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/pipeline/definitions.py
+-rw-r--r--   0 vsts      (1001) docker     (127)    29571 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/pipeline/pipeline.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.067991 switch_api-0.5.4b3/switch_api/platform_insights/
+-rw-r--r--   0 vsts      (1001) docker     (127)      542 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/platform_insights/__init__.py
+-rw-r--r--   0 vsts      (1001) docker     (127)     2214 2024-04-19 04:46:22.000000 switch_api-0.5.4b3/switch_api/platform_insights/platform_insights.py
+drwxr-xr-x   0 vsts      (1001) docker     (127)        0 2024-04-19 04:46:31.059991 switch_api-0.5.4b3/switch_api.egg-info/
+-rw-r--r--   0 vsts      (1001) docker     (127)    35953 2024-04-19 04:46:30.000000 switch_api-0.5.4b3/switch_api.egg-info/PKG-INFO
+-rw-r--r--   0 vsts      (1001) docker     (127)     1909 2024-04-19 04:46:31.000000 switch_api-0.5.4b3/switch_api.egg-info/SOURCES.txt
+-rw-r--r--   0 vsts      (1001) docker     (127)        1 2024-04-19 04:46:30.000000 switch_api-0.5.4b3/switch_api.egg-info/dependency_links.txt
+-rw-r--r--   0 vsts      (1001) docker     (127)      148 2024-04-19 04:46:30.000000 switch_api-0.5.4b3/switch_api.egg-info/requires.txt
+-rw-r--r--   0 vsts      (1001) docker     (127)       11 2024-04-19 04:46:30.000000 switch_api-0.5.4b3/switch_api.egg-info/top_level.txt
```

### Comparing `switch_api-0.5.4b2/HISTORY.md` & `switch_api-0.5.4b3/HISTORY.md`

 * *Files 15% similar despite different names*

```diff
@@ -1,939 +1,922 @@
-# History
-
-## 0.5.4-b2
-
-### Added
-
-In the `integration` module:
-- Added new function `upsert_reservations()`
-  - Upserts data to the ReservationHistory table
-  - Two attributes added to assist with creation of the input dataframe:
-    - `upsert_reservations.df_required_columns` - returns list of required columns for the input `df`
-    - `upsert_reservations.df_optional_columns` - returns list of required columns for the input `df`
-  - The following datetime fields are required and must use the ``local_date_time_cols`` and ``utc_date_time_cols``
-    parameters to define whether their values are in site-local timezone or UTC timezone:
-    - ``CreatedDate``
-    - ``LastModifiedDate``
-    - ``ReservationStart``
-    - ``ReservationEnd``
-
-## 0.5.3
-
-### Added
-
-- In the `integration` module:
-  - Added `override_existing` parameter in `upsert_discovered_records`
-    - Flag if it the values passed to df will override existing integration records. Only valid if running locally,
-      not on a deployed task where it is triggered via UI.
-    - Defaults to False
-
-## 0.5
-
-### Added
-
-- In the `pipeline` module:
-  - Added a new task type called `Guide`.
-    - this task type should be sub-classed in concert with one of the Task sub-classes when deploying a guide to the
-      marketplace.
-  - Added a new method to the `Automation` class called `register_guide_task()`
-    - this method is used to register tasks that sub-class the `Guide` task and also posts form files to blob and
-      registers the guide to the Marketplace.
-- New `_guide` module - only to be referenced when doing initial development of a Guide
-  - `guide`'s `local_start' method
-    - Allows to run mock guides engine locally that ables to debug `Guide` task types with Form Kit playground.
-
-### Fixed
-
-- In `controls` module:
-  - modify `submit_control` method parameters - typings
-  - remove extra columns from payload to IoT API requests
-
-## 0.4.9
-
-### Added
-
-- New method added in `automation` module:
-  - `run_data_feed()` - Run python job based on data feed id. This will be sent to the queue for processing and will
-    undergo same procedure as the rest of the datafeed.
-    - Required parameters are `api_inputs` and `data_feed_id`
-    - This has a restriction of only allowing an AnalyticsTask type datafeed to be run and deployed as a Timer
-- New method added in `analytics` module:
-  - `upsert_performance_statistics` - this method should only be used by tasks used to populate the Portfolio
-    Benchmarking feature in the Switch Automation platform
-- New `controls` module added and new method added to this module:
-  - `submit_control()` - method to submit control of sensors
-    - this method returns a tuple: `(control_response, missing_response)`:
-      - `control_response` - is the list of sensors that are acknowledged and process by the MQTTT message broker
-      - `missing_response` = is the list of sensors that are sensors that were caught by the connection `time_out` -
-        default to 30 secs - meaning the response were no longer waited to be received by the python package.
-        Increasing the time out can potentially help with this.
-
-### Fixed
-
-- In the `integration` module, minor fixes to:
-  - An unhandled exception when using `pandas==2.1.1` on the following functions:
-    - `upsert_sites()`
-    - `upsert_device_sensors()`
-    - `upsert_device_sensors_ext()`
-    - `upsert_workorders()`
-    - `upsert_timeseries_ds()`
-    - `upsert_timeseries()`
-  - Handle deprecation of `pandas.DataFrame.append()` on the following functions:
-    - `upsert_device_sensors()`
-    - `upsert_device_sensors_ext()`
-  - An unhandled exception for `connect_to_sql()` function when the internal API call within
-    `_get_sql_connection_string()` fails.
-
-## 0.4.8
-
-### Added
-
-- New class added to the `pipeline` module:
-  - `BlobTask` - This class is used to create integrations that post data to the Switch Automation Platform using a
-    blob container & Event Hub Queue as the source.
-    - Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can be
-      registered or deployed.
-    - requires `process_file()` abstract method to be created when sub-classing
-- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
-  - this new method is only applicable for tasks that subclass the `BlobTask` base class.
-- In the `integration` module, new helper methods have been added:
-  - `connect_to_sql()` method creates a pyodbc connection object to enable easier querying of the SQL database via the
-    `pyodbc` library
-  - `amortise_across_days()` method enables easier amortisation of data across days in a period, either inclusive or
-    exclusive of end date.
-  - `get_metadata_where_clause()` method enables creation of `sql_where_clause` for the `get_device_sensors`() method
-    where for each metadata key the sql checks its not null.
-- In the `error_handlers` module:
-  - `check_duplicates()` method added to check for duplicates & post appropriate errors to Task Insights UI in the
-    Switch Automation platform.
-- In the `_utils._utils` module:
-  - `requests_retry_session2` helper function added to enable automatic retries of API calls
-
-### Updated
-
-- In the `integration` module:
-
-  - New parameter `include_removed_sites` added to the `get_sites()` function.
-    - Determines whether or not to include sites marked as "IsRemoved" in the returned dataframe.
-    - Defaults to False, indicating removed sites will not be included.
-  - Updated the`get_device_sensor()` method to check if requested metadata keys or requested
-    tag groups exist for the portfolio and exception if they don't.
-  - New parameter `send_notification` added to the `upsert_timeseries()` function.
-    - This enables Iq Notification messages to be sent when set to `True`
-    - Defaults to `False`
-  - For the `get_sites()`, `get_device_sensors()` and `get_data()` functions, additional parameters have
-    been added to allow customisation of the newly implemented retry logic:
-    - `retries : int`
-      - Number of retries performed beforereturning last retry instance's response status. Max retries = 10.
-        Defaults to 0 currently for backwards compatibility.
-    - `backoff_factor`
-      - If A backoff factor to apply between attempts after the second try (most errors are resolved immediately by a
-        second try without a delay).
-        {_backoff factor_} \* (2 \*\* ({_retry count_} - 1)) seconds
-
-- In the `error_handlers` module:
-  - For the `validate_datetime` function, added two new parameters to enable automatic
-    posting of errors to the Switch Platform:
-    - `errors` : boolean, defaults to False. To enable posting of errors, set to True.
-    - `api_inputs`: defaults to None. Needs to be set to the object returned from switch_api.initialize() if `errors=True`.
-
-### Fixed
-
-- In the `integration` module:
-  - Resolved outlier scenario resulting in unhandled exception on the `upsert_sites()` function.
-  - Minor fix to the `upsert_discovered_records()` method to handle the case when unexpected columns
-    are present in the dataframe passed to `df` input parameter
-
-## 0.4.6
-
-### Added
-
-- Task Priority and Task Framework data feed deployment settings
-  - Task Priority and Task Framework are now available to set when deploying data feeds
-    - Task Priority
-      - Determines the priority of the datafeed tasks when processing.
-      - This equates to how much resources would be alloted to run the task
-      - Available options are: `default`, `standard`, or `advanced`.
-        - set to `advanced` for higher resource when processing data feed task
-      - Defaults to 'default'.
-    - Task Framework
-      - Determines the framework of the datafeed tasks when processing.
-        - 'PythonScriptFramework' for the old task runner engine.
-        - 'TaskInsightsEngine' for the new task running in container apps.
-        - Defaults to 'PythonScriptFramework'
-
-## 0.4.5
-
-### Added
-
-- Email Sender Module
-  - Send emails to active users within a Portfolio in Switch Automation Platform
-  - Limitations:
-    - Emails cannot be sent to users outside of the Portfolio including other users within the platform
-    - Maximum of five attachments per email
-    - Each attachment has a maximum size of 5mb
-  - See function code documentation and usage example below
-- New `generate_filepath` method to provide a filepath where files can be stored
-  - Works well with the attachment feature of the Email Sender Module. Store files in the generated filepath of this method and pass into email attachments
-  - See function code documentation and usage example below
-
-### Email Sender Usage
-
-```python
-import switch_api as sw
-
-sw.email.send_email(
-    api_inputs=api_inputs,
-    subject='',
-    body='',
-    to_recipients=[],
-    cc_recipients=[], # Optional
-    bcc_recipients=[], # Optional
-    attachments=['/file/path/to/attachment.csv'], # Optional
-    conversation_id='' # Optional
-)
-```
-
-### generate_filepath Usage
-
-```python
-import switch_api as sw
-
-generated_attachment_filepath = sw.generate_filepath(api_inputs=api_inputs, filename='generated_attachment.txt')
-
-# Example of where it could be used
-sw.email.send_email(
-    ...
-    attachments=[generated_attachment_filepath]
-    ...
-)
-```
-
-### Fixed
-
-- Issue where `upsert_device_sensors_ext` method was not posting metadata and tag_columns to API
-
-## 0.3.3
-
-### Added
-
-- New `upsert_device_sensors_ext` method to the `integration` module.
-  - Compared to existing `upsert_device_sensors` following are supported:
-    - Installation Code or Installation Id may be provided
-      - BUT cannot provide mix of the two, all must have either code or id and not both.
-    - DriverClassName
-    - DriverDeviceType
-    - PropertyName
-
-### Added Feature - Switch Python Extensions
-
-- Extensions may be used in Task Insights and Switch Guides for code reuse
-- Extensions maybe located in any directory structure within the repo where the usage scripts are located
-- May need to adjust your environment to detect the files if you're not running a project environment
-  - Tested on VSCode and PyCharm - contact Switch Support for issues.
-
-#### Extensions Usage
-
-```python
-import switch_api as sw
-
-# Single import line per extension
-from extensions.my_extension import MyExtension
-
-@sw.extensions.provide(field="some_extension")
-class MyTask:
-    some_extension: MyExtension
-
-if __name__ == "__main__":
-    task = MyTask()
-    task.some_extension.do_something()
-```
-
-#### Extensions Registration
-
-```python
-import uuid
-import switch_api as sw
-
-class SimpleExtension(sw.extensions.ExtensionTask):
-    @property
-    def id(self) -> uuid.UUID:
-        # Unique ID for the extension.
-        # Generate in CLI using:
-        #   python -c 'import uuid; print(uuid.uuid4())'
-        return '46759cfe-68fa-440c-baa9-c859264368db'
-
-    @property
-    def description(self) -> str:
-        return 'Extension with a simple get_name function.'
-
-    @property
-    def author(self) -> str:
-        return 'Amruth Akoju'
-
-    @property
-    def version(self) -> str:
-        return '1.0.1'
-
-    def get_name(self):
-        return "Simple Extension"
-
-# Scaffold code for registration. This will not be persisted in the extension.
-if __name__ == '__main__':
-    task = SimpleExtension()
-
-    api_inputs = sw.initialize(api_project_id='<portfolio-id>')
-
-    # Usage test
-    print(task.get_name())
-
-    # =================================================================
-    # REGISTER TASK & DATAFEED ========================================
-    # =================================================================
-    register = sw.pipeline.Automation.register_task(api_inputs, task)
-    print(register)
-
-```
-
-### Updated
-
-- get_data now has an optional parameter to return a pandas.DataFrame or JSON
-
-## 0.2.27
-
-### Fix
-
-- Issue where Timezone DST Offsets API response of `upsert_timeseries` in `integration` module was handled incorrectly
-
-## 0.2.26
-
-### Updated
-
-- Optional `table_def` parameter on `upsert_data`, `append_data`, and `replace_data` in `integration` module
-  - Enable clients to specify the table structure. It will be merged to the inferred table structure.
-- `list_deployments` in Automation module now provides `Settings` and `DriverId` associated with the deployments
-
-## 0.2.25
-
-### Updated
-
-- Update handling of empty Timezone DST Offsets of `upsert_timeseries` in `integration` module
-
-## 0.2.24
-
-### Updated
-
-- Fix default `ingestion_mode` parameter value to 'Queue' instead of 'Queued' on `upsert_timeseries` in `integration` module
-
-## 0.2.23
-
-### Updated
-
-- Optional `ingestion_mode` parameter on `upsert_timeseries` in `integration` module
-  - Include `ingestionMode` in json payload passed to backend API
-  - `IngestionMode` type must be `Queue` or `Stream`
-  - Default `ingestion_mode` parameter value in `upsert_timeseries` is `Queue`
-  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
-
-## 0.2.22
-
-### Updated
-
-- Optional `ingestion_mode` parameter on `upsert_data` in `integration` module
-  - Include `ingestionMode` in json payload passed to backend API
-  - `IngestionMode` type must be `Queue` or `Stream`
-  - Default `ingestion_mode` parameter value in `upsert_data` is `Queue`
-  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
-
-### Fix
-
-- sw.pipeline.logger handlers stacking
-
-## 0.2.21
-
-### Updated
-
-- Fix on `get_data` method in `dataset` module
-  - Sync parameter structure to backend API for `get_data`
-  - List of dict containing properties of `name`, `value`, and `type` items
-  - `type` property must be one of subset of the new Literal `DATA_SET_QUERY_PARAMETER_TYPES`
-
-## 0.2.20
-
-### Added
-
-- Newly supported Azure Storage Account: GatewayMqttStorage
-- An optional property on QueueTask to specific QueueType
-  - Default: DataIngestion
-
-## 0.2.19
-
-### Fixed
-
-- Fix on `upsert_timeseries` method in `integration` module
-  - Normalized TimestampId and TimestampLocalId seconds
-- Minor fix on `upsert_entities_affected` method in `integration` utils module
-  - Prevent upsert entities affected count when data feed file status Id is not valid
-- Minor fix on `get_metadata_keys` method in `integration` helper module
-  - Fix for issue when a portfolio does not contain any values in the ApiMetadata table
-
-## 0.2.18
-
-### Added
-
-- Added new `is_specific_timezone` parameter in `upsert_timeseries` method of `integration` module
-
-  - Accepts a timezone name as the specific timezone used by the source data.
-  - Can either be of type str or bool and defaults to the value of False.
-  - Cannot have value if 'is_local_time' is set to True.
-  - Retrieve list of available timezones using 'get_timezones' method in `integration` module
-
-    | is_specific_timezone | is_local_time | Description                                                                                                                                                     |
-    | -------------------- | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
-    | False                | False         | Datetimes in provided data is already in UTC and should remain as the value of Timestamp. The TimestampLocal (conversion to site-local Timezone) is calculated. |
-    | False                | True          | Datetimes in provided data is already in the site-local Timezone & should be used to set the value of the TimestampLocal field. The UTC Timestamp is calculated |
-    | Has Value            | True          | NOT ALLOWED                                                                                                                                                     |
-    | Has Value            | False         | Both Timestamp and TimestampLocal fields will are calculated. Datetime is converted to UTC then to Local.                                                       |
-    | True                 |               | NOT ALLOWED                                                                                                                                                     |
-    | '' (empty string)    |               | NOT ALLOWED                                                                                                                                                     |
-
-### Fixed
-
-- Minor fix on `upsert_tags` and `upsert_device_metadata` methods in `integration` module
-  - List of required_columns was incorrectly being updated when these functions were called
-- Minor fix on `upsert_event_work_order_id` method in `integration` module when attempting to update status of an Event
-
-### Updated
-
-- Update on `DiscoveryIntegrationInput` namedtuple - added `job_id`
-- Update `upsert_discovered_records` method required columns in `integration` module
-  - add required `JobId` column for Data Frame parameter
-
-## 0.2.17
-
-### Fixed
-
-- Fix on `upsert_timeseries()` method in `integration` module for duplicate records in ingestion files
-  - records whose Timestamp falls in the exact DST start created 2 records with identical values but different TimestampLocal
-    - one has the TimestampLocal of a DST and the other does not
-
-### Updated
-
-- Update on `get_sites()` method in `integration` module for `InstallationCode` column
-  - when the `InstallationCode' value is null in the database it returns an empty string
-  - `InstallationCode` column is explicity casted to dtype 'str'
-
-## 0.2.16
-
-### Added
-
-- Added new 5 minute interval for `EXPECTED_DELIVERY` Literal in `automation` module
-  - support for data feed deployments Email, FTP, Upload, and Timer
-  - usage: expected_delivery='5min'
-
-### Fixed
-
-- Minor fix on `upsert_timeseries()` method using `data_feed_file_status_id` parameter in `integration` module.
-  - `data_feed_file_status_id` parameter value now synced between process records and ingestion files when supplied
-
-### Updated
-
-- Reduced ingestion files records chunking by half in `upsert_timeseries()` method in `integration` module.
-  - from 100k records chunk down to 50k records chunk
-
-## 0.2.15
-
-### Updated
-
-- Optimized `upsert_timeseries()` method memory upkeep in `integration` module.
-
-## 0.2.14
-
-### Fixed
-
-- Minor fix on `invalid_file_format()` method creating structured logs in `error_handlers` module.
-
-## 0.2.13
-
-### Updated
-
-- Freeze Pandera[io] version to 0.7.1
-  - PandasDtype has been deprecated since 0.8.0
-
-### Compatibility
-
-- Ensure local environment is running Pandera==0.7.1 to match cloud container state
-- Downgrade/Upgrade otherwise by running:
-  - pip uninstall pandera
-  - pip install switch_api
-
-## 0.2.12
-
-### Added
-
-- Added `upsert_tags()` method to the `integration` module.
-  - Upsert tags to existing sites, devices, and sensors
-  - Upserting of tags are categorised by the tagging level which are Site, Device, and Sensor level
-  - Input dataframe requires `Identifier' column whose value depends on the tagging level specified
-    - For Site tag level, InstallationIds are expected to be in the `Identifier` column
-    - For Device tag level, DeviceIds are expected to be in the `Identifier` column
-    - For Sensor tag level, ObjectPropertyIds are expected to be in the `Identifier` column
-- Added `upsert_device_metadata()` method to the `integration` module.
-  - Upsert metadata to existing devices
-
-### Usage
-
-- `upsert_tags()`
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Device')
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Sensor')
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Site')
-- `upsert_device_metadata()`
-  - sw.integration.upsert_device_metadata(api_inputs=api_inputs, df=raw_df)
-
-## 0.2.11
-
-### Added
-
-- New `cache` module that handles cache data related transactions
-  - `set_cache` method that stores data to cache
-  - `get_cache` method that gets stored data from cache
-  - Stored data can be scoped / retrieved into three categories namely Task, Portfolio, and DataFeed scopes
-    - For Task scope,
-      - Data cache can be retrieved by any Portfolio or Datafeed that runs in same Task
-      - provide TaskId (self.id when calling from the driver)
-    - For DataFeed scope,
-      - Data cache can be retrieved (or set) within the Datafeed deployed in portfolio
-      - Provide UUID4 for local testing. api_inputs.data_feed_id will be used when running in the cloud.
-    - For Portfolio scope:
-      - Data cache can be retrieved (or set) by any Datafeed deployed in portfolio
-      - scope_id will be ignored and api_inputs.api_project_id will be used.
-
-## 0.2.10
-
-### Fixed
-
-- Fixed issue with `upsert_timeseries_ds()` method in the `integration` module where required fields such as
-  `Timestamp`, `ObjectPropertyId`, `Value` were being removed.
-
-## 0.2.9
-
-### Added
-
-- Added `upsert_timeseries()` method to the `integration` module.
-  - Data ingested into table storage in addition to ADX Timeseries table
-  - Carbon calculation performed where appropriate
-    - Please note: If carbon or cost are included as fields in the `Meta` column then no carbon / cost calculation will be performed
-
-### Changed
-
-- Added `DriverClassName` to required columns for `upsert_discovered_records()` method in the `integration` module
-
-### Fixed
-
-- A minor fix to 15-minute interval in `upsert_timeseries_ds()` method in the `integration` module.
-
-## 0.2.8
-
-### Changed
-
-- For the `EventWorkOrderTask` class in the `pipeline` module, the `check_work_order_input_valid()` and the
-  `generate_work_order()` methods expect an additional 3 keys to be included by default in the dictionary passed to
-  the `work_order_input` parameter:
-  - `InstallationId`
-  - `EventLink`
-  - `EventSummary`
-
-### Fixed
-
-- Issue with the header/payload passed to the API within the `upsert_event_work_order_id()`
-  function of the `integration` module.
-
-## 0.2.7
-
-### Added
-
-- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
-  - this new method is only applicable for tasks that subclass the `EventWorkOrderTask` base class.
-
-### Changed
-
-- The `data_feed_id` is now a required parameter, not optional, for the following methods on the `Automation` class of
-  the `pipeline` module:
-  - `deploy_on_timer()`
-  - `deploy_as_email_data_feed()`
-  - `deploy_as_ftp_data_feed()`
-  - `deploy_as_upload_data_feed()`
-- The `email_address_domain` is now a required parameter, not optional, for the `deploy_as_email_data_feed()` method
-  on the `Automation` class of the `pipeline` module.
-
-### Fixed
-
-- issue with payload on `switch_api.pipeline.Automation.register_task()` method for `AnalyticsTask` and
-  `EventWorkOrderTask` base classes.
-
-## 0.2.6
-
-### Fixed
-
-- Fixed issues on 2 methods in the `Automation` class of the `pipeline` module:
-  - `delete_data_feed()`
-  - `cancel_deployed_data_feed()`
-
-### Added
-
-In the `pipeline` module:
-
-- Added new class `EventWorkOrderTask`
-  - This task type is for generation of work orders in 3rd party systems via the Switch Automation Platform's Events UI.
-
-### Changed
-
-In the `pipeline` module:
-
-- `AnalyticsTask` - added a new method & a new abstract property:
-  - `analytics_settings_definition` abstract property - defines the required inputs (& how these are displayed in the
-    Switch Automation Platform UI) for the task to successfully run
-  - added `check_analytics_settings_valid()` method that should be used to validate the
-    `analytics_settings` dictionary passed to the `start()` method contains the required keys for the task to
-    successfully run (as defined by the `analytics_settings_definition`)
-
-In the `error_handlers` module:
-
-- In the `post_errors()` function, the parameter `errors_df` is renamed to `errors` and now accepts strings in
-  addition to pandas.DataFrame
-
-### Removed
-
-Due to cutover to a new backend, the following have been removed:
-
-- `run_clone_modules()` function from the `analytics` module
-- the entire `platform_insights` module including the :
-  - `get_current_insights_by_equipment()` function
-
-## 0.2.5
-
-### Added
-
-- The `Automation` class of the `pipeline` module has 2 new methods added: -`delete_data_feed()`
-  - Used to delete an existing data feed and all related deployment settings
-  - `cancel_deployed_data_feed()`
-    - used to cancel the specified `deployment_type` for a given `data_feed_id`
-    - replaces and expands the functionality previously provided in the `cancel_deployed_timer()` method which has been
-      removed.
-
-### Removed
-
-- Removed the `cancel_deployed_timer()` method from the `Automation` class of the `pipeline` module
-  - this functionality is available through the new `cancel_deployed_data_feed()` method when `deployment_type`
-    parameter set to `['Timer']`
-
-## 0.2.4
-
-### Changed
-
-- New parameter `data_feed_name` added to the 4 deployment methods in the `pipeline` module's `Automation` class
-  - `deploy_as_email_data_feed()`
-  - `deploy_as_ftp_data_feed()`
-  - `deploy_as_upload_data_feed()`
-  - `deploy_on_timer()`
-
-## 0.2.3
-
-### Fixed
-
-- Resolved minor issue on `register_task()` method for the `Automation` class in the `pipeline` module.
-
-## 0.2.2
-
-### Fixed
-
-- Resolved minor issue on `upsert_discovered_records()` function in `integration` module related to device-level
-  and sensor-level tags.
-
-## 0.2.1
-
-### Added
-
-- New class added to the `pipeline` module
-  - `DiscoverableIntegrationTask` - for API integrations that are discoverable.
-    - requires `process()` & `run_discovery()` abstract methods to be created when sub-classing
-    - additional abstract property, `integration_device_type_definition`, required compared to base `Task`
-- New function `upsert_discovered_records()` added to the `integration` module
-  - Required for the `DiscoverableIntegrationTask.run_discovery()` method to upsert discovery records to Build -
-    Discovery & Selection UI
-
-### Fixed
-
-- Set minimum msal version required for the switch_api package to be installed.
-
-## 0.2.0
-
-Major overhaul done of the switch_api package. A complete replacement of the API used by the package was done.
-
-### Changed
-
-- The `user_id` parameter has been removed from the `switch_api.initialise()` function.
-  - Authentication of the user is now done via Switch Platform SSO. The call to initialise will trigger a web browser
-    window to open to the platform login screen.
-    - Note: each call to initialise for a portfolio in a different datacentre will open up browser and requires user to
-      input their username & password.
-    - for initialise on a different portfolio within the same datacentre, the authentication is cached so user will not
-      be asked to login again.
-- `api_inputs` is now a required parameter for the `switch_api.pipeline.Automation.register_task()`
-- The `deploy_on_timer()`, `deploy_as_email_data_feed()`, `deploy_as_upload_data_feed()`, and
-  `deploy_as_ftp_data_feed()` methods on the `switch_api.pipeline.Automation` class have an added parameter:
-  `data_feed_id`
-  - This new parameter allows user to update an existing deployment for the portfolio specified in the `api_inputs`.
-  - If `data_feed_id` is not supplied, a new data feed instance will be created (even if portfolio already has that
-    task deployed to it)
-
-## 0.1.18
-
-### Changed
-
-- removed rebuild of the ObjectProperties table in ADX on call to `upsert_device_sensors()`
-- removed rebuild of the Installation table in ADX on call to `upsert_sites()`
-
-## 0.1.17
-
-### Fixed
-
-- Fixed issue with `deploy_on_timer()` method of the `Automation` class in the `pipeline` module.
-- Fixed column header issue with the `get_tag_groups()` function of the `integration` module.
-- Fixed missing Meta column on table generated via `upsert_workorders()` function of the `integration` module.
-
-### Added
-
-- New method for uploading custom data to blob `Blob.custom_upload()`
-
-### Updated
-
-- Updated the `upsert_device_sensors()` to improve performance and aid release of future functionality.
-
-## 0.1.16
-
-### Added
-
-To the `pipeline` module:
-
-- New method `data_feed_history_process_errors()`, to the `Automation` class.
-  - This method returns a dataframe containing the distinct set of error types encountered for a specific
-    `data_feed_file_status_id`
-- New method `data_feed_history_errors_by_type` , to the `Automation` class.
-  - This method returns a dataframe containing the actual errors identified for the specified `error_type` and
-    `data_feed_file_status_id`
-
-Additional logging was also incorporated in the backend to support the Switch Platform UI.
-
-### Fixed
-
-- Fixed issue with `register()` method of the `Automation` class in the `pipeline` module.
-
-### Changed
-
-For the `pipeline` module:
-
-- Standardised the following methods of the `Automation` class to return pandas.DataFrame objects.
-- Added additional error checks to ensure only allowed values are passed to the various `Automation` class methods
-  for the parameters:
-  - `expected_delivery`
-  - `deploy_type`
-  - `queue_name`
-  - `error_type`
-
-For the `integration` module:
-
-- Added additional error checks to ensure only allowed values are passed to `post_errors` function for the parameters:
-  - `error_type`
-  - `process_status`
-
-For the `dataset` module:
-
-- Added additional error check to ensure only allowed values are provided for the `query_language` parameter of the
-  `get_data` function.
-
-For the `_platform` module:
-
-- Added additional error checks to ensure only allowed values are provided for the `account` parameter.
-
-## 0.1.14
-
-### Changed
-
-- updated get_device_sensors() to not auto-detect the data type - to prevent issues such as stripping leading zeroes,
-  etc from metadata values.
-
-## 0.1.13
-
-### Added
-
-To the `pipeline` module:
-
-- Added a new method, `data_feed_history_process_output`, to the `Automation` class
-
-## 0.1.11
-
-### Changed
-
-- Update to access to `logger` - now available as `switch_api.pipeline.logger()`
-- Update to function documentation
-
-## 0.1.10
-
-### Changed
-
-- Updated the calculation of min/max date (for timezone conversions) inside the `upsert_device_sensors` function as
-  the previous calculation method will not be supported in a future release of numpy.
-
-### Fixed
-
-- Fixed issue with retrieval of tag groups and tags via the functions:
-  - `get_sites`
-  - `get_device_sensors`
-
-## 0.1.9
-
-### Added
-
-- New module `platform_insights`
-
-In the `integration` module:
-
-- New function `get_sites` added to lookup site information (optionally with site-level tags)
-- New function `get_device_sensors` added to assist with lookup of device/sensor information, optionally including
-  either metadata or tags.
-- New function `get_tag_groups` added to lookup list of sensor-level tag groups
-- New function `get_metadata_keys` added to lookup list of device-level metadata keys
-
-### Changed
-
-- Modifications to connections to storage accounts.
-- Additional parameter `queue_name` added to the following methods of the `Automation` class of the `pipeline`
-  module:
-  - `deploy_on_timer`
-  - `deploy_as_email_data_feed`
-  - `deploy_as_upload_data_feed`
-  - `deploy_as_ftp_data_feed`
-
-### Fixed
-
-In the `pipeline` module:
-
-- Addressed issue with the schema validation for the `upsert_workorders` function
-
-## 0.1.8
-
-### Changed
-
-In the `integrations` module:
-
-- Updated to batch upserts by DeviceCode to improve reliability & performance of the `upsert_device_sensors` function.
-
-### Fixed
-
-In the `analytics` module:
-
-- typing issue that caused error in the import of the switch_api package for python 3.8
-
-## 0.1.7
-
-### Added
-
-In the `integrations` module:
-
-- Added new function `upsert_workorders`
-  - Provides ability to ingest work order data into the Switch Automation Platform.
-  - Documentation provides details on required & optional fields in the input dataframe and also provides information
-    on allowed values for some fields.
-  - Two attributes available for function, added to assist with creation of scripts by providing list of required &
-    optional fields:
-    - `upsert_workorders.df_required_columns`
-    - `upsert_workorders.df_optional_columns`
-- Added new function `get_states_by_country`:
-  - Retrieves the list of states for a given country. Returns a dataframe containing both the state name and
-    abbreviation.
-- Added new function `get_equipment_classes`:
-  - Retrieves the list of allowed values for Equipment Class.
-    - EquipmentClass is a required field for the upsert_device_sensors function
-
-### Changed
-
-In the `integrations` module:
-
-- For the `upsert_device_sensors` function:
-  - New attributes added to assist with creation of tasks:
-    - `upsert_device_sensors.df_required_columns` - returns list of required columns for the input `df`
-  - Two new fields required to be present in the dataframe passed to function by parameter `df`:
-    - `EquipmentClass`
-    - `EquipmentLabel`
-  - Fix to documentation so required fields in documentation match.
-- For the `upsert_sites` function:
-  - New attributes added to assist with creation of tasks:
-    - `upsert_sites.df_required_columns` - returns list of required columns for the input `df`
-    - `upsert_sites.df_optional_columns` - returns list of required columns for the input `df`
-- For the `get_templates` function:
-  - Added functionality to filter by type via new parameter `object_property_type`
-  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
-    converted to lowercase.
-- For the `get_units_of_measure` function:
-  - Added functionality to filter by type via new parameter `object_property_type`
-  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
-    converted to lowercase.
-
-In the `analytics` module:
-
-- Modifications to type hints and documentation for the functions:
-  - `get_clone_modules_list`
-  - `run_clone_modules`
-- Additional logging added to `run_clone_modules`
-
-## 0.1.6
-
-### Added
-
-- Added new function `upsert_timeseries_ds()` to the `integrations` module
-
-### Changed
-
-- Additional logging added to `invalid_file_format()` function from the `error_handlers` module.
-
-### Removed
-
-- Removed `append_timeseries()` function
-
-## 0.1.5
-
-### Fixed
-
-- bug with `upsert_sites()` function that caused optional columns to be treated as required columns.
-
-### Added
-
-Added additional functions to the `error_handlers` module:
-
-- `validate_datetime()` - which checks whether the values of the datetime column(s) of the source file are valid. Any
-  datetime errors identified by this function should be passed to the `post_errors()` function.
-- `post_errors()` - used to post errors (apart from those identified by the `invalid_file_format()` function) to
-  the data feed dashboard.
-
-## 0.1.4
-
-### Changed
-
-Added additional required properties to the Abstract Base Classes (ABC): Task, IntegrationTask, AnalyticsTask,
-LogicModuleTask. These properties are:
-
-- Author
-- Version
-
-Added additional parameter `query_language` to the `switch.integration.get_data()` function. Allowed values for this
-parameter are:
-
-- `sql`
-- `kql`
-
-Removed the `name_as_filename` and `treat_as_timeseries` parameter from the following functions:
-
-- `switch.integration.replace_data()`
-- `switch.integration.append_data()`
-- `switch.integration.upload_data()`
+# History
+
+## 0.5.3
+
+### Added
+
+- In the `integration` module:
+  - Added `override_existing` parameter in `upsert_discovered_records`
+  - Flag if it the values passed to df will override existing integration records. Only valid if running locally,
+    not on a deployed task where it is triggered via UI.
+  - Defaults to False
+
+## 0.5
+
+### Added
+
+- In the `pipeline` module:
+  - Added a new task type called `Guide`.
+    - this task type should be sub-classed in concert with one of the Task sub-classes when deploying a guide to the
+      marketplace.
+  - Added a new method to the `Automation` class called `register_guide_task()`
+    - this method is used to register tasks that sub-class the `Guide` task and also posts form files to blob and
+      registers the guide to the Marketplace.
+- New `_guide` module - only to be referenced when doing initial development of a Guide
+  - `guide`'s `local_start' method
+    - Allows to run mock guides engine locally that ables to debug `Guide` task types with Form Kit playground.
+
+### Fixed
+
+- In `controls` module:
+  - modify `submit_control` method parameters - typings
+  - remove extra columns from payload to IoT API requests
+
+## 0.4.9
+
+### Added
+
+- New method added in `automation` module:
+  - `run_data_feed()` - Run python job based on data feed id. This will be sent to the queue for processing and will
+    undergo same procedure as the rest of the datafeed.
+    - Required parameters are `api_inputs` and `data_feed_id`
+    - This has a restriction of only allowing an AnalyticsTask type datafeed to be run and deployed as a Timer
+- New method added in `analytics` module:
+  - `upsert_performance_statistics` - this method should only be used by tasks used to populate the Portfolio
+    Benchmarking feature in the Switch Automation platform
+- New `controls` module added and new method added to this module:
+  - `submit_control()` - method to submit control of sensors
+    - this method returns a tuple: `(control_response, missing_response)`:
+      - `control_response` - is the list of sensors that are acknowledged and process by the MQTTT message broker
+      - `missing_response` = is the list of sensors that are sensors that were caught by the connection `time_out` -
+        default to 30 secs - meaning the response were no longer waited to be received by the python package.
+        Increasing the time out can potentially help with this.
+
+### Fixed
+
+- In the `integration` module, minor fixes to:
+  - An unhandled exception when using `pandas==2.1.1` on the following functions:
+    - `upsert_sites()`
+    - `upsert_device_sensors()`
+    - `upsert_device_sensors_ext()`
+    - `upsert_workorders()`
+    - `upsert_timeseries_ds()`
+    - `upsert_timeseries()`
+  - Handle deprecation of `pandas.DataFrame.append()` on the following functions:
+    - `upsert_device_sensors()`
+    - `upsert_device_sensors_ext()`
+  - An unhandled exception for `connect_to_sql()` function when the internal API call within
+    `_get_sql_connection_string()` fails.
+
+## 0.4.8
+
+### Added
+
+- New class added to the `pipeline` module:
+  - `BlobTask` - This class is used to create integrations that post data to the Switch Automation Platform using a
+    blob container & Event Hub Queue as the source.
+    - Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can be
+      registered or deployed.
+    - requires `process_file()` abstract method to be created when sub-classing
+- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
+  - this new method is only applicable for tasks that subclass the `BlobTask` base class.
+- In the `integration` module, new helper methods have been added:
+  - `connect_to_sql()` method creates a pyodbc connection object to enable easier querying of the SQL database via the
+    `pyodbc` library
+  - `amortise_across_days()` method enables easier amortisation of data across days in a period, either inclusive or
+    exclusive of end date.
+  - `get_metadata_where_clause()` method enables creation of `sql_where_clause` for the `get_device_sensors`() method
+    where for each metadata key the sql checks its not null.
+- In the `error_handlers` module:
+  - `check_duplicates()` method added to check for duplicates & post appropriate errors to Task Insights UI in the
+    Switch Automation platform.
+- In the `_utils._utils` module:
+  - `requests_retry_session2` helper function added to enable automatic retries of API calls
+
+### Updated
+
+- In the `integration` module:
+
+  - New parameter `include_removed_sites` added to the `get_sites()` function.
+    - Determines whether or not to include sites marked as "IsRemoved" in the returned dataframe.
+    - Defaults to False, indicating removed sites will not be included.
+  - Updated the`get_device_sensor()` method to check if requested metadata keys or requested
+    tag groups exist for the portfolio and exception if they don't.
+  - New parameter `send_notification` added to the `upsert_timeseries()` function.
+    - This enables Iq Notification messages to be sent when set to `True`
+    - Defaults to `False`
+  - For the `get_sites()`, `get_device_sensors()` and `get_data()` functions, additional parameters have
+    been added to allow customisation of the newly implemented retry logic:
+    - `retries : int`
+      - Number of retries performed beforereturning last retry instance's response status. Max retries = 10.
+        Defaults to 0 currently for backwards compatibility.
+    - `backoff_factor`
+      - If A backoff factor to apply between attempts after the second try (most errors are resolved immediately by a
+        second try without a delay).
+        {_backoff factor_} \* (2 \*\* ({_retry count_} - 1)) seconds
+
+- In the `error_handlers` module:
+  - For the `validate_datetime` function, added two new parameters to enable automatic
+    posting of errors to the Switch Platform:
+    - `errors` : boolean, defaults to False. To enable posting of errors, set to True.
+    - `api_inputs`: defaults to None. Needs to be set to the object returned from switch_api.initialize() if `errors=True`.
+
+### Fixed
+
+- In the `integration` module:
+  - Resolved outlier scenario resulting in unhandled exception on the `upsert_sites()` function.
+  - Minor fix to the `upsert_discovered_records()` method to handle the case when unexpected columns
+    are present in the dataframe passed to `df` input parameter
+
+## 0.4.6
+
+### Added
+
+- Task Priority and Task Framework data feed deployment settings
+  - Task Priority and Task Framework are now available to set when deploying data feeds
+    - Task Priority
+      - Determines the priority of the datafeed tasks when processing.
+      - This equates to how much resources would be alloted to run the task
+      - Available options are: `default`, `standard`, or `advanced`.
+        - set to `advanced` for higher resource when processing data feed task
+      - Defaults to 'default'.
+    - Task Framework
+      - Determines the framework of the datafeed tasks when processing.
+        - 'PythonScriptFramework' for the old task runner engine.
+        - 'TaskInsightsEngine' for the new task running in container apps.
+        - Defaults to 'PythonScriptFramework'
+
+## 0.4.5
+
+### Added
+
+- Email Sender Module
+  - Send emails to active users within a Portfolio in Switch Automation Platform
+  - Limitations:
+    - Emails cannot be sent to users outside of the Portfolio including other users within the platform
+    - Maximum of five attachments per email
+    - Each attachment has a maximum size of 5mb
+  - See function code documentation and usage example below
+- New `generate_filepath` method to provide a filepath where files can be stored
+  - Works well with the attachment feature of the Email Sender Module. Store files in the generated filepath of this method and pass into email attachments
+  - See function code documentation and usage example below
+
+### Email Sender Usage
+
+```python
+import switch_api as sw
+
+sw.email.send_email(
+    api_inputs=api_inputs,
+    subject='',
+    body='',
+    to_recipients=[],
+    cc_recipients=[], # Optional
+    bcc_recipients=[], # Optional
+    attachments=['/file/path/to/attachment.csv'], # Optional
+    conversation_id='' # Optional
+)
+```
+
+### generate_filepath Usage
+
+```python
+import switch_api as sw
+
+generated_attachment_filepath = sw.generate_filepath(api_inputs=api_inputs, filename='generated_attachment.txt')
+
+# Example of where it could be used
+sw.email.send_email(
+    ...
+    attachments=[generated_attachment_filepath]
+    ...
+)
+```
+
+### Fixed
+
+- Issue where `upsert_device_sensors_ext` method was not posting metadata and tag_columns to API
+
+## 0.3.3
+
+### Added
+
+- New `upsert_device_sensors_ext` method to the `integration` module.
+  - Compared to existing `upsert_device_sensors` following are supported:
+    - Installation Code or Installation Id may be provided
+      - BUT cannot provide mix of the two, all must have either code or id and not both.
+    - DriverClassName
+    - DriverDeviceType
+    - PropertyName
+
+### Added Feature - Switch Python Extensions
+
+- Extensions may be used in Task Insights and Switch Guides for code reuse
+- Extensions maybe located in any directory structure within the repo where the usage scripts are located
+- May need to adjust your environment to detect the files if you're not running a project environment
+  - Tested on VSCode and PyCharm - contact Switch Support for issues.
+
+#### Extensions Usage
+
+```python
+import switch_api as sw
+
+# Single import line per extension
+from extensions.my_extension import MyExtension
+
+@sw.extensions.provide(field="some_extension")
+class MyTask:
+    some_extension: MyExtension
+
+if __name__ == "__main__":
+    task = MyTask()
+    task.some_extension.do_something()
+```
+
+#### Extensions Registration
+
+```python
+import uuid
+import switch_api as sw
+
+class SimpleExtension(sw.extensions.ExtensionTask):
+    @property
+    def id(self) -> uuid.UUID:
+        # Unique ID for the extension.
+        # Generate in CLI using:
+        #   python -c 'import uuid; print(uuid.uuid4())'
+        return '46759cfe-68fa-440c-baa9-c859264368db'
+
+    @property
+    def description(self) -> str:
+        return 'Extension with a simple get_name function.'
+
+    @property
+    def author(self) -> str:
+        return 'Amruth Akoju'
+
+    @property
+    def version(self) -> str:
+        return '1.0.1'
+
+    def get_name(self):
+        return "Simple Extension"
+
+# Scaffold code for registration. This will not be persisted in the extension.
+if __name__ == '__main__':
+    task = SimpleExtension()
+
+    api_inputs = sw.initialize(api_project_id='<portfolio-id>')
+
+    # Usage test
+    print(task.get_name())
+
+    # =================================================================
+    # REGISTER TASK & DATAFEED ========================================
+    # =================================================================
+    register = sw.pipeline.Automation.register_task(api_inputs, task)
+    print(register)
+
+```
+
+### Updated
+
+- get_data now has an optional parameter to return a pandas.DataFrame or JSON
+
+## 0.2.27
+
+### Fix
+
+- Issue where Timezone DST Offsets API response of `upsert_timeseries` in `integration` module was handled incorrectly
+
+## 0.2.26
+
+### Updated
+
+- Optional `table_def` parameter on `upsert_data`, `append_data`, and `replace_data` in `integration` module
+  - Enable clients to specify the table structure. It will be merged to the inferred table structure.
+- `list_deployments` in Automation module now provides `Settings` and `DriverId` associated with the deployments
+
+## 0.2.25
+
+### Updated
+
+- Update handling of empty Timezone DST Offsets of `upsert_timeseries` in `integration` module
+
+## 0.2.24
+
+### Updated
+
+- Fix default `ingestion_mode` parameter value to 'Queue' instead of 'Queued' on `upsert_timeseries` in `integration` module
+
+## 0.2.23
+
+### Updated
+
+- Optional `ingestion_mode` parameter on `upsert_timeseries` in `integration` module
+  - Include `ingestionMode` in json payload passed to backend API
+  - `IngestionMode` type must be `Queue` or `Stream`
+  - Default `ingestion_mode` parameter value in `upsert_timeseries` is `Queue`
+  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
+
+## 0.2.22
+
+### Updated
+
+- Optional `ingestion_mode` parameter on `upsert_data` in `integration` module
+  - Include `ingestionMode` in json payload passed to backend API
+  - `IngestionMode` type must be `Queue` or `Stream`
+  - Default `ingestion_mode` parameter value in `upsert_data` is `Queue`
+  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
+
+### Fix
+
+- sw.pipeline.logger handlers stacking
+
+## 0.2.21
+
+### Updated
+
+- Fix on `get_data` method in `dataset` module
+  - Sync parameter structure to backend API for `get_data`
+  - List of dict containing properties of `name`, `value`, and `type` items
+  - `type` property must be one of subset of the new Literal `DATA_SET_QUERY_PARAMETER_TYPES`
+
+## 0.2.20
+
+### Added
+
+- Newly supported Azure Storage Account: GatewayMqttStorage
+- An optional property on QueueTask to specific QueueType
+  - Default: DataIngestion
+
+## 0.2.19
+
+### Fixed
+
+- Fix on `upsert_timeseries` method in `integration` module
+  - Normalized TimestampId and TimestampLocalId seconds
+- Minor fix on `upsert_entities_affected` method in `integration` utils module
+  - Prevent upsert entities affected count when data feed file status Id is not valid
+- Minor fix on `get_metadata_keys` method in `integration` helper module
+  - Fix for issue when a portfolio does not contain any values in the ApiMetadata table
+
+## 0.2.18
+
+### Added
+
+- Added new `is_specific_timezone` parameter in `upsert_timeseries` method of `integration` module
+
+  - Accepts a timezone name as the specific timezone used by the source data.
+  - Can either be of type str or bool and defaults to the value of False.
+  - Cannot have value if 'is_local_time' is set to True.
+  - Retrieve list of available timezones using 'get_timezones' method in `integration` module
+
+    | is_specific_timezone | is_local_time | Description                                                                                                                                                     |
+    | -------------------- | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
+    | False                | False         | Datetimes in provided data is already in UTC and should remain as the value of Timestamp. The TimestampLocal (conversion to site-local Timezone) is calculated. |
+    | False                | True          | Datetimes in provided data is already in the site-local Timezone & should be used to set the value of the TimestampLocal field. The UTC Timestamp is calculated |
+    | Has Value            | True          | NOT ALLOWED                                                                                                                                                     |
+    | Has Value            | False         | Both Timestamp and TimestampLocal fields will are calculated. Datetime is converted to UTC then to Local.                                                       |
+    | True                 |               | NOT ALLOWED                                                                                                                                                     |
+    | '' (empty string)    |               | NOT ALLOWED                                                                                                                                                     |
+
+### Fixed
+
+- Minor fix on `upsert_tags` and `upsert_device_metadata` methods in `integration` module
+  - List of required_columns was incorrectly being updated when these functions were called
+- Minor fix on `upsert_event_work_order_id` method in `integration` module when attempting to update status of an Event
+
+### Updated
+
+- Update on `DiscoveryIntegrationInput` namedtuple - added `job_id`
+- Update `upsert_discovered_records` method required columns in `integration` module
+  - add required `JobId` column for Data Frame parameter
+
+## 0.2.17
+
+### Fixed
+
+- Fix on `upsert_timeseries()` method in `integration` module for duplicate records in ingestion files
+  - records whose Timestamp falls in the exact DST start created 2 records with identical values but different TimestampLocal
+    - one has the TimestampLocal of a DST and the other does not
+
+### Updated
+
+- Update on `get_sites()` method in `integration` module for `InstallationCode` column
+  - when the `InstallationCode' value is null in the database it returns an empty string
+  - `InstallationCode` column is explicity casted to dtype 'str'
+
+## 0.2.16
+
+### Added
+
+- Added new 5 minute interval for `EXPECTED_DELIVERY` Literal in `automation` module
+  - support for data feed deployments Email, FTP, Upload, and Timer
+  - usage: expected_delivery='5min'
+
+### Fixed
+
+- Minor fix on `upsert_timeseries()` method using `data_feed_file_status_id` parameter in `integration` module.
+  - `data_feed_file_status_id` parameter value now synced between process records and ingestion files when supplied
+
+### Updated
+
+- Reduced ingestion files records chunking by half in `upsert_timeseries()` method in `integration` module.
+  - from 100k records chunk down to 50k records chunk
+
+## 0.2.15
+
+### Updated
+
+- Optimized `upsert_timeseries()` method memory upkeep in `integration` module.
+
+## 0.2.14
+
+### Fixed
+
+- Minor fix on `invalid_file_format()` method creating structured logs in `error_handlers` module.
+
+## 0.2.13
+
+### Updated
+
+- Freeze Pandera[io] version to 0.7.1
+  - PandasDtype has been deprecated since 0.8.0
+
+### Compatibility
+
+- Ensure local environment is running Pandera==0.7.1 to match cloud container state
+- Downgrade/Upgrade otherwise by running:
+  - pip uninstall pandera
+  - pip install switch_api
+
+## 0.2.12
+
+### Added
+
+- Added `upsert_tags()` method to the `integration` module.
+  - Upsert tags to existing sites, devices, and sensors
+  - Upserting of tags are categorised by the tagging level which are Site, Device, and Sensor level
+  - Input dataframe requires `Identifier' column whose value depends on the tagging level specified
+    - For Site tag level, InstallationIds are expected to be in the `Identifier` column
+    - For Device tag level, DeviceIds are expected to be in the `Identifier` column
+    - For Sensor tag level, ObjectPropertyIds are expected to be in the `Identifier` column
+- Added `upsert_device_metadata()` method to the `integration` module.
+  - Upsert metadata to existing devices
+
+### Usage
+
+- `upsert_tags()`
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Device')
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Sensor')
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Site')
+- `upsert_device_metadata()`
+  - sw.integration.upsert_device_metadata(api_inputs=api_inputs, df=raw_df)
+
+## 0.2.11
+
+### Added
+
+- New `cache` module that handles cache data related transactions
+  - `set_cache` method that stores data to cache
+  - `get_cache` method that gets stored data from cache
+  - Stored data can be scoped / retrieved into three categories namely Task, Portfolio, and DataFeed scopes
+    - For Task scope,
+      - Data cache can be retrieved by any Portfolio or Datafeed that runs in same Task
+      - provide TaskId (self.id when calling from the driver)
+    - For DataFeed scope,
+      - Data cache can be retrieved (or set) within the Datafeed deployed in portfolio
+      - Provide UUID4 for local testing. api_inputs.data_feed_id will be used when running in the cloud.
+    - For Portfolio scope:
+      - Data cache can be retrieved (or set) by any Datafeed deployed in portfolio
+      - scope_id will be ignored and api_inputs.api_project_id will be used.
+
+## 0.2.10
+
+### Fixed
+
+- Fixed issue with `upsert_timeseries_ds()` method in the `integration` module where required fields such as
+  `Timestamp`, `ObjectPropertyId`, `Value` were being removed.
+
+## 0.2.9
+
+### Added
+
+- Added `upsert_timeseries()` method to the `integration` module.
+  - Data ingested into table storage in addition to ADX Timeseries table
+  - Carbon calculation performed where appropriate
+    - Please note: If carbon or cost are included as fields in the `Meta` column then no carbon / cost calculation will be performed
+
+### Changed
+
+- Added `DriverClassName` to required columns for `upsert_discovered_records()` method in the `integration` module
+
+### Fixed
+
+- A minor fix to 15-minute interval in `upsert_timeseries_ds()` method in the `integration` module.
+
+## 0.2.8
+
+### Changed
+
+- For the `EventWorkOrderTask` class in the `pipeline` module, the `check_work_order_input_valid()` and the
+  `generate_work_order()` methods expect an additional 3 keys to be included by default in the dictionary passed to
+  the `work_order_input` parameter:
+  - `InstallationId`
+  - `EventLink`
+  - `EventSummary`
+
+### Fixed
+
+- Issue with the header/payload passed to the API within the `upsert_event_work_order_id()`
+  function of the `integration` module.
+
+## 0.2.7
+
+### Added
+
+- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
+  - this new method is only applicable for tasks that subclass the `EventWorkOrderTask` base class.
+
+### Changed
+
+- The `data_feed_id` is now a required parameter, not optional, for the following methods on the `Automation` class of
+  the `pipeline` module:
+  - `deploy_on_timer()`
+  - `deploy_as_email_data_feed()`
+  - `deploy_as_ftp_data_feed()`
+  - `deploy_as_upload_data_feed()`
+- The `email_address_domain` is now a required parameter, not optional, for the `deploy_as_email_data_feed()` method
+  on the `Automation` class of the `pipeline` module.
+
+### Fixed
+
+- issue with payload on `switch_api.pipeline.Automation.register_task()` method for `AnalyticsTask` and
+  `EventWorkOrderTask` base classes.
+
+## 0.2.6
+
+### Fixed
+
+- Fixed issues on 2 methods in the `Automation` class of the `pipeline` module:
+  - `delete_data_feed()`
+  - `cancel_deployed_data_feed()`
+
+### Added
+
+In the `pipeline` module:
+
+- Added new class `EventWorkOrderTask`
+  - This task type is for generation of work orders in 3rd party systems via the Switch Automation Platform's Events UI.
+
+### Changed
+
+In the `pipeline` module:
+
+- `AnalyticsTask` - added a new method & a new abstract property:
+  - `analytics_settings_definition` abstract property - defines the required inputs (& how these are displayed in the
+    Switch Automation Platform UI) for the task to successfully run
+  - added `check_analytics_settings_valid()` method that should be used to validate the
+    `analytics_settings` dictionary passed to the `start()` method contains the required keys for the task to
+    successfully run (as defined by the `analytics_settings_definition`)
+
+In the `error_handlers` module:
+
+- In the `post_errors()` function, the parameter `errors_df` is renamed to `errors` and now accepts strings in
+  addition to pandas.DataFrame
+
+### Removed
+
+Due to cutover to a new backend, the following have been removed:
+
+- `run_clone_modules()` function from the `analytics` module
+- the entire `platform_insights` module including the :
+  - `get_current_insights_by_equipment()` function
+
+## 0.2.5
+
+### Added
+
+- The `Automation` class of the `pipeline` module has 2 new methods added: -`delete_data_feed()`
+  - Used to delete an existing data feed and all related deployment settings
+  - `cancel_deployed_data_feed()`
+    - used to cancel the specified `deployment_type` for a given `data_feed_id`
+    - replaces and expands the functionality previously provided in the `cancel_deployed_timer()` method which has been
+      removed.
+
+### Removed
+
+- Removed the `cancel_deployed_timer()` method from the `Automation` class of the `pipeline` module
+  - this functionality is available through the new `cancel_deployed_data_feed()` method when `deployment_type`
+    parameter set to `['Timer']`
+
+## 0.2.4
+
+### Changed
+
+- New parameter `data_feed_name` added to the 4 deployment methods in the `pipeline` module's `Automation` class
+  - `deploy_as_email_data_feed()`
+  - `deploy_as_ftp_data_feed()`
+  - `deploy_as_upload_data_feed()`
+  - `deploy_on_timer()`
+
+## 0.2.3
+
+### Fixed
+
+- Resolved minor issue on `register_task()` method for the `Automation` class in the `pipeline` module.
+
+## 0.2.2
+
+### Fixed
+
+- Resolved minor issue on `upsert_discovered_records()` function in `integration` module related to device-level
+  and sensor-level tags.
+
+## 0.2.1
+
+### Added
+
+- New class added to the `pipeline` module
+  - `DiscoverableIntegrationTask` - for API integrations that are discoverable.
+    - requires `process()` & `run_discovery()` abstract methods to be created when sub-classing
+    - additional abstract property, `integration_device_type_definition`, required compared to base `Task`
+- New function `upsert_discovered_records()` added to the `integration` module
+  - Required for the `DiscoverableIntegrationTask.run_discovery()` method to upsert discovery records to Build -
+    Discovery & Selection UI
+
+### Fixed
+
+- Set minimum msal version required for the switch_api package to be installed.
+
+## 0.2.0
+
+Major overhaul done of the switch_api package. A complete replacement of the API used by the package was done.
+
+### Changed
+
+- The `user_id` parameter has been removed from the `switch_api.initialise()` function.
+  - Authentication of the user is now done via Switch Platform SSO. The call to initialise will trigger a web browser
+    window to open to the platform login screen.
+    - Note: each call to initialise for a portfolio in a different datacentre will open up browser and requires user to
+      input their username & password.
+    - for initialise on a different portfolio within the same datacentre, the authentication is cached so user will not
+      be asked to login again.
+- `api_inputs` is now a required parameter for the `switch_api.pipeline.Automation.register_task()`
+- The `deploy_on_timer()`, `deploy_as_email_data_feed()`, `deploy_as_upload_data_feed()`, and
+  `deploy_as_ftp_data_feed()` methods on the `switch_api.pipeline.Automation` class have an added parameter:
+  `data_feed_id`
+  - This new parameter allows user to update an existing deployment for the portfolio specified in the `api_inputs`.
+  - If `data_feed_id` is not supplied, a new data feed instance will be created (even if portfolio already has that
+    task deployed to it)
+
+## 0.1.18
+
+### Changed
+
+- removed rebuild of the ObjectProperties table in ADX on call to `upsert_device_sensors()`
+- removed rebuild of the Installation table in ADX on call to `upsert_sites()`
+
+## 0.1.17
+
+### Fixed
+
+- Fixed issue with `deploy_on_timer()` method of the `Automation` class in the `pipeline` module.
+- Fixed column header issue with the `get_tag_groups()` function of the `integration` module.
+- Fixed missing Meta column on table generated via `upsert_workorders()` function of the `integration` module.
+
+### Added
+
+- New method for uploading custom data to blob `Blob.custom_upload()`
+
+### Updated
+
+- Updated the `upsert_device_sensors()` to improve performance and aid release of future functionality.
+
+## 0.1.16
+
+### Added
+
+To the `pipeline` module:
+
+- New method `data_feed_history_process_errors()`, to the `Automation` class.
+  - This method returns a dataframe containing the distinct set of error types encountered for a specific
+    `data_feed_file_status_id`
+- New method `data_feed_history_errors_by_type` , to the `Automation` class.
+  - This method returns a dataframe containing the actual errors identified for the specified `error_type` and
+    `data_feed_file_status_id`
+
+Additional logging was also incorporated in the backend to support the Switch Platform UI.
+
+### Fixed
+
+- Fixed issue with `register()` method of the `Automation` class in the `pipeline` module.
+
+### Changed
+
+For the `pipeline` module:
+
+- Standardised the following methods of the `Automation` class to return pandas.DataFrame objects.
+- Added additional error checks to ensure only allowed values are passed to the various `Automation` class methods
+  for the parameters:
+  - `expected_delivery`
+  - `deploy_type`
+  - `queue_name`
+  - `error_type`
+
+For the `integration` module:
+
+- Added additional error checks to ensure only allowed values are passed to `post_errors` function for the parameters:
+  - `error_type`
+  - `process_status`
+
+For the `dataset` module:
+
+- Added additional error check to ensure only allowed values are provided for the `query_language` parameter of the
+  `get_data` function.
+
+For the `_platform` module:
+
+- Added additional error checks to ensure only allowed values are provided for the `account` parameter.
+
+## 0.1.14
+
+### Changed
+
+- updated get_device_sensors() to not auto-detect the data type - to prevent issues such as stripping leading zeroes,
+  etc from metadata values.
+
+## 0.1.13
+
+### Added
+
+To the `pipeline` module:
+
+- Added a new method, `data_feed_history_process_output`, to the `Automation` class
+
+## 0.1.11
+
+### Changed
+
+- Update to access to `logger` - now available as `switch_api.pipeline.logger()`
+- Update to function documentation
+
+## 0.1.10
+
+### Changed
+
+- Updated the calculation of min/max date (for timezone conversions) inside the `upsert_device_sensors` function as
+  the previous calculation method will not be supported in a future release of numpy.
+
+### Fixed
+
+- Fixed issue with retrieval of tag groups and tags via the functions:
+  - `get_sites`
+  - `get_device_sensors`
+
+## 0.1.9
+
+### Added
+
+- New module `platform_insights`
+
+In the `integration` module:
+
+- New function `get_sites` added to lookup site information (optionally with site-level tags)
+- New function `get_device_sensors` added to assist with lookup of device/sensor information, optionally including
+  either metadata or tags.
+- New function `get_tag_groups` added to lookup list of sensor-level tag groups
+- New function `get_metadata_keys` added to lookup list of device-level metadata keys
+
+### Changed
+
+- Modifications to connections to storage accounts.
+- Additional parameter `queue_name` added to the following methods of the `Automation` class of the `pipeline`
+  module:
+  - `deploy_on_timer`
+  - `deploy_as_email_data_feed`
+  - `deploy_as_upload_data_feed`
+  - `deploy_as_ftp_data_feed`
+
+### Fixed
+
+In the `pipeline` module:
+
+- Addressed issue with the schema validation for the `upsert_workorders` function
+
+## 0.1.8
+
+### Changed
+
+In the `integrations` module:
+
+- Updated to batch upserts by DeviceCode to improve reliability & performance of the `upsert_device_sensors` function.
+
+### Fixed
+
+In the `analytics` module:
+
+- typing issue that caused error in the import of the switch_api package for python 3.8
+
+## 0.1.7
+
+### Added
+
+In the `integrations` module:
+
+- Added new function `upsert_workorders`
+  - Provides ability to ingest work order data into the Switch Automation Platform.
+  - Documentation provides details on required & optional fields in the input dataframe and also provides information
+    on allowed values for some fields.
+  - Two attributes available for function, added to assist with creation of scripts by providing list of required &
+    optional fields:
+    - `upsert_workorders.df_required_columns`
+    - `upsert_workorders.df_optional_columns`
+- Added new function `get_states_by_country`:
+  - Retrieves the list of states for a given country. Returns a dataframe containing both the state name and
+    abbreviation.
+- Added new function `get_equipment_classes`:
+  - Retrieves the list of allowed values for Equipment Class.
+    - EquipmentClass is a required field for the upsert_device_sensors function
+
+### Changed
+
+In the `integrations` module:
+
+- For the `upsert_device_sensors` function:
+  - New attributes added to assist with creation of tasks:
+    - `upsert_device_sensors.df_required_columns` - returns list of required columns for the input `df`
+  - Two new fields required to be present in the dataframe passed to function by parameter `df`:
+    - `EquipmentClass`
+    - `EquipmentLabel`
+  - Fix to documentation so required fields in documentation match.
+- For the `upsert_sites` function:
+  - New attributes added to assist with creation of tasks:
+    - `upsert_sites.df_required_columns` - returns list of required columns for the input `df`
+    - `upsert_sites.df_optional_columns` - returns list of required columns for the input `df`
+- For the `get_templates` function:
+  - Added functionality to filter by type via new parameter `object_property_type`
+  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
+    converted to lowercase.
+- For the `get_units_of_measure` function:
+  - Added functionality to filter by type via new parameter `object_property_type`
+  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
+    converted to lowercase.
+
+In the `analytics` module:
+
+- Modifications to type hints and documentation for the functions:
+  - `get_clone_modules_list`
+  - `run_clone_modules`
+- Additional logging added to `run_clone_modules`
+
+## 0.1.6
+
+### Added
+
+- Added new function `upsert_timeseries_ds()` to the `integrations` module
+
+### Changed
+
+- Additional logging added to `invalid_file_format()` function from the `error_handlers` module.
+
+### Removed
+
+- Removed `append_timeseries()` function
+
+## 0.1.5
+
+### Fixed
+
+- bug with `upsert_sites()` function that caused optional columns to be treated as required columns.
+
+### Added
+
+Added additional functions to the `error_handlers` module:
+
+- `validate_datetime()` - which checks whether the values of the datetime column(s) of the source file are valid. Any
+  datetime errors identified by this function should be passed to the `post_errors()` function.
+- `post_errors()` - used to post errors (apart from those identified by the `invalid_file_format()` function) to
+  the data feed dashboard.
+
+## 0.1.4
+
+### Changed
+
+Added additional required properties to the Abstract Base Classes (ABC): Task, IntegrationTask, AnalyticsTask,
+LogicModuleTask. These properties are:
+
+- Author
+- Version
+
+Added additional parameter `query_language` to the `switch.integration.get_data()` function. Allowed values for this
+parameter are:
+
+- `sql`
+- `kql`
+
+Removed the `name_as_filename` and `treat_as_timeseries` parameter from the following functions:
+
+- `switch.integration.replace_data()`
+- `switch.integration.append_data()`
+- `switch.integration.upload_data()`
```

### Comparing `switch_api-0.5.4b2/LICENCE` & `switch_api-0.5.4b3/LICENCE`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-The MIT License (MIT)
-
-Copyright (c) 2021 Switch Automation Pty Ltd. 
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+The MIT License (MIT)
+
+Copyright (c) 2021 Switch Automation Pty Ltd. 
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
```

### Comparing `switch_api-0.5.4b2/PKG-INFO` & `switch_api-0.5.4b3/PKG-INFO`

 * *Files 16% similar despite different names*

```diff
@@ -1,979 +1,962 @@
-Metadata-Version: 2.1
-Name: switch_api
-Version: 0.5.4b2
-Summary: A complete package for data ingestion into the Switch Automation Platform.
-Home-page: UNKNOWN
-Author: Switch Automation Pty Ltd.
-License: MIT License
-Platform: UNKNOWN
-Classifier: Development Status :: 2 - Pre-Alpha
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Intended Audience :: Other Audience
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Natural Language :: English
-Requires-Python: >=3.8.0
-Description-Content-Type: text/markdown
-License-File: LICENCE
-License-File: AUTHORS.rst
-
-# Switch Automation library for Python
-This is a package for data ingestion into the Switch Automation software platform. 
-
-You can find out more about the platform on [Switch Automation](https://www.switchautomation.com)
-
-## Getting started
-
-### Prerequisites
-* Python 3.8 or later is required to use this package. 
-* You must have a [Switch Automation user account](https://www.switchautomation.com/our-solution/) to use this package. 
-
-### Install the package
-Install the Switch Automation library for Python with [pip](https://pypi.org/project/pip/):
-
-```bash
-pip install switch_api
-```
-
-# History
-
-## 0.5.4-b2
-
-### Added
-
-In the `integration` module:
-- Added new function `upsert_reservations()`
-  - Upserts data to the ReservationHistory table
-  - Two attributes added to assist with creation of the input dataframe:
-    - `upsert_reservations.df_required_columns` - returns list of required columns for the input `df`
-    - `upsert_reservations.df_optional_columns` - returns list of required columns for the input `df`
-  - The following datetime fields are required and must use the ``local_date_time_cols`` and ``utc_date_time_cols``
-    parameters to define whether their values are in site-local timezone or UTC timezone:
-    - ``CreatedDate``
-    - ``LastModifiedDate``
-    - ``ReservationStart``
-    - ``ReservationEnd``
-
-## 0.5.3
-
-### Added
-
-- In the `integration` module:
-  - Added `override_existing` parameter in `upsert_discovered_records`
-    - Flag if it the values passed to df will override existing integration records. Only valid if running locally,
-      not on a deployed task where it is triggered via UI.
-    - Defaults to False
-
-## 0.5
-
-### Added
-
-- In the `pipeline` module:
-  - Added a new task type called `Guide`.
-    - this task type should be sub-classed in concert with one of the Task sub-classes when deploying a guide to the
-      marketplace.
-  - Added a new method to the `Automation` class called `register_guide_task()`
-    - this method is used to register tasks that sub-class the `Guide` task and also posts form files to blob and
-      registers the guide to the Marketplace.
-- New `_guide` module - only to be referenced when doing initial development of a Guide
-  - `guide`'s `local_start' method
-    - Allows to run mock guides engine locally that ables to debug `Guide` task types with Form Kit playground.
-
-### Fixed
-
-- In `controls` module:
-  - modify `submit_control` method parameters - typings
-  - remove extra columns from payload to IoT API requests
-
-## 0.4.9
-
-### Added
-
-- New method added in `automation` module:
-  - `run_data_feed()` - Run python job based on data feed id. This will be sent to the queue for processing and will
-    undergo same procedure as the rest of the datafeed.
-    - Required parameters are `api_inputs` and `data_feed_id`
-    - This has a restriction of only allowing an AnalyticsTask type datafeed to be run and deployed as a Timer
-- New method added in `analytics` module:
-  - `upsert_performance_statistics` - this method should only be used by tasks used to populate the Portfolio
-    Benchmarking feature in the Switch Automation platform
-- New `controls` module added and new method added to this module:
-  - `submit_control()` - method to submit control of sensors
-    - this method returns a tuple: `(control_response, missing_response)`:
-      - `control_response` - is the list of sensors that are acknowledged and process by the MQTTT message broker
-      - `missing_response` = is the list of sensors that are sensors that were caught by the connection `time_out` -
-        default to 30 secs - meaning the response were no longer waited to be received by the python package.
-        Increasing the time out can potentially help with this.
-
-### Fixed
-
-- In the `integration` module, minor fixes to:
-  - An unhandled exception when using `pandas==2.1.1` on the following functions:
-    - `upsert_sites()`
-    - `upsert_device_sensors()`
-    - `upsert_device_sensors_ext()`
-    - `upsert_workorders()`
-    - `upsert_timeseries_ds()`
-    - `upsert_timeseries()`
-  - Handle deprecation of `pandas.DataFrame.append()` on the following functions:
-    - `upsert_device_sensors()`
-    - `upsert_device_sensors_ext()`
-  - An unhandled exception for `connect_to_sql()` function when the internal API call within
-    `_get_sql_connection_string()` fails.
-
-## 0.4.8
-
-### Added
-
-- New class added to the `pipeline` module:
-  - `BlobTask` - This class is used to create integrations that post data to the Switch Automation Platform using a
-    blob container & Event Hub Queue as the source.
-    - Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can be
-      registered or deployed.
-    - requires `process_file()` abstract method to be created when sub-classing
-- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
-  - this new method is only applicable for tasks that subclass the `BlobTask` base class.
-- In the `integration` module, new helper methods have been added:
-  - `connect_to_sql()` method creates a pyodbc connection object to enable easier querying of the SQL database via the
-    `pyodbc` library
-  - `amortise_across_days()` method enables easier amortisation of data across days in a period, either inclusive or
-    exclusive of end date.
-  - `get_metadata_where_clause()` method enables creation of `sql_where_clause` for the `get_device_sensors`() method
-    where for each metadata key the sql checks its not null.
-- In the `error_handlers` module:
-  - `check_duplicates()` method added to check for duplicates & post appropriate errors to Task Insights UI in the
-    Switch Automation platform.
-- In the `_utils._utils` module:
-  - `requests_retry_session2` helper function added to enable automatic retries of API calls
-
-### Updated
-
-- In the `integration` module:
-
-  - New parameter `include_removed_sites` added to the `get_sites()` function.
-    - Determines whether or not to include sites marked as "IsRemoved" in the returned dataframe.
-    - Defaults to False, indicating removed sites will not be included.
-  - Updated the`get_device_sensor()` method to check if requested metadata keys or requested
-    tag groups exist for the portfolio and exception if they don't.
-  - New parameter `send_notification` added to the `upsert_timeseries()` function.
-    - This enables Iq Notification messages to be sent when set to `True`
-    - Defaults to `False`
-  - For the `get_sites()`, `get_device_sensors()` and `get_data()` functions, additional parameters have
-    been added to allow customisation of the newly implemented retry logic:
-    - `retries : int`
-      - Number of retries performed beforereturning last retry instance's response status. Max retries = 10.
-        Defaults to 0 currently for backwards compatibility.
-    - `backoff_factor`
-      - If A backoff factor to apply between attempts after the second try (most errors are resolved immediately by a
-        second try without a delay).
-        {_backoff factor_} \* (2 \*\* ({_retry count_} - 1)) seconds
-
-- In the `error_handlers` module:
-  - For the `validate_datetime` function, added two new parameters to enable automatic
-    posting of errors to the Switch Platform:
-    - `errors` : boolean, defaults to False. To enable posting of errors, set to True.
-    - `api_inputs`: defaults to None. Needs to be set to the object returned from switch_api.initialize() if `errors=True`.
-
-### Fixed
-
-- In the `integration` module:
-  - Resolved outlier scenario resulting in unhandled exception on the `upsert_sites()` function.
-  - Minor fix to the `upsert_discovered_records()` method to handle the case when unexpected columns
-    are present in the dataframe passed to `df` input parameter
-
-## 0.4.6
-
-### Added
-
-- Task Priority and Task Framework data feed deployment settings
-  - Task Priority and Task Framework are now available to set when deploying data feeds
-    - Task Priority
-      - Determines the priority of the datafeed tasks when processing.
-      - This equates to how much resources would be alloted to run the task
-      - Available options are: `default`, `standard`, or `advanced`.
-        - set to `advanced` for higher resource when processing data feed task
-      - Defaults to 'default'.
-    - Task Framework
-      - Determines the framework of the datafeed tasks when processing.
-        - 'PythonScriptFramework' for the old task runner engine.
-        - 'TaskInsightsEngine' for the new task running in container apps.
-        - Defaults to 'PythonScriptFramework'
-
-## 0.4.5
-
-### Added
-
-- Email Sender Module
-  - Send emails to active users within a Portfolio in Switch Automation Platform
-  - Limitations:
-    - Emails cannot be sent to users outside of the Portfolio including other users within the platform
-    - Maximum of five attachments per email
-    - Each attachment has a maximum size of 5mb
-  - See function code documentation and usage example below
-- New `generate_filepath` method to provide a filepath where files can be stored
-  - Works well with the attachment feature of the Email Sender Module. Store files in the generated filepath of this method and pass into email attachments
-  - See function code documentation and usage example below
-
-### Email Sender Usage
-
-```python
-import switch_api as sw
-
-sw.email.send_email(
-    api_inputs=api_inputs,
-    subject='',
-    body='',
-    to_recipients=[],
-    cc_recipients=[], # Optional
-    bcc_recipients=[], # Optional
-    attachments=['/file/path/to/attachment.csv'], # Optional
-    conversation_id='' # Optional
-)
-```
-
-### generate_filepath Usage
-
-```python
-import switch_api as sw
-
-generated_attachment_filepath = sw.generate_filepath(api_inputs=api_inputs, filename='generated_attachment.txt')
-
-# Example of where it could be used
-sw.email.send_email(
-    ...
-    attachments=[generated_attachment_filepath]
-    ...
-)
-```
-
-### Fixed
-
-- Issue where `upsert_device_sensors_ext` method was not posting metadata and tag_columns to API
-
-## 0.3.3
-
-### Added
-
-- New `upsert_device_sensors_ext` method to the `integration` module.
-  - Compared to existing `upsert_device_sensors` following are supported:
-    - Installation Code or Installation Id may be provided
-      - BUT cannot provide mix of the two, all must have either code or id and not both.
-    - DriverClassName
-    - DriverDeviceType
-    - PropertyName
-
-### Added Feature - Switch Python Extensions
-
-- Extensions may be used in Task Insights and Switch Guides for code reuse
-- Extensions maybe located in any directory structure within the repo where the usage scripts are located
-- May need to adjust your environment to detect the files if you're not running a project environment
-  - Tested on VSCode and PyCharm - contact Switch Support for issues.
-
-#### Extensions Usage
-
-```python
-import switch_api as sw
-
-# Single import line per extension
-from extensions.my_extension import MyExtension
-
-@sw.extensions.provide(field="some_extension")
-class MyTask:
-    some_extension: MyExtension
-
-if __name__ == "__main__":
-    task = MyTask()
-    task.some_extension.do_something()
-```
-
-#### Extensions Registration
-
-```python
-import uuid
-import switch_api as sw
-
-class SimpleExtension(sw.extensions.ExtensionTask):
-    @property
-    def id(self) -> uuid.UUID:
-        # Unique ID for the extension.
-        # Generate in CLI using:
-        #   python -c 'import uuid; print(uuid.uuid4())'
-        return '46759cfe-68fa-440c-baa9-c859264368db'
-
-    @property
-    def description(self) -> str:
-        return 'Extension with a simple get_name function.'
-
-    @property
-    def author(self) -> str:
-        return 'Amruth Akoju'
-
-    @property
-    def version(self) -> str:
-        return '1.0.1'
-
-    def get_name(self):
-        return "Simple Extension"
-
-# Scaffold code for registration. This will not be persisted in the extension.
-if __name__ == '__main__':
-    task = SimpleExtension()
-
-    api_inputs = sw.initialize(api_project_id='<portfolio-id>')
-
-    # Usage test
-    print(task.get_name())
-
-    # =================================================================
-    # REGISTER TASK & DATAFEED ========================================
-    # =================================================================
-    register = sw.pipeline.Automation.register_task(api_inputs, task)
-    print(register)
-
-```
-
-### Updated
-
-- get_data now has an optional parameter to return a pandas.DataFrame or JSON
-
-## 0.2.27
-
-### Fix
-
-- Issue where Timezone DST Offsets API response of `upsert_timeseries` in `integration` module was handled incorrectly
-
-## 0.2.26
-
-### Updated
-
-- Optional `table_def` parameter on `upsert_data`, `append_data`, and `replace_data` in `integration` module
-  - Enable clients to specify the table structure. It will be merged to the inferred table structure.
-- `list_deployments` in Automation module now provides `Settings` and `DriverId` associated with the deployments
-
-## 0.2.25
-
-### Updated
-
-- Update handling of empty Timezone DST Offsets of `upsert_timeseries` in `integration` module
-
-## 0.2.24
-
-### Updated
-
-- Fix default `ingestion_mode` parameter value to 'Queue' instead of 'Queued' on `upsert_timeseries` in `integration` module
-
-## 0.2.23
-
-### Updated
-
-- Optional `ingestion_mode` parameter on `upsert_timeseries` in `integration` module
-  - Include `ingestionMode` in json payload passed to backend API
-  - `IngestionMode` type must be `Queue` or `Stream`
-  - Default `ingestion_mode` parameter value in `upsert_timeseries` is `Queue`
-  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
-
-## 0.2.22
-
-### Updated
-
-- Optional `ingestion_mode` parameter on `upsert_data` in `integration` module
-  - Include `ingestionMode` in json payload passed to backend API
-  - `IngestionMode` type must be `Queue` or `Stream`
-  - Default `ingestion_mode` parameter value in `upsert_data` is `Queue`
-  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
-
-### Fix
-
-- sw.pipeline.logger handlers stacking
-
-## 0.2.21
-
-### Updated
-
-- Fix on `get_data` method in `dataset` module
-  - Sync parameter structure to backend API for `get_data`
-  - List of dict containing properties of `name`, `value`, and `type` items
-  - `type` property must be one of subset of the new Literal `DATA_SET_QUERY_PARAMETER_TYPES`
-
-## 0.2.20
-
-### Added
-
-- Newly supported Azure Storage Account: GatewayMqttStorage
-- An optional property on QueueTask to specific QueueType
-  - Default: DataIngestion
-
-## 0.2.19
-
-### Fixed
-
-- Fix on `upsert_timeseries` method in `integration` module
-  - Normalized TimestampId and TimestampLocalId seconds
-- Minor fix on `upsert_entities_affected` method in `integration` utils module
-  - Prevent upsert entities affected count when data feed file status Id is not valid
-- Minor fix on `get_metadata_keys` method in `integration` helper module
-  - Fix for issue when a portfolio does not contain any values in the ApiMetadata table
-
-## 0.2.18
-
-### Added
-
-- Added new `is_specific_timezone` parameter in `upsert_timeseries` method of `integration` module
-
-  - Accepts a timezone name as the specific timezone used by the source data.
-  - Can either be of type str or bool and defaults to the value of False.
-  - Cannot have value if 'is_local_time' is set to True.
-  - Retrieve list of available timezones using 'get_timezones' method in `integration` module
-
-    | is_specific_timezone | is_local_time | Description                                                                                                                                                     |
-    | -------------------- | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
-    | False                | False         | Datetimes in provided data is already in UTC and should remain as the value of Timestamp. The TimestampLocal (conversion to site-local Timezone) is calculated. |
-    | False                | True          | Datetimes in provided data is already in the site-local Timezone & should be used to set the value of the TimestampLocal field. The UTC Timestamp is calculated |
-    | Has Value            | True          | NOT ALLOWED                                                                                                                                                     |
-    | Has Value            | False         | Both Timestamp and TimestampLocal fields will are calculated. Datetime is converted to UTC then to Local.                                                       |
-    | True                 |               | NOT ALLOWED                                                                                                                                                     |
-    | '' (empty string)    |               | NOT ALLOWED                                                                                                                                                     |
-
-### Fixed
-
-- Minor fix on `upsert_tags` and `upsert_device_metadata` methods in `integration` module
-  - List of required_columns was incorrectly being updated when these functions were called
-- Minor fix on `upsert_event_work_order_id` method in `integration` module when attempting to update status of an Event
-
-### Updated
-
-- Update on `DiscoveryIntegrationInput` namedtuple - added `job_id`
-- Update `upsert_discovered_records` method required columns in `integration` module
-  - add required `JobId` column for Data Frame parameter
-
-## 0.2.17
-
-### Fixed
-
-- Fix on `upsert_timeseries()` method in `integration` module for duplicate records in ingestion files
-  - records whose Timestamp falls in the exact DST start created 2 records with identical values but different TimestampLocal
-    - one has the TimestampLocal of a DST and the other does not
-
-### Updated
-
-- Update on `get_sites()` method in `integration` module for `InstallationCode` column
-  - when the `InstallationCode' value is null in the database it returns an empty string
-  - `InstallationCode` column is explicity casted to dtype 'str'
-
-## 0.2.16
-
-### Added
-
-- Added new 5 minute interval for `EXPECTED_DELIVERY` Literal in `automation` module
-  - support for data feed deployments Email, FTP, Upload, and Timer
-  - usage: expected_delivery='5min'
-
-### Fixed
-
-- Minor fix on `upsert_timeseries()` method using `data_feed_file_status_id` parameter in `integration` module.
-  - `data_feed_file_status_id` parameter value now synced between process records and ingestion files when supplied
-
-### Updated
-
-- Reduced ingestion files records chunking by half in `upsert_timeseries()` method in `integration` module.
-  - from 100k records chunk down to 50k records chunk
-
-## 0.2.15
-
-### Updated
-
-- Optimized `upsert_timeseries()` method memory upkeep in `integration` module.
-
-## 0.2.14
-
-### Fixed
-
-- Minor fix on `invalid_file_format()` method creating structured logs in `error_handlers` module.
-
-## 0.2.13
-
-### Updated
-
-- Freeze Pandera[io] version to 0.7.1
-  - PandasDtype has been deprecated since 0.8.0
-
-### Compatibility
-
-- Ensure local environment is running Pandera==0.7.1 to match cloud container state
-- Downgrade/Upgrade otherwise by running:
-  - pip uninstall pandera
-  - pip install switch_api
-
-## 0.2.12
-
-### Added
-
-- Added `upsert_tags()` method to the `integration` module.
-  - Upsert tags to existing sites, devices, and sensors
-  - Upserting of tags are categorised by the tagging level which are Site, Device, and Sensor level
-  - Input dataframe requires `Identifier' column whose value depends on the tagging level specified
-    - For Site tag level, InstallationIds are expected to be in the `Identifier` column
-    - For Device tag level, DeviceIds are expected to be in the `Identifier` column
-    - For Sensor tag level, ObjectPropertyIds are expected to be in the `Identifier` column
-- Added `upsert_device_metadata()` method to the `integration` module.
-  - Upsert metadata to existing devices
-
-### Usage
-
-- `upsert_tags()`
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Device')
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Sensor')
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Site')
-- `upsert_device_metadata()`
-  - sw.integration.upsert_device_metadata(api_inputs=api_inputs, df=raw_df)
-
-## 0.2.11
-
-### Added
-
-- New `cache` module that handles cache data related transactions
-  - `set_cache` method that stores data to cache
-  - `get_cache` method that gets stored data from cache
-  - Stored data can be scoped / retrieved into three categories namely Task, Portfolio, and DataFeed scopes
-    - For Task scope,
-      - Data cache can be retrieved by any Portfolio or Datafeed that runs in same Task
-      - provide TaskId (self.id when calling from the driver)
-    - For DataFeed scope,
-      - Data cache can be retrieved (or set) within the Datafeed deployed in portfolio
-      - Provide UUID4 for local testing. api_inputs.data_feed_id will be used when running in the cloud.
-    - For Portfolio scope:
-      - Data cache can be retrieved (or set) by any Datafeed deployed in portfolio
-      - scope_id will be ignored and api_inputs.api_project_id will be used.
-
-## 0.2.10
-
-### Fixed
-
-- Fixed issue with `upsert_timeseries_ds()` method in the `integration` module where required fields such as
-  `Timestamp`, `ObjectPropertyId`, `Value` were being removed.
-
-## 0.2.9
-
-### Added
-
-- Added `upsert_timeseries()` method to the `integration` module.
-  - Data ingested into table storage in addition to ADX Timeseries table
-  - Carbon calculation performed where appropriate
-    - Please note: If carbon or cost are included as fields in the `Meta` column then no carbon / cost calculation will be performed
-
-### Changed
-
-- Added `DriverClassName` to required columns for `upsert_discovered_records()` method in the `integration` module
-
-### Fixed
-
-- A minor fix to 15-minute interval in `upsert_timeseries_ds()` method in the `integration` module.
-
-## 0.2.8
-
-### Changed
-
-- For the `EventWorkOrderTask` class in the `pipeline` module, the `check_work_order_input_valid()` and the
-  `generate_work_order()` methods expect an additional 3 keys to be included by default in the dictionary passed to
-  the `work_order_input` parameter:
-  - `InstallationId`
-  - `EventLink`
-  - `EventSummary`
-
-### Fixed
-
-- Issue with the header/payload passed to the API within the `upsert_event_work_order_id()`
-  function of the `integration` module.
-
-## 0.2.7
-
-### Added
-
-- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
-  - this new method is only applicable for tasks that subclass the `EventWorkOrderTask` base class.
-
-### Changed
-
-- The `data_feed_id` is now a required parameter, not optional, for the following methods on the `Automation` class of
-  the `pipeline` module:
-  - `deploy_on_timer()`
-  - `deploy_as_email_data_feed()`
-  - `deploy_as_ftp_data_feed()`
-  - `deploy_as_upload_data_feed()`
-- The `email_address_domain` is now a required parameter, not optional, for the `deploy_as_email_data_feed()` method
-  on the `Automation` class of the `pipeline` module.
-
-### Fixed
-
-- issue with payload on `switch_api.pipeline.Automation.register_task()` method for `AnalyticsTask` and
-  `EventWorkOrderTask` base classes.
-
-## 0.2.6
-
-### Fixed
-
-- Fixed issues on 2 methods in the `Automation` class of the `pipeline` module:
-  - `delete_data_feed()`
-  - `cancel_deployed_data_feed()`
-
-### Added
-
-In the `pipeline` module:
-
-- Added new class `EventWorkOrderTask`
-  - This task type is for generation of work orders in 3rd party systems via the Switch Automation Platform's Events UI.
-
-### Changed
-
-In the `pipeline` module:
-
-- `AnalyticsTask` - added a new method & a new abstract property:
-  - `analytics_settings_definition` abstract property - defines the required inputs (& how these are displayed in the
-    Switch Automation Platform UI) for the task to successfully run
-  - added `check_analytics_settings_valid()` method that should be used to validate the
-    `analytics_settings` dictionary passed to the `start()` method contains the required keys for the task to
-    successfully run (as defined by the `analytics_settings_definition`)
-
-In the `error_handlers` module:
-
-- In the `post_errors()` function, the parameter `errors_df` is renamed to `errors` and now accepts strings in
-  addition to pandas.DataFrame
-
-### Removed
-
-Due to cutover to a new backend, the following have been removed:
-
-- `run_clone_modules()` function from the `analytics` module
-- the entire `platform_insights` module including the :
-  - `get_current_insights_by_equipment()` function
-
-## 0.2.5
-
-### Added
-
-- The `Automation` class of the `pipeline` module has 2 new methods added: -`delete_data_feed()`
-  - Used to delete an existing data feed and all related deployment settings
-  - `cancel_deployed_data_feed()`
-    - used to cancel the specified `deployment_type` for a given `data_feed_id`
-    - replaces and expands the functionality previously provided in the `cancel_deployed_timer()` method which has been
-      removed.
-
-### Removed
-
-- Removed the `cancel_deployed_timer()` method from the `Automation` class of the `pipeline` module
-  - this functionality is available through the new `cancel_deployed_data_feed()` method when `deployment_type`
-    parameter set to `['Timer']`
-
-## 0.2.4
-
-### Changed
-
-- New parameter `data_feed_name` added to the 4 deployment methods in the `pipeline` module's `Automation` class
-  - `deploy_as_email_data_feed()`
-  - `deploy_as_ftp_data_feed()`
-  - `deploy_as_upload_data_feed()`
-  - `deploy_on_timer()`
-
-## 0.2.3
-
-### Fixed
-
-- Resolved minor issue on `register_task()` method for the `Automation` class in the `pipeline` module.
-
-## 0.2.2
-
-### Fixed
-
-- Resolved minor issue on `upsert_discovered_records()` function in `integration` module related to device-level
-  and sensor-level tags.
-
-## 0.2.1
-
-### Added
-
-- New class added to the `pipeline` module
-  - `DiscoverableIntegrationTask` - for API integrations that are discoverable.
-    - requires `process()` & `run_discovery()` abstract methods to be created when sub-classing
-    - additional abstract property, `integration_device_type_definition`, required compared to base `Task`
-- New function `upsert_discovered_records()` added to the `integration` module
-  - Required for the `DiscoverableIntegrationTask.run_discovery()` method to upsert discovery records to Build -
-    Discovery & Selection UI
-
-### Fixed
-
-- Set minimum msal version required for the switch_api package to be installed.
-
-## 0.2.0
-
-Major overhaul done of the switch_api package. A complete replacement of the API used by the package was done.
-
-### Changed
-
-- The `user_id` parameter has been removed from the `switch_api.initialise()` function.
-  - Authentication of the user is now done via Switch Platform SSO. The call to initialise will trigger a web browser
-    window to open to the platform login screen.
-    - Note: each call to initialise for a portfolio in a different datacentre will open up browser and requires user to
-      input their username & password.
-    - for initialise on a different portfolio within the same datacentre, the authentication is cached so user will not
-      be asked to login again.
-- `api_inputs` is now a required parameter for the `switch_api.pipeline.Automation.register_task()`
-- The `deploy_on_timer()`, `deploy_as_email_data_feed()`, `deploy_as_upload_data_feed()`, and
-  `deploy_as_ftp_data_feed()` methods on the `switch_api.pipeline.Automation` class have an added parameter:
-  `data_feed_id`
-  - This new parameter allows user to update an existing deployment for the portfolio specified in the `api_inputs`.
-  - If `data_feed_id` is not supplied, a new data feed instance will be created (even if portfolio already has that
-    task deployed to it)
-
-## 0.1.18
-
-### Changed
-
-- removed rebuild of the ObjectProperties table in ADX on call to `upsert_device_sensors()`
-- removed rebuild of the Installation table in ADX on call to `upsert_sites()`
-
-## 0.1.17
-
-### Fixed
-
-- Fixed issue with `deploy_on_timer()` method of the `Automation` class in the `pipeline` module.
-- Fixed column header issue with the `get_tag_groups()` function of the `integration` module.
-- Fixed missing Meta column on table generated via `upsert_workorders()` function of the `integration` module.
-
-### Added
-
-- New method for uploading custom data to blob `Blob.custom_upload()`
-
-### Updated
-
-- Updated the `upsert_device_sensors()` to improve performance and aid release of future functionality.
-
-## 0.1.16
-
-### Added
-
-To the `pipeline` module:
-
-- New method `data_feed_history_process_errors()`, to the `Automation` class.
-  - This method returns a dataframe containing the distinct set of error types encountered for a specific
-    `data_feed_file_status_id`
-- New method `data_feed_history_errors_by_type` , to the `Automation` class.
-  - This method returns a dataframe containing the actual errors identified for the specified `error_type` and
-    `data_feed_file_status_id`
-
-Additional logging was also incorporated in the backend to support the Switch Platform UI.
-
-### Fixed
-
-- Fixed issue with `register()` method of the `Automation` class in the `pipeline` module.
-
-### Changed
-
-For the `pipeline` module:
-
-- Standardised the following methods of the `Automation` class to return pandas.DataFrame objects.
-- Added additional error checks to ensure only allowed values are passed to the various `Automation` class methods
-  for the parameters:
-  - `expected_delivery`
-  - `deploy_type`
-  - `queue_name`
-  - `error_type`
-
-For the `integration` module:
-
-- Added additional error checks to ensure only allowed values are passed to `post_errors` function for the parameters:
-  - `error_type`
-  - `process_status`
-
-For the `dataset` module:
-
-- Added additional error check to ensure only allowed values are provided for the `query_language` parameter of the
-  `get_data` function.
-
-For the `_platform` module:
-
-- Added additional error checks to ensure only allowed values are provided for the `account` parameter.
-
-## 0.1.14
-
-### Changed
-
-- updated get_device_sensors() to not auto-detect the data type - to prevent issues such as stripping leading zeroes,
-  etc from metadata values.
-
-## 0.1.13
-
-### Added
-
-To the `pipeline` module:
-
-- Added a new method, `data_feed_history_process_output`, to the `Automation` class
-
-## 0.1.11
-
-### Changed
-
-- Update to access to `logger` - now available as `switch_api.pipeline.logger()`
-- Update to function documentation
-
-## 0.1.10
-
-### Changed
-
-- Updated the calculation of min/max date (for timezone conversions) inside the `upsert_device_sensors` function as
-  the previous calculation method will not be supported in a future release of numpy.
-
-### Fixed
-
-- Fixed issue with retrieval of tag groups and tags via the functions:
-  - `get_sites`
-  - `get_device_sensors`
-
-## 0.1.9
-
-### Added
-
-- New module `platform_insights`
-
-In the `integration` module:
-
-- New function `get_sites` added to lookup site information (optionally with site-level tags)
-- New function `get_device_sensors` added to assist with lookup of device/sensor information, optionally including
-  either metadata or tags.
-- New function `get_tag_groups` added to lookup list of sensor-level tag groups
-- New function `get_metadata_keys` added to lookup list of device-level metadata keys
-
-### Changed
-
-- Modifications to connections to storage accounts.
-- Additional parameter `queue_name` added to the following methods of the `Automation` class of the `pipeline`
-  module:
-  - `deploy_on_timer`
-  - `deploy_as_email_data_feed`
-  - `deploy_as_upload_data_feed`
-  - `deploy_as_ftp_data_feed`
-
-### Fixed
-
-In the `pipeline` module:
-
-- Addressed issue with the schema validation for the `upsert_workorders` function
-
-## 0.1.8
-
-### Changed
-
-In the `integrations` module:
-
-- Updated to batch upserts by DeviceCode to improve reliability & performance of the `upsert_device_sensors` function.
-
-### Fixed
-
-In the `analytics` module:
-
-- typing issue that caused error in the import of the switch_api package for python 3.8
-
-## 0.1.7
-
-### Added
-
-In the `integrations` module:
-
-- Added new function `upsert_workorders`
-  - Provides ability to ingest work order data into the Switch Automation Platform.
-  - Documentation provides details on required & optional fields in the input dataframe and also provides information
-    on allowed values for some fields.
-  - Two attributes available for function, added to assist with creation of scripts by providing list of required &
-    optional fields:
-    - `upsert_workorders.df_required_columns`
-    - `upsert_workorders.df_optional_columns`
-- Added new function `get_states_by_country`:
-  - Retrieves the list of states for a given country. Returns a dataframe containing both the state name and
-    abbreviation.
-- Added new function `get_equipment_classes`:
-  - Retrieves the list of allowed values for Equipment Class.
-    - EquipmentClass is a required field for the upsert_device_sensors function
-
-### Changed
-
-In the `integrations` module:
-
-- For the `upsert_device_sensors` function:
-  - New attributes added to assist with creation of tasks:
-    - `upsert_device_sensors.df_required_columns` - returns list of required columns for the input `df`
-  - Two new fields required to be present in the dataframe passed to function by parameter `df`:
-    - `EquipmentClass`
-    - `EquipmentLabel`
-  - Fix to documentation so required fields in documentation match.
-- For the `upsert_sites` function:
-  - New attributes added to assist with creation of tasks:
-    - `upsert_sites.df_required_columns` - returns list of required columns for the input `df`
-    - `upsert_sites.df_optional_columns` - returns list of required columns for the input `df`
-- For the `get_templates` function:
-  - Added functionality to filter by type via new parameter `object_property_type`
-  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
-    converted to lowercase.
-- For the `get_units_of_measure` function:
-  - Added functionality to filter by type via new parameter `object_property_type`
-  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
-    converted to lowercase.
-
-In the `analytics` module:
-
-- Modifications to type hints and documentation for the functions:
-  - `get_clone_modules_list`
-  - `run_clone_modules`
-- Additional logging added to `run_clone_modules`
-
-## 0.1.6
-
-### Added
-
-- Added new function `upsert_timeseries_ds()` to the `integrations` module
-
-### Changed
-
-- Additional logging added to `invalid_file_format()` function from the `error_handlers` module.
-
-### Removed
-
-- Removed `append_timeseries()` function
-
-## 0.1.5
-
-### Fixed
-
-- bug with `upsert_sites()` function that caused optional columns to be treated as required columns.
-
-### Added
-
-Added additional functions to the `error_handlers` module:
-
-- `validate_datetime()` - which checks whether the values of the datetime column(s) of the source file are valid. Any
-  datetime errors identified by this function should be passed to the `post_errors()` function.
-- `post_errors()` - used to post errors (apart from those identified by the `invalid_file_format()` function) to
-  the data feed dashboard.
-
-## 0.1.4
-
-### Changed
-
-Added additional required properties to the Abstract Base Classes (ABC): Task, IntegrationTask, AnalyticsTask,
-LogicModuleTask. These properties are:
-
-- Author
-- Version
-
-Added additional parameter `query_language` to the `switch.integration.get_data()` function. Allowed values for this
-parameter are:
-
-- `sql`
-- `kql`
-
-Removed the `name_as_filename` and `treat_as_timeseries` parameter from the following functions:
-
-- `switch.integration.replace_data()`
-- `switch.integration.append_data()`
-- `switch.integration.upload_data()`
-
-
+Metadata-Version: 2.1
+Name: switch_api
+Version: 0.5.4b3
+Summary: A complete package for data ingestion into the Switch Automation Platform.
+Home-page: UNKNOWN
+Author: Switch Automation Pty Ltd.
+License: MIT License
+Platform: UNKNOWN
+Classifier: Development Status :: 2 - Pre-Alpha
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Intended Audience :: Other Audience
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Natural Language :: English
+Requires-Python: >=3.8.0
+Description-Content-Type: text/markdown
+License-File: LICENCE
+License-File: AUTHORS.rst
+
+# Switch Automation library for Python
+This is a package for data ingestion into the Switch Automation software platform. 
+
+You can find out more about the platform on [Switch Automation](https://www.switchautomation.com)
+
+## Getting started
+
+### Prerequisites
+* Python 3.8 or later is required to use this package. 
+* You must have a [Switch Automation user account](https://www.switchautomation.com/our-solution/) to use this package. 
+
+### Install the package
+Install the Switch Automation library for Python with [pip](https://pypi.org/project/pip/):
+
+```bash
+pip install switch_api
+```
+
+# History
+
+## 0.5.3
+
+### Added
+
+- In the `integration` module:
+  - Added `override_existing` parameter in `upsert_discovered_records`
+  - Flag if it the values passed to df will override existing integration records. Only valid if running locally,
+    not on a deployed task where it is triggered via UI.
+  - Defaults to False
+
+## 0.5
+
+### Added
+
+- In the `pipeline` module:
+  - Added a new task type called `Guide`.
+    - this task type should be sub-classed in concert with one of the Task sub-classes when deploying a guide to the
+      marketplace.
+  - Added a new method to the `Automation` class called `register_guide_task()`
+    - this method is used to register tasks that sub-class the `Guide` task and also posts form files to blob and
+      registers the guide to the Marketplace.
+- New `_guide` module - only to be referenced when doing initial development of a Guide
+  - `guide`'s `local_start' method
+    - Allows to run mock guides engine locally that ables to debug `Guide` task types with Form Kit playground.
+
+### Fixed
+
+- In `controls` module:
+  - modify `submit_control` method parameters - typings
+  - remove extra columns from payload to IoT API requests
+
+## 0.4.9
+
+### Added
+
+- New method added in `automation` module:
+  - `run_data_feed()` - Run python job based on data feed id. This will be sent to the queue for processing and will
+    undergo same procedure as the rest of the datafeed.
+    - Required parameters are `api_inputs` and `data_feed_id`
+    - This has a restriction of only allowing an AnalyticsTask type datafeed to be run and deployed as a Timer
+- New method added in `analytics` module:
+  - `upsert_performance_statistics` - this method should only be used by tasks used to populate the Portfolio
+    Benchmarking feature in the Switch Automation platform
+- New `controls` module added and new method added to this module:
+  - `submit_control()` - method to submit control of sensors
+    - this method returns a tuple: `(control_response, missing_response)`:
+      - `control_response` - is the list of sensors that are acknowledged and process by the MQTTT message broker
+      - `missing_response` = is the list of sensors that are sensors that were caught by the connection `time_out` -
+        default to 30 secs - meaning the response were no longer waited to be received by the python package.
+        Increasing the time out can potentially help with this.
+
+### Fixed
+
+- In the `integration` module, minor fixes to:
+  - An unhandled exception when using `pandas==2.1.1` on the following functions:
+    - `upsert_sites()`
+    - `upsert_device_sensors()`
+    - `upsert_device_sensors_ext()`
+    - `upsert_workorders()`
+    - `upsert_timeseries_ds()`
+    - `upsert_timeseries()`
+  - Handle deprecation of `pandas.DataFrame.append()` on the following functions:
+    - `upsert_device_sensors()`
+    - `upsert_device_sensors_ext()`
+  - An unhandled exception for `connect_to_sql()` function when the internal API call within
+    `_get_sql_connection_string()` fails.
+
+## 0.4.8
+
+### Added
+
+- New class added to the `pipeline` module:
+  - `BlobTask` - This class is used to create integrations that post data to the Switch Automation Platform using a
+    blob container & Event Hub Queue as the source.
+    - Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can be
+      registered or deployed.
+    - requires `process_file()` abstract method to be created when sub-classing
+- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
+  - this new method is only applicable for tasks that subclass the `BlobTask` base class.
+- In the `integration` module, new helper methods have been added:
+  - `connect_to_sql()` method creates a pyodbc connection object to enable easier querying of the SQL database via the
+    `pyodbc` library
+  - `amortise_across_days()` method enables easier amortisation of data across days in a period, either inclusive or
+    exclusive of end date.
+  - `get_metadata_where_clause()` method enables creation of `sql_where_clause` for the `get_device_sensors`() method
+    where for each metadata key the sql checks its not null.
+- In the `error_handlers` module:
+  - `check_duplicates()` method added to check for duplicates & post appropriate errors to Task Insights UI in the
+    Switch Automation platform.
+- In the `_utils._utils` module:
+  - `requests_retry_session2` helper function added to enable automatic retries of API calls
+
+### Updated
+
+- In the `integration` module:
+
+  - New parameter `include_removed_sites` added to the `get_sites()` function.
+    - Determines whether or not to include sites marked as "IsRemoved" in the returned dataframe.
+    - Defaults to False, indicating removed sites will not be included.
+  - Updated the`get_device_sensor()` method to check if requested metadata keys or requested
+    tag groups exist for the portfolio and exception if they don't.
+  - New parameter `send_notification` added to the `upsert_timeseries()` function.
+    - This enables Iq Notification messages to be sent when set to `True`
+    - Defaults to `False`
+  - For the `get_sites()`, `get_device_sensors()` and `get_data()` functions, additional parameters have
+    been added to allow customisation of the newly implemented retry logic:
+    - `retries : int`
+      - Number of retries performed beforereturning last retry instance's response status. Max retries = 10.
+        Defaults to 0 currently for backwards compatibility.
+    - `backoff_factor`
+      - If A backoff factor to apply between attempts after the second try (most errors are resolved immediately by a
+        second try without a delay).
+        {_backoff factor_} \* (2 \*\* ({_retry count_} - 1)) seconds
+
+- In the `error_handlers` module:
+  - For the `validate_datetime` function, added two new parameters to enable automatic
+    posting of errors to the Switch Platform:
+    - `errors` : boolean, defaults to False. To enable posting of errors, set to True.
+    - `api_inputs`: defaults to None. Needs to be set to the object returned from switch_api.initialize() if `errors=True`.
+
+### Fixed
+
+- In the `integration` module:
+  - Resolved outlier scenario resulting in unhandled exception on the `upsert_sites()` function.
+  - Minor fix to the `upsert_discovered_records()` method to handle the case when unexpected columns
+    are present in the dataframe passed to `df` input parameter
+
+## 0.4.6
+
+### Added
+
+- Task Priority and Task Framework data feed deployment settings
+  - Task Priority and Task Framework are now available to set when deploying data feeds
+    - Task Priority
+      - Determines the priority of the datafeed tasks when processing.
+      - This equates to how much resources would be alloted to run the task
+      - Available options are: `default`, `standard`, or `advanced`.
+        - set to `advanced` for higher resource when processing data feed task
+      - Defaults to 'default'.
+    - Task Framework
+      - Determines the framework of the datafeed tasks when processing.
+        - 'PythonScriptFramework' for the old task runner engine.
+        - 'TaskInsightsEngine' for the new task running in container apps.
+        - Defaults to 'PythonScriptFramework'
+
+## 0.4.5
+
+### Added
+
+- Email Sender Module
+  - Send emails to active users within a Portfolio in Switch Automation Platform
+  - Limitations:
+    - Emails cannot be sent to users outside of the Portfolio including other users within the platform
+    - Maximum of five attachments per email
+    - Each attachment has a maximum size of 5mb
+  - See function code documentation and usage example below
+- New `generate_filepath` method to provide a filepath where files can be stored
+  - Works well with the attachment feature of the Email Sender Module. Store files in the generated filepath of this method and pass into email attachments
+  - See function code documentation and usage example below
+
+### Email Sender Usage
+
+```python
+import switch_api as sw
+
+sw.email.send_email(
+    api_inputs=api_inputs,
+    subject='',
+    body='',
+    to_recipients=[],
+    cc_recipients=[], # Optional
+    bcc_recipients=[], # Optional
+    attachments=['/file/path/to/attachment.csv'], # Optional
+    conversation_id='' # Optional
+)
+```
+
+### generate_filepath Usage
+
+```python
+import switch_api as sw
+
+generated_attachment_filepath = sw.generate_filepath(api_inputs=api_inputs, filename='generated_attachment.txt')
+
+# Example of where it could be used
+sw.email.send_email(
+    ...
+    attachments=[generated_attachment_filepath]
+    ...
+)
+```
+
+### Fixed
+
+- Issue where `upsert_device_sensors_ext` method was not posting metadata and tag_columns to API
+
+## 0.3.3
+
+### Added
+
+- New `upsert_device_sensors_ext` method to the `integration` module.
+  - Compared to existing `upsert_device_sensors` following are supported:
+    - Installation Code or Installation Id may be provided
+      - BUT cannot provide mix of the two, all must have either code or id and not both.
+    - DriverClassName
+    - DriverDeviceType
+    - PropertyName
+
+### Added Feature - Switch Python Extensions
+
+- Extensions may be used in Task Insights and Switch Guides for code reuse
+- Extensions maybe located in any directory structure within the repo where the usage scripts are located
+- May need to adjust your environment to detect the files if you're not running a project environment
+  - Tested on VSCode and PyCharm - contact Switch Support for issues.
+
+#### Extensions Usage
+
+```python
+import switch_api as sw
+
+# Single import line per extension
+from extensions.my_extension import MyExtension
+
+@sw.extensions.provide(field="some_extension")
+class MyTask:
+    some_extension: MyExtension
+
+if __name__ == "__main__":
+    task = MyTask()
+    task.some_extension.do_something()
+```
+
+#### Extensions Registration
+
+```python
+import uuid
+import switch_api as sw
+
+class SimpleExtension(sw.extensions.ExtensionTask):
+    @property
+    def id(self) -> uuid.UUID:
+        # Unique ID for the extension.
+        # Generate in CLI using:
+        #   python -c 'import uuid; print(uuid.uuid4())'
+        return '46759cfe-68fa-440c-baa9-c859264368db'
+
+    @property
+    def description(self) -> str:
+        return 'Extension with a simple get_name function.'
+
+    @property
+    def author(self) -> str:
+        return 'Amruth Akoju'
+
+    @property
+    def version(self) -> str:
+        return '1.0.1'
+
+    def get_name(self):
+        return "Simple Extension"
+
+# Scaffold code for registration. This will not be persisted in the extension.
+if __name__ == '__main__':
+    task = SimpleExtension()
+
+    api_inputs = sw.initialize(api_project_id='<portfolio-id>')
+
+    # Usage test
+    print(task.get_name())
+
+    # =================================================================
+    # REGISTER TASK & DATAFEED ========================================
+    # =================================================================
+    register = sw.pipeline.Automation.register_task(api_inputs, task)
+    print(register)
+
+```
+
+### Updated
+
+- get_data now has an optional parameter to return a pandas.DataFrame or JSON
+
+## 0.2.27
+
+### Fix
+
+- Issue where Timezone DST Offsets API response of `upsert_timeseries` in `integration` module was handled incorrectly
+
+## 0.2.26
+
+### Updated
+
+- Optional `table_def` parameter on `upsert_data`, `append_data`, and `replace_data` in `integration` module
+  - Enable clients to specify the table structure. It will be merged to the inferred table structure.
+- `list_deployments` in Automation module now provides `Settings` and `DriverId` associated with the deployments
+
+## 0.2.25
+
+### Updated
+
+- Update handling of empty Timezone DST Offsets of `upsert_timeseries` in `integration` module
+
+## 0.2.24
+
+### Updated
+
+- Fix default `ingestion_mode` parameter value to 'Queue' instead of 'Queued' on `upsert_timeseries` in `integration` module
+
+## 0.2.23
+
+### Updated
+
+- Optional `ingestion_mode` parameter on `upsert_timeseries` in `integration` module
+  - Include `ingestionMode` in json payload passed to backend API
+  - `IngestionMode` type must be `Queue` or `Stream`
+  - Default `ingestion_mode` parameter value in `upsert_timeseries` is `Queue`
+  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
+
+## 0.2.22
+
+### Updated
+
+- Optional `ingestion_mode` parameter on `upsert_data` in `integration` module
+  - Include `ingestionMode` in json payload passed to backend API
+  - `IngestionMode` type must be `Queue` or `Stream`
+  - Default `ingestion_mode` parameter value in `upsert_data` is `Queue`
+  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
+
+### Fix
+
+- sw.pipeline.logger handlers stacking
+
+## 0.2.21
+
+### Updated
+
+- Fix on `get_data` method in `dataset` module
+  - Sync parameter structure to backend API for `get_data`
+  - List of dict containing properties of `name`, `value`, and `type` items
+  - `type` property must be one of subset of the new Literal `DATA_SET_QUERY_PARAMETER_TYPES`
+
+## 0.2.20
+
+### Added
+
+- Newly supported Azure Storage Account: GatewayMqttStorage
+- An optional property on QueueTask to specific QueueType
+  - Default: DataIngestion
+
+## 0.2.19
+
+### Fixed
+
+- Fix on `upsert_timeseries` method in `integration` module
+  - Normalized TimestampId and TimestampLocalId seconds
+- Minor fix on `upsert_entities_affected` method in `integration` utils module
+  - Prevent upsert entities affected count when data feed file status Id is not valid
+- Minor fix on `get_metadata_keys` method in `integration` helper module
+  - Fix for issue when a portfolio does not contain any values in the ApiMetadata table
+
+## 0.2.18
+
+### Added
+
+- Added new `is_specific_timezone` parameter in `upsert_timeseries` method of `integration` module
+
+  - Accepts a timezone name as the specific timezone used by the source data.
+  - Can either be of type str or bool and defaults to the value of False.
+  - Cannot have value if 'is_local_time' is set to True.
+  - Retrieve list of available timezones using 'get_timezones' method in `integration` module
+
+    | is_specific_timezone | is_local_time | Description                                                                                                                                                     |
+    | -------------------- | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
+    | False                | False         | Datetimes in provided data is already in UTC and should remain as the value of Timestamp. The TimestampLocal (conversion to site-local Timezone) is calculated. |
+    | False                | True          | Datetimes in provided data is already in the site-local Timezone & should be used to set the value of the TimestampLocal field. The UTC Timestamp is calculated |
+    | Has Value            | True          | NOT ALLOWED                                                                                                                                                     |
+    | Has Value            | False         | Both Timestamp and TimestampLocal fields will are calculated. Datetime is converted to UTC then to Local.                                                       |
+    | True                 |               | NOT ALLOWED                                                                                                                                                     |
+    | '' (empty string)    |               | NOT ALLOWED                                                                                                                                                     |
+
+### Fixed
+
+- Minor fix on `upsert_tags` and `upsert_device_metadata` methods in `integration` module
+  - List of required_columns was incorrectly being updated when these functions were called
+- Minor fix on `upsert_event_work_order_id` method in `integration` module when attempting to update status of an Event
+
+### Updated
+
+- Update on `DiscoveryIntegrationInput` namedtuple - added `job_id`
+- Update `upsert_discovered_records` method required columns in `integration` module
+  - add required `JobId` column for Data Frame parameter
+
+## 0.2.17
+
+### Fixed
+
+- Fix on `upsert_timeseries()` method in `integration` module for duplicate records in ingestion files
+  - records whose Timestamp falls in the exact DST start created 2 records with identical values but different TimestampLocal
+    - one has the TimestampLocal of a DST and the other does not
+
+### Updated
+
+- Update on `get_sites()` method in `integration` module for `InstallationCode` column
+  - when the `InstallationCode' value is null in the database it returns an empty string
+  - `InstallationCode` column is explicity casted to dtype 'str'
+
+## 0.2.16
+
+### Added
+
+- Added new 5 minute interval for `EXPECTED_DELIVERY` Literal in `automation` module
+  - support for data feed deployments Email, FTP, Upload, and Timer
+  - usage: expected_delivery='5min'
+
+### Fixed
+
+- Minor fix on `upsert_timeseries()` method using `data_feed_file_status_id` parameter in `integration` module.
+  - `data_feed_file_status_id` parameter value now synced between process records and ingestion files when supplied
+
+### Updated
+
+- Reduced ingestion files records chunking by half in `upsert_timeseries()` method in `integration` module.
+  - from 100k records chunk down to 50k records chunk
+
+## 0.2.15
+
+### Updated
+
+- Optimized `upsert_timeseries()` method memory upkeep in `integration` module.
+
+## 0.2.14
+
+### Fixed
+
+- Minor fix on `invalid_file_format()` method creating structured logs in `error_handlers` module.
+
+## 0.2.13
+
+### Updated
+
+- Freeze Pandera[io] version to 0.7.1
+  - PandasDtype has been deprecated since 0.8.0
+
+### Compatibility
+
+- Ensure local environment is running Pandera==0.7.1 to match cloud container state
+- Downgrade/Upgrade otherwise by running:
+  - pip uninstall pandera
+  - pip install switch_api
+
+## 0.2.12
+
+### Added
+
+- Added `upsert_tags()` method to the `integration` module.
+  - Upsert tags to existing sites, devices, and sensors
+  - Upserting of tags are categorised by the tagging level which are Site, Device, and Sensor level
+  - Input dataframe requires `Identifier' column whose value depends on the tagging level specified
+    - For Site tag level, InstallationIds are expected to be in the `Identifier` column
+    - For Device tag level, DeviceIds are expected to be in the `Identifier` column
+    - For Sensor tag level, ObjectPropertyIds are expected to be in the `Identifier` column
+- Added `upsert_device_metadata()` method to the `integration` module.
+  - Upsert metadata to existing devices
+
+### Usage
+
+- `upsert_tags()`
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Device')
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Sensor')
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Site')
+- `upsert_device_metadata()`
+  - sw.integration.upsert_device_metadata(api_inputs=api_inputs, df=raw_df)
+
+## 0.2.11
+
+### Added
+
+- New `cache` module that handles cache data related transactions
+  - `set_cache` method that stores data to cache
+  - `get_cache` method that gets stored data from cache
+  - Stored data can be scoped / retrieved into three categories namely Task, Portfolio, and DataFeed scopes
+    - For Task scope,
+      - Data cache can be retrieved by any Portfolio or Datafeed that runs in same Task
+      - provide TaskId (self.id when calling from the driver)
+    - For DataFeed scope,
+      - Data cache can be retrieved (or set) within the Datafeed deployed in portfolio
+      - Provide UUID4 for local testing. api_inputs.data_feed_id will be used when running in the cloud.
+    - For Portfolio scope:
+      - Data cache can be retrieved (or set) by any Datafeed deployed in portfolio
+      - scope_id will be ignored and api_inputs.api_project_id will be used.
+
+## 0.2.10
+
+### Fixed
+
+- Fixed issue with `upsert_timeseries_ds()` method in the `integration` module where required fields such as
+  `Timestamp`, `ObjectPropertyId`, `Value` were being removed.
+
+## 0.2.9
+
+### Added
+
+- Added `upsert_timeseries()` method to the `integration` module.
+  - Data ingested into table storage in addition to ADX Timeseries table
+  - Carbon calculation performed where appropriate
+    - Please note: If carbon or cost are included as fields in the `Meta` column then no carbon / cost calculation will be performed
+
+### Changed
+
+- Added `DriverClassName` to required columns for `upsert_discovered_records()` method in the `integration` module
+
+### Fixed
+
+- A minor fix to 15-minute interval in `upsert_timeseries_ds()` method in the `integration` module.
+
+## 0.2.8
+
+### Changed
+
+- For the `EventWorkOrderTask` class in the `pipeline` module, the `check_work_order_input_valid()` and the
+  `generate_work_order()` methods expect an additional 3 keys to be included by default in the dictionary passed to
+  the `work_order_input` parameter:
+  - `InstallationId`
+  - `EventLink`
+  - `EventSummary`
+
+### Fixed
+
+- Issue with the header/payload passed to the API within the `upsert_event_work_order_id()`
+  function of the `integration` module.
+
+## 0.2.7
+
+### Added
+
+- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
+  - this new method is only applicable for tasks that subclass the `EventWorkOrderTask` base class.
+
+### Changed
+
+- The `data_feed_id` is now a required parameter, not optional, for the following methods on the `Automation` class of
+  the `pipeline` module:
+  - `deploy_on_timer()`
+  - `deploy_as_email_data_feed()`
+  - `deploy_as_ftp_data_feed()`
+  - `deploy_as_upload_data_feed()`
+- The `email_address_domain` is now a required parameter, not optional, for the `deploy_as_email_data_feed()` method
+  on the `Automation` class of the `pipeline` module.
+
+### Fixed
+
+- issue with payload on `switch_api.pipeline.Automation.register_task()` method for `AnalyticsTask` and
+  `EventWorkOrderTask` base classes.
+
+## 0.2.6
+
+### Fixed
+
+- Fixed issues on 2 methods in the `Automation` class of the `pipeline` module:
+  - `delete_data_feed()`
+  - `cancel_deployed_data_feed()`
+
+### Added
+
+In the `pipeline` module:
+
+- Added new class `EventWorkOrderTask`
+  - This task type is for generation of work orders in 3rd party systems via the Switch Automation Platform's Events UI.
+
+### Changed
+
+In the `pipeline` module:
+
+- `AnalyticsTask` - added a new method & a new abstract property:
+  - `analytics_settings_definition` abstract property - defines the required inputs (& how these are displayed in the
+    Switch Automation Platform UI) for the task to successfully run
+  - added `check_analytics_settings_valid()` method that should be used to validate the
+    `analytics_settings` dictionary passed to the `start()` method contains the required keys for the task to
+    successfully run (as defined by the `analytics_settings_definition`)
+
+In the `error_handlers` module:
+
+- In the `post_errors()` function, the parameter `errors_df` is renamed to `errors` and now accepts strings in
+  addition to pandas.DataFrame
+
+### Removed
+
+Due to cutover to a new backend, the following have been removed:
+
+- `run_clone_modules()` function from the `analytics` module
+- the entire `platform_insights` module including the :
+  - `get_current_insights_by_equipment()` function
+
+## 0.2.5
+
+### Added
+
+- The `Automation` class of the `pipeline` module has 2 new methods added: -`delete_data_feed()`
+  - Used to delete an existing data feed and all related deployment settings
+  - `cancel_deployed_data_feed()`
+    - used to cancel the specified `deployment_type` for a given `data_feed_id`
+    - replaces and expands the functionality previously provided in the `cancel_deployed_timer()` method which has been
+      removed.
+
+### Removed
+
+- Removed the `cancel_deployed_timer()` method from the `Automation` class of the `pipeline` module
+  - this functionality is available through the new `cancel_deployed_data_feed()` method when `deployment_type`
+    parameter set to `['Timer']`
+
+## 0.2.4
+
+### Changed
+
+- New parameter `data_feed_name` added to the 4 deployment methods in the `pipeline` module's `Automation` class
+  - `deploy_as_email_data_feed()`
+  - `deploy_as_ftp_data_feed()`
+  - `deploy_as_upload_data_feed()`
+  - `deploy_on_timer()`
+
+## 0.2.3
+
+### Fixed
+
+- Resolved minor issue on `register_task()` method for the `Automation` class in the `pipeline` module.
+
+## 0.2.2
+
+### Fixed
+
+- Resolved minor issue on `upsert_discovered_records()` function in `integration` module related to device-level
+  and sensor-level tags.
+
+## 0.2.1
+
+### Added
+
+- New class added to the `pipeline` module
+  - `DiscoverableIntegrationTask` - for API integrations that are discoverable.
+    - requires `process()` & `run_discovery()` abstract methods to be created when sub-classing
+    - additional abstract property, `integration_device_type_definition`, required compared to base `Task`
+- New function `upsert_discovered_records()` added to the `integration` module
+  - Required for the `DiscoverableIntegrationTask.run_discovery()` method to upsert discovery records to Build -
+    Discovery & Selection UI
+
+### Fixed
+
+- Set minimum msal version required for the switch_api package to be installed.
+
+## 0.2.0
+
+Major overhaul done of the switch_api package. A complete replacement of the API used by the package was done.
+
+### Changed
+
+- The `user_id` parameter has been removed from the `switch_api.initialise()` function.
+  - Authentication of the user is now done via Switch Platform SSO. The call to initialise will trigger a web browser
+    window to open to the platform login screen.
+    - Note: each call to initialise for a portfolio in a different datacentre will open up browser and requires user to
+      input their username & password.
+    - for initialise on a different portfolio within the same datacentre, the authentication is cached so user will not
+      be asked to login again.
+- `api_inputs` is now a required parameter for the `switch_api.pipeline.Automation.register_task()`
+- The `deploy_on_timer()`, `deploy_as_email_data_feed()`, `deploy_as_upload_data_feed()`, and
+  `deploy_as_ftp_data_feed()` methods on the `switch_api.pipeline.Automation` class have an added parameter:
+  `data_feed_id`
+  - This new parameter allows user to update an existing deployment for the portfolio specified in the `api_inputs`.
+  - If `data_feed_id` is not supplied, a new data feed instance will be created (even if portfolio already has that
+    task deployed to it)
+
+## 0.1.18
+
+### Changed
+
+- removed rebuild of the ObjectProperties table in ADX on call to `upsert_device_sensors()`
+- removed rebuild of the Installation table in ADX on call to `upsert_sites()`
+
+## 0.1.17
+
+### Fixed
+
+- Fixed issue with `deploy_on_timer()` method of the `Automation` class in the `pipeline` module.
+- Fixed column header issue with the `get_tag_groups()` function of the `integration` module.
+- Fixed missing Meta column on table generated via `upsert_workorders()` function of the `integration` module.
+
+### Added
+
+- New method for uploading custom data to blob `Blob.custom_upload()`
+
+### Updated
+
+- Updated the `upsert_device_sensors()` to improve performance and aid release of future functionality.
+
+## 0.1.16
+
+### Added
+
+To the `pipeline` module:
+
+- New method `data_feed_history_process_errors()`, to the `Automation` class.
+  - This method returns a dataframe containing the distinct set of error types encountered for a specific
+    `data_feed_file_status_id`
+- New method `data_feed_history_errors_by_type` , to the `Automation` class.
+  - This method returns a dataframe containing the actual errors identified for the specified `error_type` and
+    `data_feed_file_status_id`
+
+Additional logging was also incorporated in the backend to support the Switch Platform UI.
+
+### Fixed
+
+- Fixed issue with `register()` method of the `Automation` class in the `pipeline` module.
+
+### Changed
+
+For the `pipeline` module:
+
+- Standardised the following methods of the `Automation` class to return pandas.DataFrame objects.
+- Added additional error checks to ensure only allowed values are passed to the various `Automation` class methods
+  for the parameters:
+  - `expected_delivery`
+  - `deploy_type`
+  - `queue_name`
+  - `error_type`
+
+For the `integration` module:
+
+- Added additional error checks to ensure only allowed values are passed to `post_errors` function for the parameters:
+  - `error_type`
+  - `process_status`
+
+For the `dataset` module:
+
+- Added additional error check to ensure only allowed values are provided for the `query_language` parameter of the
+  `get_data` function.
+
+For the `_platform` module:
+
+- Added additional error checks to ensure only allowed values are provided for the `account` parameter.
+
+## 0.1.14
+
+### Changed
+
+- updated get_device_sensors() to not auto-detect the data type - to prevent issues such as stripping leading zeroes,
+  etc from metadata values.
+
+## 0.1.13
+
+### Added
+
+To the `pipeline` module:
+
+- Added a new method, `data_feed_history_process_output`, to the `Automation` class
+
+## 0.1.11
+
+### Changed
+
+- Update to access to `logger` - now available as `switch_api.pipeline.logger()`
+- Update to function documentation
+
+## 0.1.10
+
+### Changed
+
+- Updated the calculation of min/max date (for timezone conversions) inside the `upsert_device_sensors` function as
+  the previous calculation method will not be supported in a future release of numpy.
+
+### Fixed
+
+- Fixed issue with retrieval of tag groups and tags via the functions:
+  - `get_sites`
+  - `get_device_sensors`
+
+## 0.1.9
+
+### Added
+
+- New module `platform_insights`
+
+In the `integration` module:
+
+- New function `get_sites` added to lookup site information (optionally with site-level tags)
+- New function `get_device_sensors` added to assist with lookup of device/sensor information, optionally including
+  either metadata or tags.
+- New function `get_tag_groups` added to lookup list of sensor-level tag groups
+- New function `get_metadata_keys` added to lookup list of device-level metadata keys
+
+### Changed
+
+- Modifications to connections to storage accounts.
+- Additional parameter `queue_name` added to the following methods of the `Automation` class of the `pipeline`
+  module:
+  - `deploy_on_timer`
+  - `deploy_as_email_data_feed`
+  - `deploy_as_upload_data_feed`
+  - `deploy_as_ftp_data_feed`
+
+### Fixed
+
+In the `pipeline` module:
+
+- Addressed issue with the schema validation for the `upsert_workorders` function
+
+## 0.1.8
+
+### Changed
+
+In the `integrations` module:
+
+- Updated to batch upserts by DeviceCode to improve reliability & performance of the `upsert_device_sensors` function.
+
+### Fixed
+
+In the `analytics` module:
+
+- typing issue that caused error in the import of the switch_api package for python 3.8
+
+## 0.1.7
+
+### Added
+
+In the `integrations` module:
+
+- Added new function `upsert_workorders`
+  - Provides ability to ingest work order data into the Switch Automation Platform.
+  - Documentation provides details on required & optional fields in the input dataframe and also provides information
+    on allowed values for some fields.
+  - Two attributes available for function, added to assist with creation of scripts by providing list of required &
+    optional fields:
+    - `upsert_workorders.df_required_columns`
+    - `upsert_workorders.df_optional_columns`
+- Added new function `get_states_by_country`:
+  - Retrieves the list of states for a given country. Returns a dataframe containing both the state name and
+    abbreviation.
+- Added new function `get_equipment_classes`:
+  - Retrieves the list of allowed values for Equipment Class.
+    - EquipmentClass is a required field for the upsert_device_sensors function
+
+### Changed
+
+In the `integrations` module:
+
+- For the `upsert_device_sensors` function:
+  - New attributes added to assist with creation of tasks:
+    - `upsert_device_sensors.df_required_columns` - returns list of required columns for the input `df`
+  - Two new fields required to be present in the dataframe passed to function by parameter `df`:
+    - `EquipmentClass`
+    - `EquipmentLabel`
+  - Fix to documentation so required fields in documentation match.
+- For the `upsert_sites` function:
+  - New attributes added to assist with creation of tasks:
+    - `upsert_sites.df_required_columns` - returns list of required columns for the input `df`
+    - `upsert_sites.df_optional_columns` - returns list of required columns for the input `df`
+- For the `get_templates` function:
+  - Added functionality to filter by type via new parameter `object_property_type`
+  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
+    converted to lowercase.
+- For the `get_units_of_measure` function:
+  - Added functionality to filter by type via new parameter `object_property_type`
+  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
+    converted to lowercase.
+
+In the `analytics` module:
+
+- Modifications to type hints and documentation for the functions:
+  - `get_clone_modules_list`
+  - `run_clone_modules`
+- Additional logging added to `run_clone_modules`
+
+## 0.1.6
+
+### Added
+
+- Added new function `upsert_timeseries_ds()` to the `integrations` module
+
+### Changed
+
+- Additional logging added to `invalid_file_format()` function from the `error_handlers` module.
+
+### Removed
+
+- Removed `append_timeseries()` function
+
+## 0.1.5
+
+### Fixed
+
+- bug with `upsert_sites()` function that caused optional columns to be treated as required columns.
+
+### Added
+
+Added additional functions to the `error_handlers` module:
+
+- `validate_datetime()` - which checks whether the values of the datetime column(s) of the source file are valid. Any
+  datetime errors identified by this function should be passed to the `post_errors()` function.
+- `post_errors()` - used to post errors (apart from those identified by the `invalid_file_format()` function) to
+  the data feed dashboard.
+
+## 0.1.4
+
+### Changed
+
+Added additional required properties to the Abstract Base Classes (ABC): Task, IntegrationTask, AnalyticsTask,
+LogicModuleTask. These properties are:
+
+- Author
+- Version
+
+Added additional parameter `query_language` to the `switch.integration.get_data()` function. Allowed values for this
+parameter are:
+
+- `sql`
+- `kql`
+
+Removed the `name_as_filename` and `treat_as_timeseries` parameter from the following functions:
+
+- `switch.integration.replace_data()`
+- `switch.integration.append_data()`
+- `switch.integration.upload_data()`
+
+
```

### Comparing `switch_api-0.5.4b2/README.md` & `switch_api-0.5.4b3/README.md`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-# Switch Automation library for Python
-This is a package for data ingestion into the Switch Automation software platform. 
-
-You can find out more about the platform on [Switch Automation](https://www.switchautomation.com)
-
-## Getting started
-
-### Prerequisites
-* Python 3.8 or later is required to use this package. 
-* You must have a [Switch Automation user account](https://www.switchautomation.com/our-solution/) to use this package. 
-
-### Install the package
-Install the Switch Automation library for Python with [pip](https://pypi.org/project/pip/):
-
-```bash
-pip install switch_api
+# Switch Automation library for Python
+This is a package for data ingestion into the Switch Automation software platform. 
+
+You can find out more about the platform on [Switch Automation](https://www.switchautomation.com)
+
+## Getting started
+
+### Prerequisites
+* Python 3.8 or later is required to use this package. 
+* You must have a [Switch Automation user account](https://www.switchautomation.com/our-solution/) to use this package. 
+
+### Install the package
+Install the Switch Automation library for Python with [pip](https://pypi.org/project/pip/):
+
+```bash
+pip install switch_api
 ```
```

### Comparing `switch_api-0.5.4b2/setup.py` & `switch_api-0.5.4b3/setup.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-
-# Import required functions
-from setuptools import setup, find_packages
-
-# Call setup function
-setup(
-    author="Switch Automation Pty Ltd.",
-    description="A complete package for data ingestion into the Switch Automation Platform.",
-    long_description=open('README.md', 'r').read() +
-    '\n\n' + open('HISTORY.md', 'r').read(),
-    long_description_content_type='text/markdown',
-    license='MIT License',
-    name="switch_api",
-    version="0.5.4b2",
-    packages=find_packages(include=["switch_api", "switch_api.*"]),
-    install_requires=['pandas', 'requests', 'azure-storage-blob', 'pandera[io]==0.7.1', 'azure-servicebus',
-                      'msal>=1.11.0', 'paho-mqtt==1.6.1', 'uvicorn==0.22.0', 'fastapi==0.98.0', 'pyodbc==4.0.39'],
-    python_requires=">=3.8.0",
-    classifiers=[
-        'Development Status :: 2 - Pre-Alpha',
-        "License :: OSI Approved :: MIT License",
-        'Intended Audience :: Other Audience',
-        'Programming Language :: Python :: 3',
-        'Programming Language :: Python :: 3.8',
-        'Programming Language :: Python :: 3.9',
-        'Natural Language :: English',
-    ]
-)
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+
+# Import required functions
+from setuptools import setup, find_packages
+
+# Call setup function
+setup(
+    author="Switch Automation Pty Ltd.",
+    description="A complete package for data ingestion into the Switch Automation Platform.",
+    long_description=open('README.md', 'r').read() +
+    '\n\n' + open('HISTORY.md', 'r').read(),
+    long_description_content_type='text/markdown',
+    license='MIT License',
+    name="switch_api",
+    version="0.5.4b3",
+    packages=find_packages(include=["switch_api", "switch_api.*"]),
+    install_requires=['pandas', 'requests', 'azure-storage-blob', 'pandera[io]==0.7.1', 'azure-servicebus',
+                      'msal>=1.11.0', 'paho-mqtt==1.6.1', 'uvicorn==0.22.0', 'fastapi==0.98.0', 'pyodbc==4.0.39'],
+    python_requires=">=3.8.0",
+    classifiers=[
+        'Development Status :: 2 - Pre-Alpha',
+        "License :: OSI Approved :: MIT License",
+        'Intended Audience :: Other Audience',
+        'Programming Language :: Python :: 3',
+        'Programming Language :: Python :: 3.8',
+        'Programming Language :: Python :: 3.9',
+        'Natural Language :: English',
+    ]
+)
```

### Comparing `switch_api-0.5.4b2/switch_api/__init__.py` & `switch_api-0.5.4b3/switch_api/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-Data Ingestion into the Switch Automation
-=========================================
-
-Complete package for ingestion data into the Switch Automation Platform.
-"""
-__all__ = ['dataset', 'analytics', 'pipeline', 'error_handlers', 'integration',
-           'initialize', 'cache', 'extensions', 'email', 'generate_filepath']
-
-from .initialize import initialize
-from . import dataset
-from . import analytics
-from . import pipeline
-from . import error_handlers
-# from . import platform_insights
-from . import integration
-from . import cache
-from . import extensions
-from . import email
-from . import controls
-from . import _guide
-from ._utils._utils import generate_filepath
-
-
-# import logging
-# switch_log = logging.getLogger(__name__).addHandler(logging.NullHandler())
-# switch_log.setLevel(logging.DEBUG)
-#
-# _ch = logging.StreamHandler(stream= sys.stdout)  # creates the handler
-# _ch.setLevel(logging.INFO)  # sets the handler info
-# _ch.setFormatter(logging.Formatter(INFOFORMATTER))  # sets the handler formatting
-#
-# # adds the handler to the global variable: log
-# log.addHandler(_ch)
-# https://dev.to/joaomcteixeira/setting-up-python-logging-for-a-library-app-6ml
-
-__version__ = "0.5.4b2"
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+Data Ingestion into the Switch Automation
+=========================================
+
+Complete package for ingestion data into the Switch Automation Platform.
+"""
+__all__ = ['dataset', 'analytics', 'pipeline', 'error_handlers', 'integration',
+           'initialize', 'cache', 'extensions', 'email', 'generate_filepath']
+
+from .initialize import initialize
+from . import dataset
+from . import analytics
+from . import pipeline
+from . import error_handlers
+# from . import platform_insights
+from . import integration
+from . import cache
+from . import extensions
+from . import email
+from . import controls
+from . import _guide
+from ._utils._utils import generate_filepath
+
+
+# import logging
+# switch_log = logging.getLogger(__name__).addHandler(logging.NullHandler())
+# switch_log.setLevel(logging.DEBUG)
+#
+# _ch = logging.StreamHandler(stream= sys.stdout)  # creates the handler
+# _ch.setLevel(logging.INFO)  # sets the handler info
+# _ch.setFormatter(logging.Formatter(INFOFORMATTER))  # sets the handler formatting
+#
+# # adds the handler to the global variable: log
+# log.addHandler(_ch)
+# https://dev.to/joaomcteixeira/setting-up-python-logging-for-a-library-app-6ml
+
+__version__ = "0.5.4b3"
```

### Comparing `switch_api-0.5.4b2/switch_api/_authentication/_authentication.py` & `switch_api-0.5.4b3/switch_api/_authentication/_authentication.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,212 +1,212 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-import logging
-import sys
-from uuid import UUID
-from msal.oauth2cli.oidc import decode_id_token
-import requests
-from ._credentials_store._credentials_store import SwitchCredentials, store_credentials
-from ._credentials_store._credentials_store import read_credentials
-from ._msal._custom_application import CustomPublicClientApplication
-from .._utils._constants import AUTH_ENDPOINT_DEV, AUTH_ENDPOINT_PROD, SWITCH_ENVIRONMENT
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-_authority = {
-  "dev": {
-    "uitemplateid": "dev",
-    "base_url": "https://switchdevb2c.b2clogin.com/switchdevb2c.onmicrosoft.com",
-  },
-  "prod": {
-    "base_url": "https://switchautomation.b2clogin.com/switchautomation.onmicrosoft.com",
-    "uitemplateid": "default"
-  }
-}
-
-
-def _validate_credentials(creds: SwitchCredentials, version: str):
-    """Validate given Credentials
-
-    Ensures that credentials are up to date and synced.
-
-    Parameters
-    ----------
-    creds
-        SwitchCredentials to validate
-    version
-        The Credentials version
-
-    Returns
-    -------
-    bool
-        Returns the True when credentials are valid; otherwise returns False
-
-    """
-
-    if creds is None or creds is False or creds.user_id == '' or creds.user_id is None or creds.api_token is None:
-        return False
-
-    if creds.version != version:
-        return False
-
-    try:
-        decode_id_token(creds.api_token)
-        return True
-    except Exception as ex:
-        logger.info('Cached token expired. Requesting login.')
-        return False
-
-
-def get_switch_credentials(environment: SWITCH_ENVIRONMENT, portfolio_id):
-    """Read Credentials
-
-    Reads credentials from credential store
-
-    Parameters
-    ----------
-    environment
-        Either development or production environment
-    portfolio_id
-        Get credentials for the given portfolio id
-
-    Returns
-    -------
-    SwitchCredentials
-        Returns the SwitchCredentials namedtuple containing credentials to call Switch APIs
-
-    """
-
-    login_context = fetch_login_context(environment=environment, api_project_id=portfolio_id)[1]
-
-    # Fetch from credentials store if it's already been cached
-    if login_context is not None:
-        datacenter = login_context['dataCenter']
-        key_prefix = f"{environment}_{datacenter}" if environment == 'Development' else datacenter
-        cur_credentials = read_credentials(key_prefix, login_context['portfolioId'], login_context['portfolioName'])
-        if _validate_credentials(cur_credentials, login_context['version']):
-            return cur_credentials
-    else:
-        raise Exception('Unable to obtain Login Context to authenticate with Switch Automation. '
-                        'Please contact your administrator.')
-
-    # Credentials do not exist, take user through browser auth journey
-    config_env = 'dev' if environment == 'Development' else 'prod'
-    authority = f"{_authority[config_env]['base_url']}/{login_context['policyName']}"
-    app = CustomPublicClientApplication(login_context['applicationId'], authority=authority)
-
-    uitemplateid = _authority[config_env]['uitemplateid']
-    prompt = login_context['prompt'] if login_context['prompt'] != '' else None
-    auth_response = app.acquire_token_interactive_custom(scopes=[], prompt=prompt, port=51796,
-                                                         auth_params={
-                                                             "uitemplateid": uitemplateid,
-                                                             "portfolioId": portfolio_id,
-                                                             "environment": environment
-    })
-
-    id_token = auth_response['id_token']
-
-    api_base_url = AUTH_ENDPOINT_DEV if environment == 'Development' else AUTH_ENDPOINT_PROD
-    _, profile = fetch_user_profile_details(api_base_url, id_token, environment, portfolio_id)
-
-    if profile is None:
-        raise Exception('Unable to obtain User Profile. Please try again or contact your administrator.')
-
-    credentials = SwitchCredentials(login_context['version'], profile['email'],
-                                    profile['userId'], login_context['portfolioId'],
-                                    login_context['portfolioName'], id_token, profile['apiEndpoint'],
-                                    environment, profile['subscriptionKey'])
-
-    store_credentials(credentials, key_prefix=key_prefix)
-
-    return credentials
-
-
-def fetch_user_profile_details(api_base_url: str, token: str, environment: SWITCH_ENVIRONMENT, api_project_id: UUID):
-    """
-    Fetch User Profile details with respect to the given environment and portfolio
-
-    Parameters
-    ----------
-    api_base_url
-        User Profile base url
-    token
-        Switch token for authentication
-    environment
-        Either development or production environment
-    api_project_id
-        Get UserProfile with respect to this portfolio id
-
-    Returns
-    -------
-    tuple[str, any]
-        Returns the User Profile details containing information to call Switch APIs
-    """
-
-    payload = {}
-    headers = {
-        'Content-Type': 'application/json; charset=utf-8',
-        'Authorization': f'bearer {token}'
-    }
-
-    url = f"{api_base_url}/user-profile?portfolioId={api_project_id}&switchEnvironment={environment}"
-
-    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, None
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, None
-    return response_status, response.json()
-
-
-def fetch_login_context(environment: SWITCH_ENVIRONMENT, api_project_id: UUID):
-    """
-    Fetches login context to assist SSO with Switch Platform
-
-    Parameters
-    ----------
-    environment
-        Either development or production environment
-    api_project_id
-        Get UserProfile with respect to this portfolio id
-
-    Returns
-    -------
-    tuple[str, any]
-        Returns the Login Context containing information to authenticate with Switch
-    """
-
-    base_url = AUTH_ENDPOINT_DEV if environment == 'Development' else AUTH_ENDPOINT_PROD
-
-    payload = {}
-    headers = {
-        'Content-Type': 'application/json; charset=utf-8'
-    }
-
-    url = f"{base_url}/login-context?portfolioId={api_project_id}&switchEnvironment={environment}&type=PyPkg"
-
-    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, None
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, None
-    return response_status, response.json()
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+import logging
+import sys
+from uuid import UUID
+from msal.oauth2cli.oidc import decode_id_token
+import requests
+from ._credentials_store._credentials_store import SwitchCredentials, store_credentials
+from ._credentials_store._credentials_store import read_credentials
+from ._msal._custom_application import CustomPublicClientApplication
+from .._utils._constants import AUTH_ENDPOINT_DEV, AUTH_ENDPOINT_PROD, SWITCH_ENVIRONMENT
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+_authority = {
+  "dev": {
+    "uitemplateid": "dev",
+    "base_url": "https://switchdevb2c.b2clogin.com/switchdevb2c.onmicrosoft.com",
+  },
+  "prod": {
+    "base_url": "https://switchautomation.b2clogin.com/switchautomation.onmicrosoft.com",
+    "uitemplateid": "default"
+  }
+}
+
+
+def _validate_credentials(creds: SwitchCredentials, version: str):
+    """Validate given Credentials
+
+    Ensures that credentials are up to date and synced.
+
+    Parameters
+    ----------
+    creds
+        SwitchCredentials to validate
+    version
+        The Credentials version
+
+    Returns
+    -------
+    bool
+        Returns the True when credentials are valid; otherwise returns False
+
+    """
+
+    if creds is None or creds is False or creds.user_id == '' or creds.user_id is None or creds.api_token is None:
+        return False
+
+    if creds.version != version:
+        return False
+
+    try:
+        decode_id_token(creds.api_token)
+        return True
+    except Exception as ex:
+        logger.info('Cached token expired. Requesting login.')
+        return False
+
+
+def get_switch_credentials(environment: SWITCH_ENVIRONMENT, portfolio_id, port: int):
+    """Read Credentials
+
+    Reads credentials from credential store
+
+    Parameters
+    ----------
+    environment
+        Either development or production environment
+    portfolio_id
+        Get credentials for the given portfolio id
+
+    Returns
+    -------
+    SwitchCredentials
+        Returns the SwitchCredentials namedtuple containing credentials to call Switch APIs
+
+    """
+
+    login_context = fetch_login_context(environment=environment, api_project_id=portfolio_id)[1]
+
+    # Fetch from credentials store if it's already been cached
+    if login_context is not None:
+        datacenter = login_context['dataCenter']
+        key_prefix = f"{environment}_{datacenter}" if environment == 'Development' else datacenter
+        cur_credentials = read_credentials(key_prefix, login_context['portfolioId'], login_context['portfolioName'])
+        if _validate_credentials(cur_credentials, login_context['version']):
+            return cur_credentials
+    else:
+        raise Exception('Unable to obtain Login Context to authenticate with Switch Automation. '
+                        'Please contact your administrator.')
+
+    # Credentials do not exist, take user through browser auth journey
+    config_env = 'dev' if environment == 'Development' else 'prod'
+    authority = f"{_authority[config_env]['base_url']}/{login_context['policyName']}"
+    app = CustomPublicClientApplication(login_context['applicationId'], authority=authority)
+
+    uitemplateid = _authority[config_env]['uitemplateid']
+    prompt = login_context['prompt'] if login_context['prompt'] != '' else None
+    auth_response = app.acquire_token_interactive_custom(scopes=[], prompt=prompt, port=port,
+                                                         auth_params={
+                                                             "uitemplateid": uitemplateid,
+                                                             "portfolioId": portfolio_id,
+                                                             "environment": environment
+    })
+
+    id_token = auth_response['id_token']
+
+    api_base_url = AUTH_ENDPOINT_DEV if environment == 'Development' else AUTH_ENDPOINT_PROD
+    _, profile = fetch_user_profile_details(api_base_url, id_token, environment, portfolio_id)
+
+    if profile is None:
+        raise Exception('Unable to obtain User Profile. Please try again or contact your administrator.')
+
+    credentials = SwitchCredentials(login_context['version'], profile['email'],
+                                    profile['userId'], login_context['portfolioId'],
+                                    login_context['portfolioName'], id_token, profile['apiEndpoint'],
+                                    environment, profile['subscriptionKey'])
+
+    store_credentials(credentials, key_prefix=key_prefix)
+
+    return credentials
+
+
+def fetch_user_profile_details(api_base_url: str, token: str, environment: SWITCH_ENVIRONMENT, api_project_id: UUID):
+    """
+    Fetch User Profile details with respect to the given environment and portfolio
+
+    Parameters
+    ----------
+    api_base_url
+        User Profile base url
+    token
+        Switch token for authentication
+    environment
+        Either development or production environment
+    api_project_id
+        Get UserProfile with respect to this portfolio id
+
+    Returns
+    -------
+    tuple[str, any]
+        Returns the User Profile details containing information to call Switch APIs
+    """
+
+    payload = {}
+    headers = {
+        'Content-Type': 'application/json; charset=utf-8',
+        'Authorization': f'bearer {token}'
+    }
+
+    url = f"{api_base_url}/user-profile?portfolioId={api_project_id}&switchEnvironment={environment}"
+
+    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, None
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, None
+    return response_status, response.json()
+
+
+def fetch_login_context(environment: SWITCH_ENVIRONMENT, api_project_id: UUID):
+    """
+    Fetches login context to assist SSO with Switch Platform
+
+    Parameters
+    ----------
+    environment
+        Either development or production environment
+    api_project_id
+        Get UserProfile with respect to this portfolio id
+
+    Returns
+    -------
+    tuple[str, any]
+        Returns the Login Context containing information to authenticate with Switch
+    """
+
+    base_url = AUTH_ENDPOINT_DEV if environment == 'Development' else AUTH_ENDPOINT_PROD
+
+    payload = {}
+    headers = {
+        'Content-Type': 'application/json; charset=utf-8'
+    }
+
+    url = f"{base_url}/login-context?portfolioId={api_project_id}&switchEnvironment={environment}&type=PyPkg"
+
+    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, None
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, None
+    return response_status, response.json()
```

### Comparing `switch_api-0.5.4b2/switch_api/_authentication/_credentials_store/_credentials_store.py` & `switch_api-0.5.4b3/switch_api/_authentication/_credentials_store/_credentials_store.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,182 +1,182 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-from collections import namedtuple
-from os import path
-from uuid import UUID
-
-cred_filename = '.env'
-
-SwitchCredentials = namedtuple('SwitchCredentials', ['version', 'email', 'user_id', 'portfolio_id', 'portfolio_name',
-                                                     'api_token', 'api_endpoint', 'environment', 'subscription_key'])
-
-
-def store_credentials(creds: SwitchCredentials, key_prefix: str = ''):
-    """
-    Store credentials
-
-    Parameters
-    ----------
-    creds
-        SwitchCredentials to store
-    key_prefix
-        Key prefix to use when storage type is Env
-        
-    Returns
-    -------
-    None
-
-    """
-
-    key_prefix = key_prefix.upper()
-
-    filename = _locate_creds_file(3)
-    if not filename:
-        filename = cred_filename
-
-    creds_dict = _read_credentials_file(filename)
-
-    creds_to_store = ''
-
-    # Avoid duplicating entries with the same key prefix    
-    if creds_dict:
-        for key in creds_dict.copy().keys():
-            if key.startswith(key_prefix):
-                del creds_dict[key]
-
-        # Ensure remaining keys are copied over
-        for key in creds_dict.keys():
-            creds_to_store += f"{key}={creds_dict[key]}\n"
-
-    with open(filename, 'w') as file_in:
-        creds_to_store += f"{key_prefix}_VERSION={creds.version}\n"
-        creds_to_store += f"{key_prefix}_EMAIL={creds.email}\n"
-        creds_to_store += f"{key_prefix}_USERID={creds.user_id}\n"
-        creds_to_store += f"{key_prefix}_APITOKEN={creds.api_token}\n"
-        creds_to_store += f"{key_prefix}_APIENDPOINT={creds.api_endpoint}\n"
-        creds_to_store += f"{key_prefix}_ENVIRONMENT={creds.environment}\n"
-        creds_to_store += f"{key_prefix}_SUBSCRIPTIONKEY={creds.subscription_key}\n"
-        file_in.write(creds_to_store)
-
-
-def read_credentials(key_prefix: str = '', portfolio_id: UUID = None, portfolio_name: str = ''):
-    """Read Credentials
-
-    Reads credentials from credential store
-
-    Parameters
-    ----------
-    key_prefix
-        Key prefix to use for the stored Keys
-    portfolio_id
-        Override output with given PortfolioId
-    portfolio_name
-        Override output with given PortfolioName
-
-    Returns
-    -------
-    SwitchCredentials
-        Returns the SwitchCredentials namedtuple containing credentials to call Switch APIs when found; otherwise False
-
-    """
-    key_prefix = key_prefix.upper()
-
-    try:
-        filepath = _locate_creds_file(num_dir_traversals=4)
-
-        if filepath == False:
-            return False
-
-        creds_dict = _read_credentials_file(filepath)
-
-        version_key = f"{key_prefix}_VERSION"
-        version = creds_dict[version_key] if version_key in creds_dict else '0'
-
-        creds = SwitchCredentials(
-            version,
-            creds_dict[f"{key_prefix}_EMAIL"],
-            creds_dict[f"{key_prefix}_USERID"],
-            portfolio_id,
-            portfolio_name,
-            creds_dict[f"{key_prefix}_APITOKEN"],
-            creds_dict[f"{key_prefix}_APIENDPOINT"],
-            creds_dict[f"{key_prefix}_ENVIRONMENT"],
-            creds_dict[f"{key_prefix}_SUBSCRIPTIONKEY"]
-        )
-
-        return creds
-    except Exception as e:
-        print('Unable to read credentials file.', e)
-        return False
-
-
-def clear_credentials(key_prefix: str = ''):
-    """Clear Credentials
-
-    Clears credentials from credential store
-
-    Parameters
-    ----------
-    storage_type
-        Type of storage for the credentials
-    key_prefix
-        Key prefix to use when storage type is Env
-
-    Returns
-    -------
-    None
-
-    """
-    key_prefix = key_prefix.upper()
-
-    filename = _locate_creds_file(num_dir_traversals=4)
-
-    store_credentials(SwitchCredentials('', '', '', '', '', '', '', '', ''), filename, key_prefix)
-
-
-def _locate_creds_file(num_dir_traversals):
-    """
-    Credentials File search begins from the directory that the python script is executed from.
-    Traversing back up dir tree could help us locate an existing creds file
-    We will traverse given number of times. If file is still not found, user will need to login again.
-
-    Limitations: Credentials file are created where a script is executed from.
-                 This could result in multiple credentials file.
-    Future: Investigate physical Credentials file alternatives.
-    """
-
-    try:
-        # First try current directory 
-        if path.isfile(cred_filename):
-            return cred_filename
-
-        # Then look at /switch directory. Only relevant during switch lib development.
-        if path.isfile(f"switch/{cred_filename}"):
-            return f"switch/{cred_filename}"
-
-        # Then traverse down the directory tree given number of times
-        # There may be times when authentication is run from a root directory
-        # but script run from a subdirectory. This will help us find credentials file in the root dir.
-        filepath = cred_filename
-        for _ in range(num_dir_traversals):
-            if path.isfile(filepath):
-                # print(f"found credentials file under {path.abspath(filepath)}")
-                return filepath
-            else:
-                # print(f"looked for credentials file under {path.abspath(filepath)}")
-                filepath = f"../{filepath}"
-    except:
-        return False
-    return False
-
-
-def _read_credentials_file(filepath: str):
-    if not path.isfile(filepath):
-        return False
-
-    with open(filepath, 'r') as file:
-        file_content = file.read().splitlines()
-
-    return dict(line.strip().split('=') for line in file_content)
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+from collections import namedtuple
+from os import path
+from uuid import UUID
+
+cred_filename = '.env'
+
+SwitchCredentials = namedtuple('SwitchCredentials', ['version', 'email', 'user_id', 'portfolio_id', 'portfolio_name',
+                                                     'api_token', 'api_endpoint', 'environment', 'subscription_key'])
+
+
+def store_credentials(creds: SwitchCredentials, key_prefix: str = ''):
+    """
+    Store credentials
+
+    Parameters
+    ----------
+    creds
+        SwitchCredentials to store
+    key_prefix
+        Key prefix to use when storage type is Env
+        
+    Returns
+    -------
+    None
+
+    """
+
+    key_prefix = key_prefix.upper()
+
+    filename = _locate_creds_file(3)
+    if not filename:
+        filename = cred_filename
+
+    creds_dict = _read_credentials_file(filename)
+
+    creds_to_store = ''
+
+    # Avoid duplicating entries with the same key prefix    
+    if creds_dict:
+        for key in creds_dict.copy().keys():
+            if key.startswith(key_prefix):
+                del creds_dict[key]
+
+        # Ensure remaining keys are copied over
+        for key in creds_dict.keys():
+            creds_to_store += f"{key}={creds_dict[key]}\n"
+
+    with open(filename, 'w') as file_in:
+        creds_to_store += f"{key_prefix}_VERSION={creds.version}\n"
+        creds_to_store += f"{key_prefix}_EMAIL={creds.email}\n"
+        creds_to_store += f"{key_prefix}_USERID={creds.user_id}\n"
+        creds_to_store += f"{key_prefix}_APITOKEN={creds.api_token}\n"
+        creds_to_store += f"{key_prefix}_APIENDPOINT={creds.api_endpoint}\n"
+        creds_to_store += f"{key_prefix}_ENVIRONMENT={creds.environment}\n"
+        creds_to_store += f"{key_prefix}_SUBSCRIPTIONKEY={creds.subscription_key}\n"
+        file_in.write(creds_to_store)
+
+
+def read_credentials(key_prefix: str = '', portfolio_id: UUID = None, portfolio_name: str = ''):
+    """Read Credentials
+
+    Reads credentials from credential store
+
+    Parameters
+    ----------
+    key_prefix
+        Key prefix to use for the stored Keys
+    portfolio_id
+        Override output with given PortfolioId
+    portfolio_name
+        Override output with given PortfolioName
+
+    Returns
+    -------
+    SwitchCredentials
+        Returns the SwitchCredentials namedtuple containing credentials to call Switch APIs when found; otherwise False
+
+    """
+    key_prefix = key_prefix.upper()
+
+    try:
+        filepath = _locate_creds_file(num_dir_traversals=4)
+
+        if filepath == False:
+            return False
+
+        creds_dict = _read_credentials_file(filepath)
+
+        version_key = f"{key_prefix}_VERSION"
+        version = creds_dict[version_key] if version_key in creds_dict else '0'
+
+        creds = SwitchCredentials(
+            version,
+            creds_dict[f"{key_prefix}_EMAIL"],
+            creds_dict[f"{key_prefix}_USERID"],
+            portfolio_id,
+            portfolio_name,
+            creds_dict[f"{key_prefix}_APITOKEN"],
+            creds_dict[f"{key_prefix}_APIENDPOINT"],
+            creds_dict[f"{key_prefix}_ENVIRONMENT"],
+            creds_dict[f"{key_prefix}_SUBSCRIPTIONKEY"]
+        )
+
+        return creds
+    except Exception as e:
+        print('Unable to read credentials file.', e)
+        return False
+
+
+def clear_credentials(key_prefix: str = ''):
+    """Clear Credentials
+
+    Clears credentials from credential store
+
+    Parameters
+    ----------
+    storage_type
+        Type of storage for the credentials
+    key_prefix
+        Key prefix to use when storage type is Env
+
+    Returns
+    -------
+    None
+
+    """
+    key_prefix = key_prefix.upper()
+
+    filename = _locate_creds_file(num_dir_traversals=4)
+
+    store_credentials(SwitchCredentials('', '', '', '', '', '', '', '', ''), filename, key_prefix)
+
+
+def _locate_creds_file(num_dir_traversals):
+    """
+    Credentials File search begins from the directory that the python script is executed from.
+    Traversing back up dir tree could help us locate an existing creds file
+    We will traverse given number of times. If file is still not found, user will need to login again.
+
+    Limitations: Credentials file are created where a script is executed from.
+                 This could result in multiple credentials file.
+    Future: Investigate physical Credentials file alternatives.
+    """
+
+    try:
+        # First try current directory 
+        if path.isfile(cred_filename):
+            return cred_filename
+
+        # Then look at /switch directory. Only relevant during switch lib development.
+        if path.isfile(f"switch/{cred_filename}"):
+            return f"switch/{cred_filename}"
+
+        # Then traverse down the directory tree given number of times
+        # There may be times when authentication is run from a root directory
+        # but script run from a subdirectory. This will help us find credentials file in the root dir.
+        filepath = cred_filename
+        for _ in range(num_dir_traversals):
+            if path.isfile(filepath):
+                # print(f"found credentials file under {path.abspath(filepath)}")
+                return filepath
+            else:
+                # print(f"looked for credentials file under {path.abspath(filepath)}")
+                filepath = f"../{filepath}"
+    except:
+        return False
+    return False
+
+
+def _read_credentials_file(filepath: str):
+    if not path.isfile(filepath):
+        return False
+
+    with open(filepath, 'r') as file:
+        file_content = file.read().splitlines()
+
+    return dict(line.strip().split('=') for line in file_content)
```

### Comparing `switch_api-0.5.4b2/switch_api/_authentication/_msal/_custom_application.py` & `switch_api-0.5.4b3/switch_api/_authentication/_msal/_custom_application.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,173 +1,173 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-import json
-import logging
-import os
-import sys
-from typing import Optional, List
-
-from msal import PublicClientApplication
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  %(name)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-def _merge_claims_challenge_and_capabilities(capabilities, claims_challenge):
-    """
-    Represent capabilities as {"access_token": {"xms_cc": {"values": capabilities}}} and then merge/add it into
-    incoming claims
-    """
-    if not capabilities:
-        return claims_challenge
-    claims_dict = json.loads(claims_challenge) if claims_challenge else {}
-    for key in ["access_token"]:  # We could add "id_token" if we'd decide to
-        claims_dict.setdefault(key, {}).update(xms_cc={"values": capabilities})
-    return json.dumps(claims_dict)
-
-
-def _clean_up(result):
-    if isinstance(result, dict):
-        result.pop("refresh_in", None)  # MSAL handled refresh_in, customers need not
-    return result
-
-
-def _preferred_browser():
-    """Register Edge and return a name suitable for subsequent webbrowser.get(...)
-    when appropriate. Otherwise return None.
-    """
-    # On Linux, only Edge will provide device-based Conditional Access support
-    if sys.platform != "linux":  # On other platforms, we have no browser preference
-        return None
-    browser_path = "/usr/bin/microsoft-edge"  # Use a full path owned by sys admin
-    user_has_no_preference = "BROWSER" not in os.environ
-    user_wont_mind_edge = "microsoft-edge" in os.environ.get("BROWSER", "")  # Note:
-    # BROWSER could contain "microsoft-edge" or "/path/to/microsoft-edge".
-    # Python documentation (https://docs.python.org/3/library/webbrowser.html)
-    # does not document the name being implicitly register,
-    # so there is no public API to know whether the ENV VAR browser would work.
-    # Therefore, we would not bother examine the env var browser's type.
-    # We would just register our own Edge instance.
-    if (user_has_no_preference or user_wont_mind_edge) and os.path.exists(browser_path):
-        try:
-            import webbrowser  # Lazy import. Some distro may not have this.
-            browser_name = "msal-edge"  # Avoid popular name "microsoft-edge"
-            # otherwise `BROWSER="microsoft-edge"; webbrowser.get("microsoft-edge")`
-            # would return a GenericBrowser instance which won't work.
-            try:
-                registration_available = isinstance(
-                    webbrowser.get(browser_name), webbrowser.BackgroundBrowser)
-            except webbrowser.Error:
-                registration_available = False
-            if not registration_available:
-                logger.debug("Register %s with %s", browser_name, browser_path)
-                # By registering our own browser instance with our own name,
-                # rather than populating a process-wide BROWSER enn var,
-                # this approach does not have side effect on non-MSAL code path.
-                webbrowser.register(  # Even double-register happens to work fine
-                    browser_name, None, webbrowser.BackgroundBrowser(browser_path))
-            return browser_name
-        except ImportError:
-            pass  # We may still proceed
-    return None
-
-
-class CustomPublicClientApplication(PublicClientApplication):
-    """
-    Extends Msal PublicClientApplication to support custom Query Parameters.
-    Future: Request feature in python msal GitHub
-    """
-
-    def __init__(self, client_id, client_credential=None, **kwargs):
-        if client_credential is not None:
-            raise ValueError("Public Client should not possess credentials")
-        super(PublicClientApplication, self).__init__(
-            client_id, client_credential=None, **kwargs)
-
-    def acquire_token_interactive_custom(self, scopes: List[str], prompt=None, login_hint: str=None,
-                                         domain_hint: str=None, claims_challenge: str=None, timeout: int=None,
-                                         port: int=None, extra_scopes_to_consent=None, auth_params: dict=None, **kwargs):
-        """Acquire token interactively i.e. via a local browser.
-        Overridden to support custom query parameters.
-
-        Prerequisite: In Azure Portal, configure the Redirect URI of your
-        "Mobile and Desktop application" as ``http://localhost``.
-
-        Parameters
-        ----------
-        scopes : List[str]
-            It is a list of case-sensitive strings.
-        prompt : str
-            By default, no prompt value will be sent, not even "none". You will have to specify a value explicitly. Its
-            valid values are defined in Open ID Connect specs
-            https://openid.net/specs/openid-connect-core-1_0.html#AuthRequest
-        login_hint : str
-            Identifier of the user. Generally a User Principal Name (UPN) (Default value = None).
-        domain_hint : str
-            Can be one of "consumers" or "organizations" or your tenant domain "contoso.com". If included, it will skip
-            the email-based discovery process that user goes through on the sign-in page, leading to a slightly more
-            streamlined user experience. More information on possible values:
-            `here <https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-auth-code-flow#request-an-
-            authorization-code>`_ and
-            `here <https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-oapx/
-            86fb452d-e34a-494e-ac61-e526e263b6d8>`_.  (Default value = None)
-        claims_challenge : str
-            The claims_challenge parameter requests specific claims requested by the resource provider in the form of a
-            claims_challenge directive in the www-authenticate header to be returned from the UserInfo Endpoint and/or
-            in the ID Token and/or Access Token. It is a string of a JSON object which contains lists of claims being
-            requested from these locations (Default value = None).
-        timeout : int
-            This method will block the current thread. This parameter specifies the timeout value in seconds. Default
-            value ``None`` means wait indefinitely.
-        port : int
-            The port to be used to listen to an incoming auth response.
-            By default we will use a system-allocated port.
-            (The rest of the redirect_uri is hard coded as ``http://localhost``.)
-        extra_scopes_to_consent : list
-            "Extra scopes to consent" is a concept only available in AAD. It refers to other resources you might want
-            to prompt to consent for, in the same interaction, but for which you won't get back a token for in this
-            particular operation.
-        auth_params : dict
-            "Additional Auth Query Params to be sent to STS"
-
-        Returns
-        -------
-        dict
-            A dict containing no "error" key, and typically contains an "access_token" key. A dict containing an
-            "error" key, when token refresh failed.
-
-        """
-        self._validate_ssh_cert_input_data(kwargs.get("data", {}))
-        claims = _merge_claims_challenge_and_capabilities(
-            self._client_capabilities, claims_challenge)
-        telemetry_context = self._build_telemetry_context(
-            self.ACQUIRE_TOKEN_INTERACTIVE)
-
-        auth_params['claims'] = claims
-        auth_params['domain_hint'] = domain_hint
-
-        response = _clean_up(self.client.obtain_token_by_browser(
-            scope=self._decorate_scope(scopes) if scopes else None,
-            extra_scope_to_consent=extra_scopes_to_consent,
-            redirect_uri="http://localhost:{port}".format(
-                # Hardcode the host, for now. AAD portal rejects 127.0.0.1 anyway
-                port=port or 0),
-            prompt=prompt,
-            login_hint=login_hint,
-            timeout=timeout,
-            auth_params=auth_params,
-            data=dict(kwargs.pop("data", {}), claims=claims),
-            headers=telemetry_context.generate_headers(),
-            browser_name=_preferred_browser(),
-            **kwargs))
-        telemetry_context.update_telemetry(response)
-        return response
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+import json
+import logging
+import os
+import sys
+from typing import Optional, List
+
+from msal import PublicClientApplication
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  %(name)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+def _merge_claims_challenge_and_capabilities(capabilities, claims_challenge):
+    """
+    Represent capabilities as {"access_token": {"xms_cc": {"values": capabilities}}} and then merge/add it into
+    incoming claims
+    """
+    if not capabilities:
+        return claims_challenge
+    claims_dict = json.loads(claims_challenge) if claims_challenge else {}
+    for key in ["access_token"]:  # We could add "id_token" if we'd decide to
+        claims_dict.setdefault(key, {}).update(xms_cc={"values": capabilities})
+    return json.dumps(claims_dict)
+
+
+def _clean_up(result):
+    if isinstance(result, dict):
+        result.pop("refresh_in", None)  # MSAL handled refresh_in, customers need not
+    return result
+
+
+def _preferred_browser():
+    """Register Edge and return a name suitable for subsequent webbrowser.get(...)
+    when appropriate. Otherwise return None.
+    """
+    # On Linux, only Edge will provide device-based Conditional Access support
+    if sys.platform != "linux":  # On other platforms, we have no browser preference
+        return None
+    browser_path = "/usr/bin/microsoft-edge"  # Use a full path owned by sys admin
+    user_has_no_preference = "BROWSER" not in os.environ
+    user_wont_mind_edge = "microsoft-edge" in os.environ.get("BROWSER", "")  # Note:
+    # BROWSER could contain "microsoft-edge" or "/path/to/microsoft-edge".
+    # Python documentation (https://docs.python.org/3/library/webbrowser.html)
+    # does not document the name being implicitly register,
+    # so there is no public API to know whether the ENV VAR browser would work.
+    # Therefore, we would not bother examine the env var browser's type.
+    # We would just register our own Edge instance.
+    if (user_has_no_preference or user_wont_mind_edge) and os.path.exists(browser_path):
+        try:
+            import webbrowser  # Lazy import. Some distro may not have this.
+            browser_name = "msal-edge"  # Avoid popular name "microsoft-edge"
+            # otherwise `BROWSER="microsoft-edge"; webbrowser.get("microsoft-edge")`
+            # would return a GenericBrowser instance which won't work.
+            try:
+                registration_available = isinstance(
+                    webbrowser.get(browser_name), webbrowser.BackgroundBrowser)
+            except webbrowser.Error:
+                registration_available = False
+            if not registration_available:
+                logger.debug("Register %s with %s", browser_name, browser_path)
+                # By registering our own browser instance with our own name,
+                # rather than populating a process-wide BROWSER enn var,
+                # this approach does not have side effect on non-MSAL code path.
+                webbrowser.register(  # Even double-register happens to work fine
+                    browser_name, None, webbrowser.BackgroundBrowser(browser_path))
+            return browser_name
+        except ImportError:
+            pass  # We may still proceed
+    return None
+
+
+class CustomPublicClientApplication(PublicClientApplication):
+    """
+    Extends Msal PublicClientApplication to support custom Query Parameters.
+    Future: Request feature in python msal GitHub
+    """
+
+    def __init__(self, client_id, client_credential=None, **kwargs):
+        if client_credential is not None:
+            raise ValueError("Public Client should not possess credentials")
+        super(PublicClientApplication, self).__init__(
+            client_id, client_credential=None, **kwargs)
+
+    def acquire_token_interactive_custom(self, scopes: List[str], prompt=None, login_hint: str=None,
+                                         domain_hint: str=None, claims_challenge: str=None, timeout: int=None,
+                                         port: int=None, extra_scopes_to_consent=None, auth_params: dict=None, **kwargs):
+        """Acquire token interactively i.e. via a local browser.
+        Overridden to support custom query parameters.
+
+        Prerequisite: In Azure Portal, configure the Redirect URI of your
+        "Mobile and Desktop application" as ``http://localhost``.
+
+        Parameters
+        ----------
+        scopes : List[str]
+            It is a list of case-sensitive strings.
+        prompt : str
+            By default, no prompt value will be sent, not even "none". You will have to specify a value explicitly. Its
+            valid values are defined in Open ID Connect specs
+            https://openid.net/specs/openid-connect-core-1_0.html#AuthRequest
+        login_hint : str
+            Identifier of the user. Generally a User Principal Name (UPN) (Default value = None).
+        domain_hint : str
+            Can be one of "consumers" or "organizations" or your tenant domain "contoso.com". If included, it will skip
+            the email-based discovery process that user goes through on the sign-in page, leading to a slightly more
+            streamlined user experience. More information on possible values:
+            `here <https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-auth-code-flow#request-an-
+            authorization-code>`_ and
+            `here <https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-oapx/
+            86fb452d-e34a-494e-ac61-e526e263b6d8>`_.  (Default value = None)
+        claims_challenge : str
+            The claims_challenge parameter requests specific claims requested by the resource provider in the form of a
+            claims_challenge directive in the www-authenticate header to be returned from the UserInfo Endpoint and/or
+            in the ID Token and/or Access Token. It is a string of a JSON object which contains lists of claims being
+            requested from these locations (Default value = None).
+        timeout : int
+            This method will block the current thread. This parameter specifies the timeout value in seconds. Default
+            value ``None`` means wait indefinitely.
+        port : int
+            The port to be used to listen to an incoming auth response.
+            By default we will use a system-allocated port.
+            (The rest of the redirect_uri is hard coded as ``http://localhost``.)
+        extra_scopes_to_consent : list
+            "Extra scopes to consent" is a concept only available in AAD. It refers to other resources you might want
+            to prompt to consent for, in the same interaction, but for which you won't get back a token for in this
+            particular operation.
+        auth_params : dict
+            "Additional Auth Query Params to be sent to STS"
+
+        Returns
+        -------
+        dict
+            A dict containing no "error" key, and typically contains an "access_token" key. A dict containing an
+            "error" key, when token refresh failed.
+
+        """
+        self._validate_ssh_cert_input_data(kwargs.get("data", {}))
+        claims = _merge_claims_challenge_and_capabilities(
+            self._client_capabilities, claims_challenge)
+        telemetry_context = self._build_telemetry_context(
+            self.ACQUIRE_TOKEN_INTERACTIVE)
+
+        auth_params['claims'] = claims
+        auth_params['domain_hint'] = domain_hint
+
+        response = _clean_up(self.client.obtain_token_by_browser(
+            scope=self._decorate_scope(scopes) if scopes else None,
+            extra_scope_to_consent=extra_scopes_to_consent,
+            redirect_uri="http://localhost:{port}".format(
+                # Hardcode the host, for now. AAD portal rejects 127.0.0.1 anyway
+                port=port or 0),
+            prompt=prompt,
+            login_hint=login_hint,
+            timeout=timeout,
+            auth_params=auth_params,
+            data=dict(kwargs.pop("data", {}), claims=claims),
+            headers=telemetry_context.generate_headers(),
+            browser_name=_preferred_browser(),
+            **kwargs))
+        telemetry_context.update_telemetry(response)
+        return response
```

### Comparing `switch_api-0.5.4b2/switch_api/_utils/_constants.py` & `switch_api-0.5.4b3/switch_api/_utils/_constants.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,94 +1,90 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module defining constants referenced by methods and functions defined in  other modules in the package. This module
-is not directly referenced by end users.
-"""
-from typing import Literal
-__all__ = ['api_prefix', 'argus_prefix', 'DATETIME_COL_FMT', 'DEPLOY_TYPE', 'EXPECTED_DELIVERY', 'WORK_ORDER_PRIORITY',
-           'WORK_ORDER_STATUS', 'WORK_ORDER_CATEGORY', 'ERROR_TYPE', 'MAPPING_ENTITIES', 'QUEUE_NAME', 'QUERY_LANGUAGE',
-           'ACCOUNT', 'PROCESS_STATUS', 'AUTH_ENDPOINT_DEV', 'AUTH_ENDPOINT_PROD', 'SCHEDULE_TIMEZONE',
-           'SWITCH_ENVIRONMENT', 'INTEGRATION_SETTINGS_EDITORS', 'DATA_INGESTION_CONTAINER', 'CACHE_SCOPE', 'TAG_LEVEL']
-
-# @deprecated suggestion: Up for deprecation as this is not used anymore in the API
-# api_prefix = "http://localhost:7071/api/SwitchApi/"
-api_prefix = "https://pivotstreams.azurewebsites.net/api/SwitchApi/"
-
-# @deprecated suggestion: Up for deprecation as this is not used anymore in the API (except for automation.reserve_instance)
-# argus_prefix = "http://localhost:7071/api/"
-argus_prefix = "https://arguslogicv4b.azurewebsites.net/api/"
-
-AUTH_ENDPOINT_DEV = "https://restapi-dev.switchautomation.com/auth"
-AUTH_ENDPOINT_PROD = "https://restapi-us.switchautomation.com/auth"
-
-DATETIME_COL_FMT = Literal['DateTime', 'Date', 'Time']
-
-# ['DateTime in 1 column',
-#  'Date and Time in 2 columns',
-#  'Split year and month in 2 columns',
-#  'Start and End Date in 2 columns']
-
-EXPECTED_DELIVERY = Literal['5min', '15min', 'Hourly', 'Daily', 'Weekly', 'Monthly', 'Quarterly']
-
-# deploy_type 'File' for (FTP. Email, Upload)
-DEPLOY_TYPE = Literal['Email', 'Ftp', 'Upload', 'Timer']
-
-ACCOUNT = Literal['SwitchStorage', 'SwitchContainer', 'Argus', 'DataIngestion', 'GatewayMqttStorage']
-
-DATA_INGESTION_CONTAINER = Literal['data-ingestion-adx', 'data-ingestion-timeseries-adx']
-
-QUERY_LANGUAGE = Literal['sql', 'kql']
-
-RESPONSE_TYPE = Literal['dataframe', 'json', 'string']
-
-ERROR_TYPE = Literal['DateTime', 'MissingDevices', 'NonNumeric', 'MissingRequiredField(s)', 'MissingSite(s)',
-                     'DuplicateRecords', 'MissingSensors', 'UnableToReadFile', 'InvalidFileType', 'InvalidInputSettings']
-
-PROCESS_STATUS = Literal['ActionRequired', 'Failed']
-
-MAPPING_ENTITIES = Literal['Installations', 'Devices/Sensors', 'Readings', 'Work Orders']
-
-WORK_ORDER_CATEGORY = Literal['Preventative Maintenance', 'Tenant Request']
-
-WORK_ORDER_PRIORITY = Literal['Low', 'Medium', 'High']
-
-WORK_ORDER_STATUS = Literal['Submitted', 'Open', 'In Progress', 'Waiting for 3rd Party', 'Resolved', 'Abandoned',
-                            'Closed']
-
-RESERVATION_STATUS = Literal['Booked', 'In Progress', 'Complete', 'Cancelled']
-
-RESOURCE_TYPE = Literal['MeetingRoom', 'ConferenceCentre', 'Desk', 'Other']
-
-QUEUE_NAME = Literal['task', 'highpriority']
-
-SCHEDULE_TIMEZONE = Literal['Local', 'Utc']
-
-SWITCH_ENVIRONMENT = Literal['Development', 'Staging', 'Testing', 'Production']
-
-SUPPORT_PAYLOAD_TYPE = Literal['Sites', 'Sensors', 'Portfolio']
-
-INTEGRATION_SETTINGS_EDITORS = Literal['text_box', 'numeric_stepper', 'custom_combo', 'tag_groups_combo',
-                                       'equipment_combo']
-
-CACHE_SCOPE = Literal['Portfolio', 'Task', 'DataFeed']
-TAG_LEVEL = Literal['Site', 'Device', 'Sensor']
-
-DATA_SET_QUERY_PARAMETER_TYPES = Literal['String', 'Number', 'DateTime', 'Keyword', 'Boolean', 'NumberArray',
-                                         'StringArray']
-
-ADX_TABLE_DEF_TYPES = Literal['object', 'dynamic', 'int32', 'int64', 'float32', 'float64', 'datetime', 'string']
-
-TASK_PRIORITY = Literal['default', 'standard', 'advanced']
-
-TASK_FRAMEWORK = Literal['PythonScriptFramework', 'TaskInsightsEngine']
-
-AMORTISATION_METHOD = Literal['Exclusive', 'Inclusive']
-
-PERFORMANCE_STATISTIC_METRIC_SYSTEM = Literal['metric', 'imperial']
-
-GUIDES_EXTERNAL_TYPES = Literal['SwitchGuides']
-
-GUIDES_SCOPE = Literal['Portfolio-wide']
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module defining constants referenced by methods and functions defined in  other modules in the package. This module
+is not directly referenced by end users.
+"""
+from typing import Literal
+__all__ = ['api_prefix', 'argus_prefix', 'DATETIME_COL_FMT', 'DEPLOY_TYPE', 'EXPECTED_DELIVERY', 'WORK_ORDER_PRIORITY',
+           'WORK_ORDER_STATUS', 'WORK_ORDER_CATEGORY', 'ERROR_TYPE', 'MAPPING_ENTITIES', 'QUEUE_NAME', 'QUERY_LANGUAGE',
+           'ACCOUNT', 'PROCESS_STATUS', 'AUTH_ENDPOINT_DEV', 'AUTH_ENDPOINT_PROD', 'SCHEDULE_TIMEZONE',
+           'SWITCH_ENVIRONMENT', 'INTEGRATION_SETTINGS_EDITORS', 'DATA_INGESTION_CONTAINER', 'CACHE_SCOPE', 'TAG_LEVEL']
+
+# @deprecated suggestion: Up for deprecation as this is not used anymore in the API
+# api_prefix = "http://localhost:7071/api/SwitchApi/"
+api_prefix = "https://pivotstreams.azurewebsites.net/api/SwitchApi/"
+
+# @deprecated suggestion: Up for deprecation as this is not used anymore in the API (except for automation.reserve_instance)
+# argus_prefix = "http://localhost:7071/api/"
+argus_prefix = "https://arguslogicv4b.azurewebsites.net/api/"
+
+AUTH_ENDPOINT_DEV = "https://restapi-dev.switchautomation.com/auth"
+AUTH_ENDPOINT_PROD = "https://restapi-us.switchautomation.com/auth"
+
+DATETIME_COL_FMT = Literal['DateTime', 'Date', 'Time']
+
+# ['DateTime in 1 column',
+#  'Date and Time in 2 columns',
+#  'Split year and month in 2 columns',
+#  'Start and End Date in 2 columns']
+
+EXPECTED_DELIVERY = Literal['5min', '15min', 'Hourly', 'Daily', 'Weekly', 'Monthly', 'Quarterly']
+
+# deploy_type 'File' for (FTP. Email, Upload)
+DEPLOY_TYPE = Literal['Email', 'Ftp', 'Upload', 'Timer']
+
+ACCOUNT = Literal['SwitchStorage', 'SwitchContainer', 'Argus', 'DataIngestion', 'GatewayMqttStorage']
+
+DATA_INGESTION_CONTAINER = Literal['data-ingestion-adx', 'data-ingestion-timeseries-adx']
+
+QUERY_LANGUAGE = Literal['sql', 'kql']
+
+RESPONSE_TYPE = Literal['dataframe', 'json', 'string']
+
+ERROR_TYPE = Literal['DateTime', 'MissingDevices', 'NonNumeric', 'MissingRequiredField(s)', 'MissingSite(s)',
+                     'DuplicateRecords', 'MissingSensors', 'UnableToReadFile', 'InvalidFileType', 'InvalidInputSettings']
+
+PROCESS_STATUS = Literal['ActionRequired', 'Failed']
+
+MAPPING_ENTITIES = Literal['Installations', 'Devices/Sensors', 'Readings', 'Work Orders']
+
+WORK_ORDER_CATEGORY = Literal['Preventative Maintenance', 'Tenant Request']
+
+WORK_ORDER_PRIORITY = Literal['Low', 'Medium', 'High']
+
+WORK_ORDER_STATUS = Literal['Submitted', 'Open', 'In Progress', 'Waiting for 3rd Party', 'Resolved', 'Abandoned',
+                            'Closed']
+
+QUEUE_NAME = Literal['task', 'highpriority']
+
+SCHEDULE_TIMEZONE = Literal['Local', 'Utc']
+
+SWITCH_ENVIRONMENT = Literal['Development', 'Staging', 'Testing', 'Production']
+
+SUPPORT_PAYLOAD_TYPE = Literal['Sites', 'Sensors', 'Portfolio']
+
+INTEGRATION_SETTINGS_EDITORS = Literal['text_box', 'numeric_stepper', 'custom_combo', 'tag_groups_combo',
+                                       'equipment_combo']
+
+CACHE_SCOPE = Literal['Portfolio', 'Task', 'DataFeed']
+TAG_LEVEL = Literal['Site', 'Device', 'Sensor']
+
+DATA_SET_QUERY_PARAMETER_TYPES = Literal['String', 'Number', 'DateTime', 'Keyword', 'Boolean', 'NumberArray',
+                                         'StringArray']
+
+ADX_TABLE_DEF_TYPES = Literal['object', 'dynamic', 'int32', 'int64', 'float32', 'float64', 'datetime', 'string']
+
+TASK_PRIORITY = Literal['default', 'standard', 'advanced']
+
+TASK_FRAMEWORK = Literal['PythonScriptFramework', 'TaskInsightsEngine']
+
+AMORTISATION_METHOD = Literal['Exclusive', 'Inclusive']
+
+PERFORMANCE_STATISTIC_METRIC_SYSTEM = Literal['metric', 'imperial']
+
+GUIDES_EXTERNAL_TYPES = Literal['SwitchGuides']
+
+GUIDES_SCOPE = Literal['Portfolio-wide']
```

### Comparing `switch_api-0.5.4b2/switch_api/_utils/_marketplace.py` & `switch_api-0.5.4b3/switch_api/_utils/_marketplace.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,146 +1,146 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for .....
-"""
-import logging
-import os
-import sys
-import uuid
-import pandas
-import requests
-from .._utils._utils import ApiInputs, _column_name_cap
-from .._utils._constants import GUIDES_EXTERNAL_TYPES, GUIDES_SCOPE
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-def add_marketplace_item(api_inputs: ApiInputs, name: str, short_description: str, description: str,
-                         tags: dict, external_id: uuid.UUID, card_image_file_name: str = '',
-                         external_type: GUIDES_EXTERNAL_TYPES = 'SwitchGuides', scope: GUIDES_SCOPE = 'Portfolio-wide',
-                         image_file_name: str = '', version: str = ''):
-    """Add datafeed driver to Marketplace Items
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Provides state for calling Switch Platform APIs.
-    name : str
-        Name of the Task when added to the Marketplace Items
-    short_description : str
-        Short description for the Marketplace Item registration. Max 250 characters.
-    description : str
-        Full description for the Marketplace Item registration
-    tags : dict
-        Tags for Marketplace item registration. Required atleast 1 tag
-    external_id : uuid.UUID
-        The Marketplace ID (defined on the task).
-    card_image_file_name : str, optional
-        Card Image File Name for Marketplace Items. Defaults to ''.
-    external_type : GUIDES_EXTERNAL_TYPES, optional
-        Marketplaec External Type. Defaults to 'SwitchGuides'.
-    scope : GUIDES_SCOPE, optional
-        Marketplace item scope. Defaults to 'Portfolio-wide'.
-    image_file_name : str, optional
-        For Marketplace item image. Defaults to ''.
-    version : str, optional
-        For Marketplace item versioning. Defaults to ''.
-
-    Returns
-    -------
-    pandas.Dataframe
-    """
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if not name.strip():
-        logger.exception("The name property cannot be empty.")
-        raise Exception("The name property cannot be empty.")
-    if not description.strip():
-        logger.exception("The description property cannot be empty.")
-        raise Exception("The description property cannot be empty.")
-    if not short_description.strip():
-        logger.exception("The short_description property cannot be empty.")
-        raise Exception("The short_description property cannot be empty.")
-    if type(tags) != dict:
-        logger.exception("The tags property must be a dictionary with at least one key. ")
-        raise Exception("The tags property must be a dictionary with at least one key. ")
-    if len(tags.keys()) < 1:
-        logger.exception("The tags property must be a dictionary with at least one key. ")
-        raise Exception("The tags property must be a dictionary with at least one key. ")
-
-    if type(image_file_name) != str:
-        logger.exception("The image_file_name property must be a string. ")
-        raise Exception("The image_file_name property must be a string. ")
-    if type(card_image_file_name) != str:
-        logger.exception("The card_image_file_name property be a string. ")
-        raise Exception("The card_image_file_name property must be a string. ")
-
-    headers = api_inputs.api_headers.default
-
-    # enforce these values
-    scope = 'Portfolio-wide'
-    external_type = 'SwitchGuides'
-    container_name = 'guides-form'
-
-    if not version.strip():
-        version = '0.1.0'
-
-    image_url = f'{image_file_name}'
-    card_image_url = f'{card_image_file_name}'
-
-    payload = {
-        "name": name,
-        "shortDescription": short_description,
-        "description": description,
-        "version": version,
-        "externalType": external_type,
-        "externalId": external_id,
-        "tags": tags,
-        "scope": scope,
-        "imageUrl": image_url,
-        "cardImageUrl": card_image_url
-    }
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/guide/register"
-
-    logger.info("Sending request: POST %s", url)
-
-    response = requests.post(url, json=payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-
-    if response.status_code != 200 and len(response.text) > 0:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        error_df = pandas.read_json(response.text)
-        return response_status, error_df
-    elif response.status_code != 200 and len(response.text) == 0:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s',
-                     response.request.url)
-        return response_status, pandas.DataFrame()
-
-    if (response.text == '[]'):
-        logger.info('No changes with the records from Marketplace.')
-        df = pandas.DataFrame()
-    else:
-        df = pandas.read_json(response.text, typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-    return df
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for .....
+"""
+import logging
+import os
+import sys
+import uuid
+import pandas
+import requests
+from .._utils._utils import ApiInputs, _column_name_cap
+from .._utils._constants import GUIDES_EXTERNAL_TYPES, GUIDES_SCOPE
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+def add_marketplace_item(api_inputs: ApiInputs, name: str, short_description: str, description: str,
+                         tags: dict, external_id: uuid.UUID, card_image_file_name: str = '',
+                         external_type: GUIDES_EXTERNAL_TYPES = 'SwitchGuides', scope: GUIDES_SCOPE = 'Portfolio-wide',
+                         image_file_name: str = '', version: str = ''):
+    """Add datafeed driver to Marketplace Items
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Provides state for calling Switch Platform APIs.
+    name : str
+        Name of the Task when added to the Marketplace Items
+    short_description : str
+        Short description for the Marketplace Item registration. Max 250 characters.
+    description : str
+        Full description for the Marketplace Item registration
+    tags : dict
+        Tags for Marketplace item registration. Required atleast 1 tag
+    external_id : uuid.UUID
+        The Marketplace ID (defined on the task).
+    card_image_file_name : str, optional
+        Card Image File Name for Marketplace Items. Defaults to ''.
+    external_type : GUIDES_EXTERNAL_TYPES, optional
+        Marketplaec External Type. Defaults to 'SwitchGuides'.
+    scope : GUIDES_SCOPE, optional
+        Marketplace item scope. Defaults to 'Portfolio-wide'.
+    image_file_name : str, optional
+        For Marketplace item image. Defaults to ''.
+    version : str, optional
+        For Marketplace item versioning. Defaults to ''.
+
+    Returns
+    -------
+    pandas.Dataframe
+    """
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if not name.strip():
+        logger.exception("The name property cannot be empty.")
+        raise Exception("The name property cannot be empty.")
+    if not description.strip():
+        logger.exception("The description property cannot be empty.")
+        raise Exception("The description property cannot be empty.")
+    if not short_description.strip():
+        logger.exception("The short_description property cannot be empty.")
+        raise Exception("The short_description property cannot be empty.")
+    if type(tags) != dict:
+        logger.exception("The tags property must be a dictionary with at least one key. ")
+        raise Exception("The tags property must be a dictionary with at least one key. ")
+    if len(tags.keys()) < 1:
+        logger.exception("The tags property must be a dictionary with at least one key. ")
+        raise Exception("The tags property must be a dictionary with at least one key. ")
+
+    if type(image_file_name) != str:
+        logger.exception("The image_file_name property must be a string. ")
+        raise Exception("The image_file_name property must be a string. ")
+    if type(card_image_file_name) != str:
+        logger.exception("The card_image_file_name property be a string. ")
+        raise Exception("The card_image_file_name property must be a string. ")
+
+    headers = api_inputs.api_headers.default
+
+    # enforce these values
+    scope = 'Portfolio-wide'
+    external_type = 'SwitchGuides'
+    container_name = 'guides-form'
+
+    if not version.strip():
+        version = '0.1.0'
+
+    image_url = f'{image_file_name}'
+    card_image_url = f'{card_image_file_name}'
+
+    payload = {
+        "name": name,
+        "shortDescription": short_description,
+        "description": description,
+        "version": version,
+        "externalType": external_type,
+        "externalId": external_id,
+        "tags": tags,
+        "scope": scope,
+        "imageUrl": image_url,
+        "cardImageUrl": card_image_url
+    }
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/guide/register"
+
+    logger.info("Sending request: POST %s", url)
+
+    response = requests.post(url, json=payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+
+    if response.status_code != 200 and len(response.text) > 0:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        error_df = pandas.read_json(response.text)
+        return response_status, error_df
+    elif response.status_code != 200 and len(response.text) == 0:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s',
+                     response.request.url)
+        return response_status, pandas.DataFrame()
+
+    if (response.text == '[]'):
+        logger.info('No changes with the records from Marketplace.')
+        df = pandas.DataFrame()
+    else:
+        df = pandas.read_json(response.text, typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+    return df
```

### Comparing `switch_api-0.5.4b2/switch_api/_utils/_platform.py` & `switch_api-0.5.4b3/switch_api/_utils/_platform.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,438 +1,438 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for .....
-"""
-import os
-from azure.storage.blob import ContainerClient, ContentSettings
-import uuid
-import pandas
-import time
-import requests
-import logging
-import sys
-from .._utils._constants import ACCOUNT, DATA_INGESTION_CONTAINER
-from .._utils._utils import ApiInputs
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-class Queue:
-    """ """
-    @staticmethod
-    def get_next_messages(account: ACCOUNT, container: str, queue_name: str, message_count: int = 1):
-        """Retrieve next message(s).
-
-        Parameters
-        ----------
-        account : ACCOUNT
-            Azure account
-        container : str
-            Queue container.
-        queue_name : str
-            Queue name.
-        message_count : int
-            Message count to retrieve (Default value = 1).
-
-        Returns
-        -------
-
-        """
-        pass
-
-    @staticmethod
-    def send_message(account: ACCOUNT, container, queue_name, messages: list = None):
-        """Send message
-
-        Parameters
-        ----------
-        account : ACCOUNT
-            Azure account.
-        container : str
-            Queue container.
-        queue_name : str
-            Queue name.
-        messages : list, default = None
-            Message (Default value = None).
-
-        Returns
-        -------
-
-        """
-        if messages is None:
-            messages = []
-
-        pass
-
-
-class Blob:
-    """ """
-    @staticmethod
-    def list(api_inputs: ApiInputs, account: ACCOUNT, container: str, prefix: str = None):
-        """Retrieve list of blobs.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        account : ACCOUNT
-            Azure account.
-        container : str
-            Blob container.
-        prefix : str, default=None
-            Prefix (Default value = None).
-
-        Returns
-        -------
-
-        """
-
-        if not set([account]).issubset(set(ACCOUNT.__args__)):
-            logger.error('account parameter must be set to one of the allowed values defined by the '
-                         'ACCOUNT literal: %s', ACCOUNT.__args__)
-            return False
-
-        if prefix is None:
-            prefix = ''
-
-        container_con_string = _get_container_sas_uri(api_inputs, container, account)
-        container_client = ContainerClient.from_container_url(container_con_string)
-
-        blob_list = container_client.list_blobs(name_starts_with=prefix)
-        for blob in blob_list:
-            logger.info('%s - %s', blob.name, str(blob.last_modified))
-        return True
-
-    @staticmethod
-    def download(api_inputs: ApiInputs, account: ACCOUNT, container: str, blob_name: str):
-        """
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        account : ACCOUNT
-            Azure account
-        container : str
-            Blob container.
-        blob_name : str
-            Blob name.
-
-        Returns
-        -------
-
-        """
-        if not set([account]).issubset(set(ACCOUNT.__args__)):
-            logger.error('account parameter must be set to one of the allowed values defined by the '
-                         'ACCOUNT literal: %s', ACCOUNT.__args__)
-            return False
-
-        container_con_string = _get_container_sas_uri(api_inputs, container, account)
-        container_client = ContainerClient.from_container_url(container_con_string)
-
-        blob_client = container_client.get_blob_client(blob_name)
-
-        return blob_client.download_blob().readall()
-
-    @staticmethod
-    def upload(api_inputs: ApiInputs, data_frame: pandas.DataFrame, name: str, batch_id: uuid.UUID = None, account: ACCOUNT = 'DataIngestion',
-               container: DATA_INGESTION_CONTAINER = 'data-ingestion-adx', folder: str = 'to-ingest', include_header: bool = False):
-        """Upload data to blob.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        account: ACCOUNT, optional
-            Blob account data will be uploaded to (Default value = 'DataIngestion').
-        data_frame : pandas.DataFrame
-            Dataframe containing the data to be uploaded to blob.
-        name : str
-            Name.
-        batch_id : str
-            Data feed file status id.
-        folder: str
-            Top folder inside the container assigned to the uploaded blob. (Default value = 'to-ingest')
-        container: DATA_INGESTION_CONTAINER
-            Container name (Literal) to where the blob goes into. (Default value = 'data-ingestion-adx')
-        include_header: bool
-            Boolean if include data frame's headers in the output file.
-            Default to False
-
-        Returns
-        -------
-
-        """
-        chunk_size = 50000
-
-        if not set([account]).issubset(set(ACCOUNT.__args__)):
-            logger.error('account parameter must be set to one of the allowed values defined by the '
-                         'ACCOUNT literal: %s', ACCOUNT.__args__)
-            return False
-
-        if not set([container]).issubset(set(DATA_INGESTION_CONTAINER.__args__)):
-            logger.error('container parameter must be set to one of the allowed values defined by the '
-                         'DATA_INGESTION_CONTAINER literal: %s', DATA_INGESTION_CONTAINER.__args__)
-            return False
-
-        if batch_id is None or batch_id == '00000000-0000-0000-0000-000000000000' or batch_id == '':
-            if api_inputs.data_feed_file_status_id is None \
-                    or api_inputs.data_feed_file_status_id == '00000000-0000-0000-0000-000000000000':
-                batch_id = uuid.uuid4()
-            else:
-                batch_id = api_inputs.data_feed_file_status_id
-
-        container_con_string = _get_container_sas_uri(api_inputs, container, account, True)
-        container_client = ContainerClient.from_container_url(container_con_string)
-
-        list_chunked_df = [data_frame[count:count + chunk_size] for count in
-                           range(0, data_frame.shape[0], chunk_size)]
-
-        upload_path = f"{folder}/{api_inputs.api_project_id}/{name}/{batch_id}/{time.time_ns()}_"
-
-        item_counter = 0
-        for current_data_frame in list_chunked_df:
-            item_counter += 1
-            blob_name = upload_path + str(item_counter) + ".csv"
-            logger.info("Uploading ... %s", blob_name)
-            blob_client = container_client.get_blob_client(blob_name)
-
-            if pandas.__version__ < "1.5.0":
-                data_csv = bytes(current_data_frame.to_csv(line_terminator='\r\n', index=False, header=include_header),
-                                 encoding='utf-8')
-            elif pandas.__version__ >= "1.5.0":
-                data_csv = bytes(current_data_frame.to_csv(lineterminator='\r\n', index=False, header=include_header),
-                                 encoding='utf-8')
-            blob_client.upload_blob(data_csv, blob_type="BlockBlob", overwrite=True)
-
-        return upload_path, item_counter
-
-    @staticmethod
-    def custom_upload(api_inputs: ApiInputs, account: ACCOUNT, container: str, upload_path: str, file_name: str,
-                      upload_object):
-        """Upload data to blob.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        account: ACCOUNT
-            Blob account data will be uploaded to.
-        container : str
-            Blob container.
-        upload_path: str
-            The prefix required to navigate from the base `container` to the folder the `upload_object` should be
-            uploaded to.
-        file_name : str
-            File name to be stored in blob.
-        upload_object :
-            Object to be uploaded to blob.
-
-        Returns
-        -------
-
-        """
-        if not set([account]).issubset(set(ACCOUNT.__args__)):
-            logger.error('account parameter must be set to one of the allowed values defined by the '
-                         'ACCOUNT literal: %s', ACCOUNT.__args__)
-            return False
-
-        container_con_string = _get_container_sas_uri(api_inputs, container, account, True)
-        container_client = ContainerClient.from_container_url(container_con_string)
-
-        if upload_path.endswith('/') == False:
-            blob_name = upload_path + '/' + file_name
-        elif upload_path.endswith('/') == True:
-            blob_name = upload_path + file_name
-
-        logger.info('Uploading to blob: %s', blob_name)
-        blob_client = container_client.get_blob_client(blob_name)
-        blob_client.upload_blob(upload_object, blob_type="BlockBlob", overwrite=True)
-
-    @staticmethod
-    def _guide_blob_upload(api_inputs: ApiInputs, local_folder_path: str, driver_id: uuid.UUID):
-        """Upload folder files to Guides' blob storage.
-
-        Returns
-        -------
-
-        """
-        account = "DataIngestion"
-        container_name = "guides-form"
-        blob_prefix = driver_id + "/"
-
-        # Get a reference to the container
-        container_con_string = _get_container_sas_uri(api_inputs, container_name, account, True)
-        container_client = ContainerClient.from_container_url(container_con_string)
-
-        # Get a list of files in the local folder
-        files = [f for f in os.listdir(local_folder_path) if os.path.isfile(
-            os.path.join(local_folder_path, f))]
-
-        try:
-            # Upload each file to storage
-            for root, _, files in os.walk(local_folder_path):
-                for file_name in files:
-                    local_file_path = os.path.join(root, file_name)
-                    blob_name = blob_prefix + os.path.relpath(local_file_path, local_folder_path).replace(
-                        os.path.sep, "/")
-
-                    with open(local_file_path, "rb") as data:
-                        container_client.upload_blob(name=blob_name, data=data, content_settings=ContentSettings(
-                            content_type='application/octet-stream'), overwrite=True)
-
-                    logger.info(f"Uploaded {local_file_path} to {blob_name}")
-
-            return True
-        except Exception as exc:
-            logger.exception(str(exc))
-            return False
-
-
-def _get_ingestion_service_bus_connection_string(api_inputs: ApiInputs, queue_type: str = 'DataIngestion'):
-    """
-    Get connection string specific to Data Ingestion Service Bus
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-            Object returned by initialize() function.
-
-    Returns
-    -------
-    str
-        Data Ingestion Service Bus connection string
-    """
-    headers = api_inputs.api_headers.default
-
-    if not queue_type or queue_type == '':
-        queue_type = 'DataIngestion'
-
-    params = {'serviceBusType': queue_type}
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/service-bus"
-    response = requests.request("GET", url, timeout=20, headers=headers, params=params)
-
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return None
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return None
-
-    return response.text
-
-
-def _get_container_sas_uri(api_inputs: ApiInputs, container: str, account: ACCOUNT = 'SwitcStorage', writable: bool = False):
-    """
-    Get container connection string from specified Storage Account
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-            Object returned by initialize() function.
-    container: str
-        Name of the container under the account specified
-    account : ACCOUNT, default = 'SwitchStorage'x
-         (Default value = 'SwitchStorage')
-    writable: bool
-        Sets permissions expectation for the generated SAS Uri
-
-    Returns
-    -------
-    str
-        container connection string
-    """
-
-    if container == None or len(container) == 0:
-        logger.error('Must set a container to get Container connection string.')
-
-    headers = api_inputs.api_headers.default
-    expire_after_hours = 1
-
-    payload = {
-        "storageOptions": __get_storage_options(account),
-        "containerName": container,
-        "expireAfterHours": expire_after_hours,
-        "isWritable": writable
-    }
-
-    url = f"{api_inputs.api_base_url}/blob/container-sas"
-    response = requests.request("POST", url, json=payload, headers=headers)
-
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return ""
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return ""
-
-    return response.text
-
-
-def _get_structure(df):
-    """Get dataframe structure
-
-    Parameters
-    ----------
-    df : pandas.DataFrame
-
-    Returns
-    -------
-
-    """
-
-    a = df.dtypes
-    a = pandas.Series(a)
-    a = a.reset_index().rename(columns={0: 'dtype'})
-    a['dtype'] = a['dtype'].astype('str')
-    a.set_index('index', inplace=True)
-    a = a.to_dict()
-    return a['dtype']
-
-
-def __get_storage_options(account: ACCOUNT):
-    """
-    Get Storage Account Options. Currently the existing account literal doesn't match the Enum equivalent of storage
-    options in the API, so we have this method
-
-    Parameters
-    ----------
-    account : ACCOUNT
-        Account to map API storage account options
-
-    Returns
-    -------
-    str
-        API equivalent account storage name
-    """
-
-    if account == None or len(account) == 0:
-        logger.error('Mapping to storage options requires Account Parameter Value.')
-        return account
-
-    if account == 'SwitchStorage':
-        return 'LegacySwitchStorage'
-    elif account == 'SwitchContainer':
-        return 'LegacySwitchContainer'
-    elif account == 'Argus':
-        return 'ArgusStorage'
-    elif account == 'DataIngestion':
-        return 'DataIngestionStorage'
-    elif account == 'SwitchGuides':
-        return 'DataIngestionStorage'
-
-    return account
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for .....
+"""
+import os
+from azure.storage.blob import ContainerClient, ContentSettings
+import uuid
+import pandas
+import time
+import requests
+import logging
+import sys
+from .._utils._constants import ACCOUNT, DATA_INGESTION_CONTAINER
+from .._utils._utils import ApiInputs
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+class Queue:
+    """ """
+    @staticmethod
+    def get_next_messages(account: ACCOUNT, container: str, queue_name: str, message_count: int = 1):
+        """Retrieve next message(s).
+
+        Parameters
+        ----------
+        account : ACCOUNT
+            Azure account
+        container : str
+            Queue container.
+        queue_name : str
+            Queue name.
+        message_count : int
+            Message count to retrieve (Default value = 1).
+
+        Returns
+        -------
+
+        """
+        pass
+
+    @staticmethod
+    def send_message(account: ACCOUNT, container, queue_name, messages: list = None):
+        """Send message
+
+        Parameters
+        ----------
+        account : ACCOUNT
+            Azure account.
+        container : str
+            Queue container.
+        queue_name : str
+            Queue name.
+        messages : list, default = None
+            Message (Default value = None).
+
+        Returns
+        -------
+
+        """
+        if messages is None:
+            messages = []
+
+        pass
+
+
+class Blob:
+    """ """
+    @staticmethod
+    def list(api_inputs: ApiInputs, account: ACCOUNT, container: str, prefix: str = None):
+        """Retrieve list of blobs.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        account : ACCOUNT
+            Azure account.
+        container : str
+            Blob container.
+        prefix : str, default=None
+            Prefix (Default value = None).
+
+        Returns
+        -------
+
+        """
+
+        if not set([account]).issubset(set(ACCOUNT.__args__)):
+            logger.error('account parameter must be set to one of the allowed values defined by the '
+                         'ACCOUNT literal: %s', ACCOUNT.__args__)
+            return False
+
+        if prefix is None:
+            prefix = ''
+
+        container_con_string = _get_container_sas_uri(api_inputs, container, account)
+        container_client = ContainerClient.from_container_url(container_con_string)
+
+        blob_list = container_client.list_blobs(name_starts_with=prefix)
+        for blob in blob_list:
+            logger.info('%s - %s', blob.name, str(blob.last_modified))
+        return True
+
+    @staticmethod
+    def download(api_inputs: ApiInputs, account: ACCOUNT, container: str, blob_name: str):
+        """
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        account : ACCOUNT
+            Azure account
+        container : str
+            Blob container.
+        blob_name : str
+            Blob name.
+
+        Returns
+        -------
+
+        """
+        if not set([account]).issubset(set(ACCOUNT.__args__)):
+            logger.error('account parameter must be set to one of the allowed values defined by the '
+                         'ACCOUNT literal: %s', ACCOUNT.__args__)
+            return False
+
+        container_con_string = _get_container_sas_uri(api_inputs, container, account)
+        container_client = ContainerClient.from_container_url(container_con_string)
+
+        blob_client = container_client.get_blob_client(blob_name)
+
+        return blob_client.download_blob().readall()
+
+    @staticmethod
+    def upload(api_inputs: ApiInputs, data_frame: pandas.DataFrame, name: str, batch_id: uuid.UUID = None, account: ACCOUNT = 'DataIngestion',
+               container: DATA_INGESTION_CONTAINER = 'data-ingestion-adx', folder: str = 'to-ingest', include_header: bool = False):
+        """Upload data to blob.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        account: ACCOUNT, optional
+            Blob account data will be uploaded to (Default value = 'DataIngestion').
+        data_frame : pandas.DataFrame
+            Dataframe containing the data to be uploaded to blob.
+        name : str
+            Name.
+        batch_id : str
+            Data feed file status id.
+        folder: str
+            Top folder inside the container assigned to the uploaded blob. (Default value = 'to-ingest')
+        container: DATA_INGESTION_CONTAINER
+            Container name (Literal) to where the blob goes into. (Default value = 'data-ingestion-adx')
+        include_header: bool
+            Boolean if include data frame's headers in the output file.
+            Default to False
+
+        Returns
+        -------
+
+        """
+        chunk_size = 50000
+
+        if not set([account]).issubset(set(ACCOUNT.__args__)):
+            logger.error('account parameter must be set to one of the allowed values defined by the '
+                         'ACCOUNT literal: %s', ACCOUNT.__args__)
+            return False
+
+        if not set([container]).issubset(set(DATA_INGESTION_CONTAINER.__args__)):
+            logger.error('container parameter must be set to one of the allowed values defined by the '
+                         'DATA_INGESTION_CONTAINER literal: %s', DATA_INGESTION_CONTAINER.__args__)
+            return False
+
+        if batch_id is None or batch_id == '00000000-0000-0000-0000-000000000000' or batch_id == '':
+            if api_inputs.data_feed_file_status_id is None \
+                    or api_inputs.data_feed_file_status_id == '00000000-0000-0000-0000-000000000000':
+                batch_id = uuid.uuid4()
+            else:
+                batch_id = api_inputs.data_feed_file_status_id
+
+        container_con_string = _get_container_sas_uri(api_inputs, container, account, True)
+        container_client = ContainerClient.from_container_url(container_con_string)
+
+        list_chunked_df = [data_frame[count:count + chunk_size] for count in
+                           range(0, data_frame.shape[0], chunk_size)]
+
+        upload_path = f"{folder}/{api_inputs.api_project_id}/{name}/{batch_id}/{time.time_ns()}_"
+
+        item_counter = 0
+        for current_data_frame in list_chunked_df:
+            item_counter += 1
+            blob_name = upload_path + str(item_counter) + ".csv"
+            logger.info("Uploading ... %s", blob_name)
+            blob_client = container_client.get_blob_client(blob_name)
+
+            if pandas.__version__ < "1.5.0":
+                data_csv = bytes(current_data_frame.to_csv(line_terminator='\r\n', index=False, header=include_header),
+                                 encoding='utf-8')
+            elif pandas.__version__ >= "1.5.0":
+                data_csv = bytes(current_data_frame.to_csv(lineterminator='\r\n', index=False, header=include_header),
+                                 encoding='utf-8')
+            blob_client.upload_blob(data_csv, blob_type="BlockBlob", overwrite=True)
+
+        return upload_path, item_counter
+
+    @staticmethod
+    def custom_upload(api_inputs: ApiInputs, account: ACCOUNT, container: str, upload_path: str, file_name: str,
+                      upload_object):
+        """Upload data to blob.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        account: ACCOUNT
+            Blob account data will be uploaded to.
+        container : str
+            Blob container.
+        upload_path: str
+            The prefix required to navigate from the base `container` to the folder the `upload_object` should be
+            uploaded to.
+        file_name : str
+            File name to be stored in blob.
+        upload_object :
+            Object to be uploaded to blob.
+
+        Returns
+        -------
+
+        """
+        if not set([account]).issubset(set(ACCOUNT.__args__)):
+            logger.error('account parameter must be set to one of the allowed values defined by the '
+                         'ACCOUNT literal: %s', ACCOUNT.__args__)
+            return False
+
+        container_con_string = _get_container_sas_uri(api_inputs, container, account, True)
+        container_client = ContainerClient.from_container_url(container_con_string)
+
+        if upload_path.endswith('/') == False:
+            blob_name = upload_path + '/' + file_name
+        elif upload_path.endswith('/') == True:
+            blob_name = upload_path + file_name
+
+        logger.info('Uploading to blob: %s', blob_name)
+        blob_client = container_client.get_blob_client(blob_name)
+        blob_client.upload_blob(upload_object, blob_type="BlockBlob", overwrite=True)
+
+    @staticmethod
+    def _guide_blob_upload(api_inputs: ApiInputs, local_folder_path: str, driver_id: uuid.UUID):
+        """Upload folder files to Guides' blob storage.
+
+        Returns
+        -------
+
+        """
+        account = "DataIngestion"
+        container_name = "guides-form"
+        blob_prefix = driver_id + "/"
+
+        # Get a reference to the container
+        container_con_string = _get_container_sas_uri(api_inputs, container_name, account, True)
+        container_client = ContainerClient.from_container_url(container_con_string)
+
+        # Get a list of files in the local folder
+        files = [f for f in os.listdir(local_folder_path) if os.path.isfile(
+            os.path.join(local_folder_path, f))]
+
+        try:
+            # Upload each file to storage
+            for root, _, files in os.walk(local_folder_path):
+                for file_name in files:
+                    local_file_path = os.path.join(root, file_name)
+                    blob_name = blob_prefix + os.path.relpath(local_file_path, local_folder_path).replace(
+                        os.path.sep, "/")
+
+                    with open(local_file_path, "rb") as data:
+                        container_client.upload_blob(name=blob_name, data=data, content_settings=ContentSettings(
+                            content_type='application/octet-stream'), overwrite=True)
+
+                    logger.info(f"Uploaded {local_file_path} to {blob_name}")
+
+            return True
+        except Exception as exc:
+            logger.exception(str(exc))
+            return False
+
+
+def _get_ingestion_service_bus_connection_string(api_inputs: ApiInputs, queue_type: str = 'DataIngestion'):
+    """
+    Get connection string specific to Data Ingestion Service Bus
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+            Object returned by initialize() function.
+
+    Returns
+    -------
+    str
+        Data Ingestion Service Bus connection string
+    """
+    headers = api_inputs.api_headers.default
+
+    if not queue_type or queue_type == '':
+        queue_type = 'DataIngestion'
+
+    params = {'serviceBusType': queue_type}
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/service-bus"
+    response = requests.request("GET", url, timeout=20, headers=headers, params=params)
+
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return None
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return None
+
+    return response.text
+
+
+def _get_container_sas_uri(api_inputs: ApiInputs, container: str, account: ACCOUNT = 'SwitcStorage', writable: bool = False):
+    """
+    Get container connection string from specified Storage Account
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+            Object returned by initialize() function.
+    container: str
+        Name of the container under the account specified
+    account : ACCOUNT, default = 'SwitchStorage'x
+         (Default value = 'SwitchStorage')
+    writable: bool
+        Sets permissions expectation for the generated SAS Uri
+
+    Returns
+    -------
+    str
+        container connection string
+    """
+
+    if container == None or len(container) == 0:
+        logger.error('Must set a container to get Container connection string.')
+
+    headers = api_inputs.api_headers.default
+    expire_after_hours = 1
+
+    payload = {
+        "storageOptions": __get_storage_options(account),
+        "containerName": container,
+        "expireAfterHours": expire_after_hours,
+        "isWritable": writable
+    }
+
+    url = f"{api_inputs.api_base_url}/blob/container-sas"
+    response = requests.request("POST", url, json=payload, headers=headers)
+
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return ""
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return ""
+
+    return response.text
+
+
+def _get_structure(df):
+    """Get dataframe structure
+
+    Parameters
+    ----------
+    df : pandas.DataFrame
+
+    Returns
+    -------
+
+    """
+
+    a = df.dtypes
+    a = pandas.Series(a)
+    a = a.reset_index().rename(columns={0: 'dtype'})
+    a['dtype'] = a['dtype'].astype('str')
+    a.set_index('index', inplace=True)
+    a = a.to_dict()
+    return a['dtype']
+
+
+def __get_storage_options(account: ACCOUNT):
+    """
+    Get Storage Account Options. Currently the existing account literal doesn't match the Enum equivalent of storage
+    options in the API, so we have this method
+
+    Parameters
+    ----------
+    account : ACCOUNT
+        Account to map API storage account options
+
+    Returns
+    -------
+    str
+        API equivalent account storage name
+    """
+
+    if account == None or len(account) == 0:
+        logger.error('Mapping to storage options requires Account Parameter Value.')
+        return account
+
+    if account == 'SwitchStorage':
+        return 'LegacySwitchStorage'
+    elif account == 'SwitchContainer':
+        return 'LegacySwitchContainer'
+    elif account == 'Argus':
+        return 'ArgusStorage'
+    elif account == 'DataIngestion':
+        return 'DataIngestionStorage'
+    elif account == 'SwitchGuides':
+        return 'DataIngestionStorage'
+
+    return account
```

### Comparing `switch_api-0.5.4b2/switch_api/analytics/__init__.py` & `switch_api-0.5.4b3/switch_api/analytics/__init__.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,12 +1,12 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for .....
-"""
-
-from .analytics import (upsert_performance_statistics, get_parent_modules_list, get_clone_modules_list, sensor_has_not_changed)  ## , run_clone_modules
-
-__all__ = ['upsert_performance_statistics', 'get_parent_modules_list', 'get_clone_modules_list', 'sensor_has_not_changed']  # 'run_clone_modules',
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for .....
+"""
+
+from .analytics import (upsert_performance_statistics, get_parent_modules_list, get_clone_modules_list, sensor_has_not_changed)  ## , run_clone_modules
+
+__all__ = ['upsert_performance_statistics', 'get_parent_modules_list', 'get_clone_modules_list', 'sensor_has_not_changed']  # 'run_clone_modules',
```

### Comparing `switch_api-0.5.4b2/switch_api/analytics/analytics.py` & `switch_api-0.5.4b3/switch_api/analytics/analytics.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,363 +1,363 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for .....
-"""
-# from io import StringIO
-import pandas
-import pandera
-import requests
-import sys
-import logging
-import uuid
-import datetime
-from typing import List, Tuple
-# from .._utils._constants import api_prefix
-from .._utils._utils import (ApiInputs, _with_func_attrs, _column_name_cap, _performance_statistics_schema)
-from .._utils._constants import PERFORMANCE_STATISTIC_METRIC_SYSTEM
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-@_with_func_attrs(df_required_columns=['InstallationId', 'Investment', 'RoiYears', 'CostSaving', 'Cost',
-                                       'ComparisonCost', 'ConsumptionSaving', 'Consumption', 'CarbonSaving', 'Carbon',
-                                       'ComparisonConsumption', 'ComparisonCarbon', 'ConsumptionUnit', 'MetricSystem'])
-def upsert_performance_statistics(api_inputs:ApiInputs, performance_project_id: uuid.UUID, standard_id: uuid.UUID,
-                                  df: pandas.DataFrame) -> Tuple[bool, str]:
-    """Upsert Performance Statistics
-
-    Upserts Performance Statistics to the Switch Platform.
-
-    The `df` dataframe passed requires the following columns to be present:
-     - 'InstallationId'
-     - 'Investment'
-     - 'RoiYears'
-     - 'CostSaving'
-     - 'Cost'
-     - 'ComparisonCost'
-     - 'ConsumptionSaving'
-     - 'Consumption'
-     - 'CarbonSaving'
-     - 'Carbon'
-     - 'ComparisonConsumption'
-     - 'ComparisonCarbon'
-     - 'ConsumptionUnit'
-     - 'MetricSystem'
-
-    The MetricSystem column must only contain "metric" or "imperial" as the values.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    performance_project_id: uuid.UUID
-        The PerformanceProjectId that records will be upserted for.
-    standard_id: uuid.UUID
-        The StandardId that records will be upserted for.
-    df: pandas.DataFrame
-        The dataframe containing the statistics to be upserted.
-
-    Returns
-    -------
-    Tuple[bool, str]
-        Tuple containing first a boolean and secondary a string. The string will be empty unless the bool is False.
-        In that case, the string will contain details of the issue preventing the function successfully running.
-
-
-    """
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    data_frame = df.copy(deep=True)
-
-    required_columns = ['InstallationId', 'Investment', 'RoiYears', 'CostSaving', 'Cost', 'ComparisonCost',
-                        'ConsumptionSaving', 'Consumption', 'CarbonSaving', 'Carbon', 'ComparisonConsumption',
-                        'ComparisonCarbon', 'ConsumptionUnit', 'MetricSystem']
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(proposed_columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        error = 'analytics.upsert_performance_statistics() - df must contain the following columns: ' + ', '.join(
-            required_columns)
-        return False, error
-
-    metric_systems_present = data_frame['MetricSystem'].unique().tolist()
-    if not set(metric_systems_present).issubset(set(PERFORMANCE_STATISTIC_METRIC_SYSTEM.__args__)):
-        invalid_uom_systems = set(metric_systems_present).difference(set(PERFORMANCE_STATISTIC_METRIC_SYSTEM.__args__))
-        error = f"The MetricSystem column can only contain 'metric' or 'imperial' as values. Invalid value(s) present: {invalid_uom_systems}"
-        logger.error(error)
-        return False, error
-
-    try:
-        _performance_statistics_schema.validate(data_frame, lazy=True)
-    except pandera.errors.SchemaErrors as err:
-        logger.error('Errors present with columns in df provided.')
-        logger.error(err.failure_cases)
-        schema_error = err.failure_cases
-        return False, schema_error
-
-    headers = api_inputs.api_headers.default
-
-    datacentre = api_inputs.api_base_url.split('-', 1)[1].split('.')[0]
-
-    base_url = f"https://platformapi-{datacentre}-staging.switchautomation.com"
-
-    # url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/benchmarking/performanceProjects/{performance_project_id}/standards/{standard_id}/sites/statistics"
-
-    url = f"{base_url}/api/1.0/projects/{api_inputs.api_project_id}/benchmarking/performanceProjects/{performance_project_id}/standards/{standard_id}/sites/statistics"
-
-    logger.info("Sending request: PUT %s", url)
-
-    response = requests.put(url, data=data_frame.to_json(orient='records'), headers=headers)
-
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        error = (f"API Call was not successful. Response Status: {response.status_code}. Reason: {response.reason}. "
-                     f"Message: {response.text}")
-        logger.error(error)
-        return False, error
-    elif len(response.text) == 0:
-        error = f'No data returned for this API call. {response.request.url}'
-        logger.error(error)
-        return False, error
-
-    response_data_frame = pandas.read_json(response.text, orient='index')
-    response_data_frame = response_data_frame.T
-    response_data_frame.columns = _column_name_cap(columns=response_data_frame.columns)
-
-    return True, response_data_frame
-
-def get_parent_modules_list(api_inputs: ApiInputs, search_filter: str = ""):
-    """Get list of data sets.
-
-    Retrieves the list of parent modules either for the entire project across all installations
-    or filter by name on the `search_filter`
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    search_filter : str
-        Uses contains on the parent module Name (Default value = "").
-
-    Returns
-    -------
-    tuple (str, pandas.DataFrame)
-        Returns the response status of the call and a dataframe containing the data returned by the call.
-
-    """
-    payload = {}
-
-    headers = api_inputs.api_headers.default
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/analytics/logic-modules"
-
-    if search_filter != "*" and search_filter != '':
-        url = url + f"?searchFilter={search_filter}"
-
-    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    logger.info("API Call successful.")
-
-    return response_status, df
-
-
-def get_clone_modules_list(api_inputs: ApiInputs, parent_module_id: uuid.UUID, search_filter="*"):
-    """Get list of data sets.
-
-    Retrieves the list of clone modules either for the specified parent_module_id
-    or filter by shared Tag Name on the `search_filter`
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    parent_module_id : uuid.UIID
-        Parent logic module identifier
-    search_filter : str
-        Uses contains on the parent module Name (Default value = "*").
-
-    Returns
-    -------
-    tuple (str, pandas.DataFrame)
-        Returns the response status of the call and a dataframe containing the data returned by the call.
-
-    """
-    payload = {}
-
-    headers = api_inputs.api_headers.default
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/analytics/logic-modules-clones/" \
-          f"parent-module/{parent_module_id}"
-
-    if search_filter != "*" and search_filter != '':
-        url = url + f"?searchFilter={search_filter}"
-
-    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    logger.info("API Call successful.")
-
-    return response_status, df
-
-
-# def run_clone_modules(api_inputs: ApiInputs, parent_module_id: uuid.UUID, start_date: datetime.date,
-#                       end_date: datetime.date, share_to_tags: List[str] = None):
-#     """Get list of data sets.
-#
-#     Run the list of clone modules for the specified parent_module_id
-#     and the share to tags list
-#
-#     Parameters
-#     ----------
-#     api_inputs : ApiInputs
-#         Object returned by initialize() function.
-#     parent_module_id : uuid.UUID
-#         Identifier for parent module.
-#     start_date : datetime.date
-#         Date the reprocess should start at.
-#     end_date : datetime.date
-#         Date the reprocess should finish at.
-#     share_to_tags : List[str], optional
-#         List of Tag names received via the get_clone_modules_list method
-#
-#     Returns
-#     -------
-#     tuple (str, pandas.DataFrame)
-#         Returns the response status of the call and a dataframe containing the data returned by the call.
-#
-#     """
-#     if start_date > end_date:
-#         logger.error('start_date must be prior to end_date: start_date= %s, end_date= %s', start_date.__str__(),
-#                      end_date.__str__())
-#         return pandas.DataFrame()
-#
-#     payload = {
-#         "parentModuleId": parent_module_id,
-#         "dateFrom": start_date.isoformat(),
-#         "dateTo": end_date.isoformat(),
-#         "tagsList": share_to_tags
-#     }
-#
-#     headers = {
-#         'x-functions-key': api_inputs.api_key,
-#         'Content-Type': 'application/json; charset=utf-8',
-#         'user-key': api_inputs.user_id
-#     }
-#
-#     if api_inputs.datacentre == '' or api_inputs.api_key == '':
-#         logger.error("You must call initialize() before using API.")
-#         return pandas.DataFrame()
-#
-#     url = api_prefix + api_inputs.datacentre + "/" + api_inputs.api_project_id + "/RunLogicModule"
-#     response = requests.post(url, json=payload, headers=headers)
-#     response_status = '{} {}'.format(response.status_code, response.reason)
-#     if response.status_code != 200:
-#         logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-#                      response.reason)
-#         return response_status, pandas.DataFrame()
-#     elif len(response.text) == 0:
-#         logger.error('No data returned for this API call. %s', response.request.url)
-#         return response_status, pandas.DataFrame()
-#
-#     table_data = StringIO(response.text)
-#     df = pandas.read_table(table_data, sep=',')
-#     logger.info("API Call successful.")
-#     return response_status, df
-
-
-def sensor_has_not_changed(api_inputs: ApiInputs, start_date: datetime.date, end_date: datetime.date,
-                           templates: List[str] = None, hours_unchanged: int = 24):
-    """Get list of sensors which haven't changed value for the given period and at least ``hours_unchanged``.
-
-    Run the Has Not Changed Query for Selected List of Sites (empty List means All)
-    and Selected List of Templates  (empty List means All)
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    start_date : datetime.date
-        Date the reprocess should start at.
-    end_date : datetime.date
-        Date the reprocess should finish at.
-    templates : List[str]
-        List of ObjectPropertyTemplateNames
-    hours_unchanged: int
-        number of hours the sensor value has not changed
-
-    Returns
-    -------
-    tuple (str, pandas.DataFrame)
-        Returns the response status of the call and a dataframe containing the data returned by the call as a tuple.
-
-    """
-
-    headers = api_inputs.api_headers.default
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if templates.count == 0:
-        logger.error("You must supply templates to report on.")
-        return pandas.DataFrame()
-
-    payload = {
-        "dateFrom": start_date.isoformat(),
-        "dateTo": end_date.isoformat(),
-        "templatesList": templates,
-        "hoursUnchanged": hours_unchanged
-    }
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/analytics/run-adx-query/SensorHasNotChanged"
-
-    response = requests.post(url, json=payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    logger.info("API Call successful.")
-    return response_status, df
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for .....
+"""
+# from io import StringIO
+import pandas
+import pandera
+import requests
+import sys
+import logging
+import uuid
+import datetime
+from typing import List, Tuple
+# from .._utils._constants import api_prefix
+from .._utils._utils import (ApiInputs, _with_func_attrs, _column_name_cap, _performance_statistics_schema)
+from .._utils._constants import PERFORMANCE_STATISTIC_METRIC_SYSTEM
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+@_with_func_attrs(df_required_columns=['InstallationId', 'Investment', 'RoiYears', 'CostSaving', 'Cost',
+                                       'ComparisonCost', 'ConsumptionSaving', 'Consumption', 'CarbonSaving', 'Carbon',
+                                       'ComparisonConsumption', 'ComparisonCarbon', 'ConsumptionUnit', 'MetricSystem'])
+def upsert_performance_statistics(api_inputs:ApiInputs, performance_project_id: uuid.UUID, standard_id: uuid.UUID,
+                                  df: pandas.DataFrame) -> Tuple[bool, str]:
+    """Upsert Performance Statistics
+
+    Upserts Performance Statistics to the Switch Platform.
+
+    The `df` dataframe passed requires the following columns to be present:
+     - 'InstallationId'
+     - 'Investment'
+     - 'RoiYears'
+     - 'CostSaving'
+     - 'Cost'
+     - 'ComparisonCost'
+     - 'ConsumptionSaving'
+     - 'Consumption'
+     - 'CarbonSaving'
+     - 'Carbon'
+     - 'ComparisonConsumption'
+     - 'ComparisonCarbon'
+     - 'ConsumptionUnit'
+     - 'MetricSystem'
+
+    The MetricSystem column must only contain "metric" or "imperial" as the values.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    performance_project_id: uuid.UUID
+        The PerformanceProjectId that records will be upserted for.
+    standard_id: uuid.UUID
+        The StandardId that records will be upserted for.
+    df: pandas.DataFrame
+        The dataframe containing the statistics to be upserted.
+
+    Returns
+    -------
+    Tuple[bool, str]
+        Tuple containing first a boolean and secondary a string. The string will be empty unless the bool is False.
+        In that case, the string will contain details of the issue preventing the function successfully running.
+
+
+    """
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    data_frame = df.copy(deep=True)
+
+    required_columns = ['InstallationId', 'Investment', 'RoiYears', 'CostSaving', 'Cost', 'ComparisonCost',
+                        'ConsumptionSaving', 'Consumption', 'CarbonSaving', 'Carbon', 'ComparisonConsumption',
+                        'ComparisonCarbon', 'ConsumptionUnit', 'MetricSystem']
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(proposed_columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        error = 'analytics.upsert_performance_statistics() - df must contain the following columns: ' + ', '.join(
+            required_columns)
+        return False, error
+
+    metric_systems_present = data_frame['MetricSystem'].unique().tolist()
+    if not set(metric_systems_present).issubset(set(PERFORMANCE_STATISTIC_METRIC_SYSTEM.__args__)):
+        invalid_uom_systems = set(metric_systems_present).difference(set(PERFORMANCE_STATISTIC_METRIC_SYSTEM.__args__))
+        error = f"The MetricSystem column can only contain 'metric' or 'imperial' as values. Invalid value(s) present: {invalid_uom_systems}"
+        logger.error(error)
+        return False, error
+
+    try:
+        _performance_statistics_schema.validate(data_frame, lazy=True)
+    except pandera.errors.SchemaErrors as err:
+        logger.error('Errors present with columns in df provided.')
+        logger.error(err.failure_cases)
+        schema_error = err.failure_cases
+        return False, schema_error
+
+    headers = api_inputs.api_headers.default
+
+    datacentre = api_inputs.api_base_url.split('-', 1)[1].split('.')[0]
+
+    base_url = f"https://platformapi-{datacentre}-staging.switchautomation.com"
+
+    # url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/benchmarking/performanceProjects/{performance_project_id}/standards/{standard_id}/sites/statistics"
+
+    url = f"{base_url}/api/1.0/projects/{api_inputs.api_project_id}/benchmarking/performanceProjects/{performance_project_id}/standards/{standard_id}/sites/statistics"
+
+    logger.info("Sending request: PUT %s", url)
+
+    response = requests.put(url, data=data_frame.to_json(orient='records'), headers=headers)
+
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        error = (f"API Call was not successful. Response Status: {response.status_code}. Reason: {response.reason}. "
+                     f"Message: {response.text}")
+        logger.error(error)
+        return False, error
+    elif len(response.text) == 0:
+        error = f'No data returned for this API call. {response.request.url}'
+        logger.error(error)
+        return False, error
+
+    response_data_frame = pandas.read_json(response.text, orient='index')
+    response_data_frame = response_data_frame.T
+    response_data_frame.columns = _column_name_cap(columns=response_data_frame.columns)
+
+    return True, response_data_frame
+
+def get_parent_modules_list(api_inputs: ApiInputs, search_filter: str = ""):
+    """Get list of data sets.
+
+    Retrieves the list of parent modules either for the entire project across all installations
+    or filter by name on the `search_filter`
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    search_filter : str
+        Uses contains on the parent module Name (Default value = "").
+
+    Returns
+    -------
+    tuple (str, pandas.DataFrame)
+        Returns the response status of the call and a dataframe containing the data returned by the call.
+
+    """
+    payload = {}
+
+    headers = api_inputs.api_headers.default
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/analytics/logic-modules"
+
+    if search_filter != "*" and search_filter != '':
+        url = url + f"?searchFilter={search_filter}"
+
+    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    logger.info("API Call successful.")
+
+    return response_status, df
+
+
+def get_clone_modules_list(api_inputs: ApiInputs, parent_module_id: uuid.UUID, search_filter="*"):
+    """Get list of data sets.
+
+    Retrieves the list of clone modules either for the specified parent_module_id
+    or filter by shared Tag Name on the `search_filter`
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    parent_module_id : uuid.UIID
+        Parent logic module identifier
+    search_filter : str
+        Uses contains on the parent module Name (Default value = "*").
+
+    Returns
+    -------
+    tuple (str, pandas.DataFrame)
+        Returns the response status of the call and a dataframe containing the data returned by the call.
+
+    """
+    payload = {}
+
+    headers = api_inputs.api_headers.default
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/analytics/logic-modules-clones/" \
+          f"parent-module/{parent_module_id}"
+
+    if search_filter != "*" and search_filter != '':
+        url = url + f"?searchFilter={search_filter}"
+
+    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    logger.info("API Call successful.")
+
+    return response_status, df
+
+
+# def run_clone_modules(api_inputs: ApiInputs, parent_module_id: uuid.UUID, start_date: datetime.date,
+#                       end_date: datetime.date, share_to_tags: List[str] = None):
+#     """Get list of data sets.
+#
+#     Run the list of clone modules for the specified parent_module_id
+#     and the share to tags list
+#
+#     Parameters
+#     ----------
+#     api_inputs : ApiInputs
+#         Object returned by initialize() function.
+#     parent_module_id : uuid.UUID
+#         Identifier for parent module.
+#     start_date : datetime.date
+#         Date the reprocess should start at.
+#     end_date : datetime.date
+#         Date the reprocess should finish at.
+#     share_to_tags : List[str], optional
+#         List of Tag names received via the get_clone_modules_list method
+#
+#     Returns
+#     -------
+#     tuple (str, pandas.DataFrame)
+#         Returns the response status of the call and a dataframe containing the data returned by the call.
+#
+#     """
+#     if start_date > end_date:
+#         logger.error('start_date must be prior to end_date: start_date= %s, end_date= %s', start_date.__str__(),
+#                      end_date.__str__())
+#         return pandas.DataFrame()
+#
+#     payload = {
+#         "parentModuleId": parent_module_id,
+#         "dateFrom": start_date.isoformat(),
+#         "dateTo": end_date.isoformat(),
+#         "tagsList": share_to_tags
+#     }
+#
+#     headers = {
+#         'x-functions-key': api_inputs.api_key,
+#         'Content-Type': 'application/json; charset=utf-8',
+#         'user-key': api_inputs.user_id
+#     }
+#
+#     if api_inputs.datacentre == '' or api_inputs.api_key == '':
+#         logger.error("You must call initialize() before using API.")
+#         return pandas.DataFrame()
+#
+#     url = api_prefix + api_inputs.datacentre + "/" + api_inputs.api_project_id + "/RunLogicModule"
+#     response = requests.post(url, json=payload, headers=headers)
+#     response_status = '{} {}'.format(response.status_code, response.reason)
+#     if response.status_code != 200:
+#         logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+#                      response.reason)
+#         return response_status, pandas.DataFrame()
+#     elif len(response.text) == 0:
+#         logger.error('No data returned for this API call. %s', response.request.url)
+#         return response_status, pandas.DataFrame()
+#
+#     table_data = StringIO(response.text)
+#     df = pandas.read_table(table_data, sep=',')
+#     logger.info("API Call successful.")
+#     return response_status, df
+
+
+def sensor_has_not_changed(api_inputs: ApiInputs, start_date: datetime.date, end_date: datetime.date,
+                           templates: List[str] = None, hours_unchanged: int = 24):
+    """Get list of sensors which haven't changed value for the given period and at least ``hours_unchanged``.
+
+    Run the Has Not Changed Query for Selected List of Sites (empty List means All)
+    and Selected List of Templates  (empty List means All)
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    start_date : datetime.date
+        Date the reprocess should start at.
+    end_date : datetime.date
+        Date the reprocess should finish at.
+    templates : List[str]
+        List of ObjectPropertyTemplateNames
+    hours_unchanged: int
+        number of hours the sensor value has not changed
+
+    Returns
+    -------
+    tuple (str, pandas.DataFrame)
+        Returns the response status of the call and a dataframe containing the data returned by the call as a tuple.
+
+    """
+
+    headers = api_inputs.api_headers.default
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if templates.count == 0:
+        logger.error("You must supply templates to report on.")
+        return pandas.DataFrame()
+
+    payload = {
+        "dateFrom": start_date.isoformat(),
+        "dateTo": end_date.isoformat(),
+        "templatesList": templates,
+        "hoursUnchanged": hours_unchanged
+    }
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/analytics/run-adx-query/SensorHasNotChanged"
+
+    response = requests.post(url, json=payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    logger.info("API Call successful.")
+    return response_status, df
```

### Comparing `switch_api-0.5.4b2/switch_api/cache/cache.py` & `switch_api-0.5.4b3/switch_api/cache/cache.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,179 +1,179 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for Cache data requests
-"""
-
-from datetime import datetime
-import json
-import logging
-import sys
-import requests
-import uuid
-
-import pandas
-
-from .._utils._utils import (ApiInputs, is_valid_uuid)
-from .._utils._constants import CACHE_SCOPE
-
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-def set_cache(api_inputs: ApiInputs, scope: CACHE_SCOPE, key: str, val: any, scope_id: uuid.UUID = None):
-    """
-    Sets data to be stored in cache for later retrieval.
-
-    Parameters
-    ----------
-    api_inputs: ApiInputs
-        Object returned by initialize() function.
-    scope: CACHE_SCOPE
-        Cache scope for the stored data
-    key: str
-        Key of to be stored data. Used when retrieving this data.
-    val: any
-        Data to be stored for later retrieval.
-    scope_id: UUID
-        UUID in relation to the scope that was set. Used as well when retrieving the data later on.
-        For Task scope provide TaskId (self.id when calling from the driver)
-        For DataFeed scope provide UUID4 for local testing. 
-            api_inputs.data_feed_id will be used when running in the cloud. 
-        For Portfolio scope, scope_id will be ignored and api_inputs.api_project_id will be used.
-
-    Returns
-    -------
-    """
-    
-    if not set([scope]).issubset(set(CACHE_SCOPE.__args__)):
-        logger.error('scope parameter must be set to one of the allowed values defined by the '
-                        'CACHE_SCOPE literal: %s', CACHE_SCOPE.__args__)
-        return pandas.DataFrame()
-    
-    scope_id_str = str(scope_id)
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if scope != 'Portfolio' and not is_valid_uuid(scope_id_str):
-        logger.error("Invalid UUID v4 passed into scope_id parameter. Please provide a valid UUID v4.")
-        return pandas.DataFrame()
-    
-    if isinstance(val, pandas.DataFrame):
-        val = val.to_json(orient="records")
-        memory_usage = sys.getsizeof(val)
-        if (memory_usage > 2000000):
-            logger.error(f"Data size limit reached. Only 2mb data size is allowed for cache. Size reached {round(memory_usage / 1000000, 2)}mb.")
-            return pandas.DataFrame()
-    elif isinstance(val, datetime):
-        val = val.isoformat()
-
-    headers = api_inputs.api_headers.integration
-    
-    json_payload = {
-        "key": key.strip(),
-        "value": val,
-        "settings": {
-            "dataFeedId": scope_id_str if api_inputs.data_feed_id is None \
-                or api_inputs.data_feed_id == '00000000-0000-0000-0000-000000000000' \
-                    or api_inputs.data_feed_id == '' else api_inputs.data_feed_id,
-            "taskId": scope_id_str,
-            "cacheScope": scope
-        }
-    }
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/task-insights/cache/set"
-    logger.info("Sending request: POST %s", url)
-
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    response_json = json.loads(response.text)
-
-    logger.info("Response status: %s", response_status)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                        response_json['message'])
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-
-    return response_json
-    
-    
-
-def get_cache(api_inputs: ApiInputs, scope: CACHE_SCOPE, key: str, scope_id: uuid.UUID = None):
-    """
-    Gets data stored in cache.
-
-    Parameters
-    ----------
-    api_inputs: ApiInputs
-        Object returned by initialize() function.
-    scope: CACHE_SCOPE
-        Cache scope for the stored data
-    key: str
-        Key of to be stored data. Used when retrieving this data.
-    scope_id: UUID
-        UUID in relation to the scope that was set. Used as well when retrieving the data later on.
-        For Task scope provide TaskId (self.id when calling from the driver)
-        For DataFeed scope provide UUID4 for local testing. 
-            api_inputs.data_feed_id will be used when running in the cloud. 
-        For Portfolio scope, scope_id will be ignored and api_inputs.api_project_id will be used.
-
-    Returns
-    -------
-    """
-
-    if not set([scope]).issubset(set(CACHE_SCOPE.__args__)):
-        logger.error('scope parameter must be set to one of the allowed values defined by the '
-                        'CACHE_SCOPE literal: %s', CACHE_SCOPE.__args__)
-        return pandas.DataFrame()
-
-    scope_id_str = str(scope_id)
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if scope != 'Portfolio' and not is_valid_uuid(scope_id_str):
-        logger.error("Invalid UUID v4 passed into scope_id parameter. Please provide a valid UUID v4.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.integration
-
-    json_payload = {
-        "key": key.strip(),
-        "settings": {
-            "dataFeedId": scope_id_str if api_inputs.data_feed_id is None \
-                or api_inputs.data_feed_id == '00000000-0000-0000-0000-000000000000' \
-                    or api_inputs.data_feed_id == '' else api_inputs.data_feed_id,
-            "taskId": scope_id_str,
-            "cacheScope": scope
-        }
-    }
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/task-insights/cache/get"
-    logger.info("Sending request: POST %s", url)
-
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    response_json = json.loads(response.text)
-    
-    logger.info("Response status: %s", response_status)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                        response_json['message'])
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-    
-    return response_json
-    
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for Cache data requests
+"""
+
+from datetime import datetime
+import json
+import logging
+import sys
+import requests
+import uuid
+
+import pandas
+
+from .._utils._utils import (ApiInputs, is_valid_uuid)
+from .._utils._constants import CACHE_SCOPE
+
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+def set_cache(api_inputs: ApiInputs, scope: CACHE_SCOPE, key: str, val: any, scope_id: uuid.UUID = None):
+    """
+    Sets data to be stored in cache for later retrieval.
+
+    Parameters
+    ----------
+    api_inputs: ApiInputs
+        Object returned by initialize() function.
+    scope: CACHE_SCOPE
+        Cache scope for the stored data
+    key: str
+        Key of to be stored data. Used when retrieving this data.
+    val: any
+        Data to be stored for later retrieval.
+    scope_id: UUID
+        UUID in relation to the scope that was set. Used as well when retrieving the data later on.
+        For Task scope provide TaskId (self.id when calling from the driver)
+        For DataFeed scope provide UUID4 for local testing. 
+            api_inputs.data_feed_id will be used when running in the cloud. 
+        For Portfolio scope, scope_id will be ignored and api_inputs.api_project_id will be used.
+
+    Returns
+    -------
+    """
+    
+    if not set([scope]).issubset(set(CACHE_SCOPE.__args__)):
+        logger.error('scope parameter must be set to one of the allowed values defined by the '
+                        'CACHE_SCOPE literal: %s', CACHE_SCOPE.__args__)
+        return pandas.DataFrame()
+    
+    scope_id_str = str(scope_id)
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if scope != 'Portfolio' and not is_valid_uuid(scope_id_str):
+        logger.error("Invalid UUID v4 passed into scope_id parameter. Please provide a valid UUID v4.")
+        return pandas.DataFrame()
+    
+    if isinstance(val, pandas.DataFrame):
+        val = val.to_json(orient="records")
+        memory_usage = sys.getsizeof(val)
+        if (memory_usage > 2000000):
+            logger.error(f"Data size limit reached. Only 2mb data size is allowed for cache. Size reached {round(memory_usage / 1000000, 2)}mb.")
+            return pandas.DataFrame()
+    elif isinstance(val, datetime):
+        val = val.isoformat()
+
+    headers = api_inputs.api_headers.integration
+    
+    json_payload = {
+        "key": key.strip(),
+        "value": val,
+        "settings": {
+            "dataFeedId": scope_id_str if api_inputs.data_feed_id is None \
+                or api_inputs.data_feed_id == '00000000-0000-0000-0000-000000000000' \
+                    or api_inputs.data_feed_id == '' else api_inputs.data_feed_id,
+            "taskId": scope_id_str,
+            "cacheScope": scope
+        }
+    }
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/task-insights/cache/set"
+    logger.info("Sending request: POST %s", url)
+
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    response_json = json.loads(response.text)
+
+    logger.info("Response status: %s", response_status)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                        response_json['message'])
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+
+    return response_json
+    
+    
+
+def get_cache(api_inputs: ApiInputs, scope: CACHE_SCOPE, key: str, scope_id: uuid.UUID = None):
+    """
+    Gets data stored in cache.
+
+    Parameters
+    ----------
+    api_inputs: ApiInputs
+        Object returned by initialize() function.
+    scope: CACHE_SCOPE
+        Cache scope for the stored data
+    key: str
+        Key of to be stored data. Used when retrieving this data.
+    scope_id: UUID
+        UUID in relation to the scope that was set. Used as well when retrieving the data later on.
+        For Task scope provide TaskId (self.id when calling from the driver)
+        For DataFeed scope provide UUID4 for local testing. 
+            api_inputs.data_feed_id will be used when running in the cloud. 
+        For Portfolio scope, scope_id will be ignored and api_inputs.api_project_id will be used.
+
+    Returns
+    -------
+    """
+
+    if not set([scope]).issubset(set(CACHE_SCOPE.__args__)):
+        logger.error('scope parameter must be set to one of the allowed values defined by the '
+                        'CACHE_SCOPE literal: %s', CACHE_SCOPE.__args__)
+        return pandas.DataFrame()
+
+    scope_id_str = str(scope_id)
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if scope != 'Portfolio' and not is_valid_uuid(scope_id_str):
+        logger.error("Invalid UUID v4 passed into scope_id parameter. Please provide a valid UUID v4.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.integration
+
+    json_payload = {
+        "key": key.strip(),
+        "settings": {
+            "dataFeedId": scope_id_str if api_inputs.data_feed_id is None \
+                or api_inputs.data_feed_id == '00000000-0000-0000-0000-000000000000' \
+                    or api_inputs.data_feed_id == '' else api_inputs.data_feed_id,
+            "taskId": scope_id_str,
+            "cacheScope": scope
+        }
+    }
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/task-insights/cache/get"
+    logger.info("Sending request: POST %s", url)
+
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    response_json = json.loads(response.text)
+    
+    logger.info("Response status: %s", response_status)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                        response_json['message'])
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+    
+    return response_json
+
```

### Comparing `switch_api-0.5.4b2/switch_api/controls/_mqtt.py` & `switch_api-0.5.4b3/switch_api/controls/_mqtt.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,260 +1,260 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module containing the SwitchMQTT class and associated methods referenced by the controls module in the package. This
-module is not directly referenced by end users.
-"""
-import json
-import logging
-import sys
-import time
-from urllib.parse import urlparse
-import paho.mqtt.client as mqtt
-import pandas
-from ._constants import (CONTROL_REQUEST_ACTION_ACK, CONTROL_REQUEST_ACTION_RESULT, WS_DEFAULT_PORT,
-                         WS_MQTT_CONNECTION_TIMEOUT, WS_MQTT_WAIT_TIME_INTERVAL)
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  %(name)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-global message_received
-
-
-class SwitchMQTT:
-    def __init__(self, host_address: str, host_port: int, username: str, password: str, session_id: str,
-                 client_id: str, email: str, project_id: str, installation_id: str):
-        """Constructor for MQTT Message Broker client.
-
-        Parameters
-        ----------
-        host_address : str
-            MQTT message broker host address.
-        host_port :int
-            MQTT message broker port. Defaults to 443.
-        username : str
-            MQTT credentials - username.
-        password :str
-            MQTT credentials - password.
-        session_id : str
-            Unique Session Id for connection.
-        client_id : str
-            Unique Client Id for connection.
-        email : str
-            User email requesting Control.
-        project_id : str
-            Portfolio to connect for control request.
-        installation_id : str
-            Installation to connect for control request.
-        """
-        self.host_address = host_address
-        self.port = WS_DEFAULT_PORT
-        self.client_id = client_id.lower()
-        self.session_id = session_id
-        self.email = email
-        self.project_id = project_id
-        self.installation_id = installation_id
-        self.is_connected = False
-        self.request_status = 0
-        self.connection_timeout = WS_MQTT_CONNECTION_TIMEOUT
-
-        self.sensor_count = 0
-        self.sensor_results: list[dict] = []
-
-        self.mqttc = mqtt.Client(client_id=client_id, transport='websockets')
-        self.mqttc.tls_set()
-        self.mqttc.username_pw_set(username, password)
-        self.mqttc.on_connect = self._on_connect
-        self.mqttc.on_message = self._on_message
-
-    def _on_connect(self, client, userdata, flags, rc):
-        """Event callback when connecting to the MQTT Message Broker.
-        """
-        if rc == 0:
-            logger.info(
-                f"Connected to MQTT broker: {self.host_address} port:{self.port}")
-
-            topics_to_subscribe = [
-                self.client_id,
-                f'control-result/pid/{self.project_id}/site/{self.installation_id}'
-            ]
-
-            for topic in topics_to_subscribe:
-                logger.info(f'Subscribing to topic: {topic}')
-                self.subscribe(topic)
-
-            self.is_connected = True
-        else:
-            logger.error(f"Connection failed with code {rc}")
-
-    def _on_message(self, client, userdata, message):
-        """Event callback when a message received by the client.
-        """
-        payload_str = message.payload.decode()
-        payload = json.loads(payload_str)
-
-        topic = message.topic
-        action = payload.get('action')
-
-        if topic == self.client_id and action == CONTROL_REQUEST_ACTION_ACK:
-            self._process_acknowledgement(payload=payload)
-        elif topic == f'control-result/pid/{self.project_id}/site/{self.installation_id}' and action == CONTROL_REQUEST_ACTION_RESULT:
-            self._process_notification(payload=payload)
-
-    def connect(self, timeout: int = WS_MQTT_CONNECTION_TIMEOUT) -> bool:
-        """Initiate connection to MQTT Broker.
-
-        Parameters
-        ----------
-        timeout : int, Optional
-            Timeout in seconds for trying to connect to MQTT Message Broker.
-                Defaults to WS_MQTT_CONNECTION_TIMEOUT.
-        """
-        self.connection_timeout = timeout
-        url = urlparse(self.host_address)
-
-        logger.info(
-            f'Attempting connection to MQTT broker with client_id={self.client_id} on host={self.host_address} port={self.port}')
-        logger.info(f'hostname={url.hostname}')
-
-        self.mqttc.connect(host=url.hostname, port=self.port)
-        self.mqttc.loop_start()
-
-        start_time = time.time()
-        while not self.is_connected and time.time() - start_time < timeout:
-            time.sleep(WS_MQTT_WAIT_TIME_INTERVAL)
-
-        return self.is_connected
-
-    def send_control_request(self, sensors: list[dict]):
-        """Sends control-request action to Switch MQTT Broker to request control
-
-        Parameters
-        ----------
-        topic : str
-            Message broker topic to pulish message to
-        sensors : dict
-            Sensors to request control and update value
-        """
-        logger.info(f'Sending Control Request for {sensors}')
-
-        self.sensor_count = len(sensors)
-        topic = f'control/pid/{self.project_id}/site/{self.installation_id}'
-
-        payload = self._build_control_request_payload(sensors=sensors)
-        payload_json = json.dumps(payload)
-
-        logger.info(f'Payload = {payload_json}')
-        self.mqttc.publish(topic=topic, payload=payload_json)
-
-        start_time = time.time()
-        while len(self.sensor_results) < self.sensor_count and time.time() - start_time < self.connection_timeout:
-            time.sleep(WS_MQTT_WAIT_TIME_INTERVAL)
-
-        if len(self.sensor_results) == self.sensor_count:
-            logger.info("All Sensor Control Request received.")
-        else:
-            logger.info("Timeout reached.")
-
-        logger.info('Checking missing items...')
-
-        sensor_ids_received = set(item['sensorId']
-                                  for item in self.sensor_results)
-
-        missing_items = [item for item in sensors if item['sensorId']
-                         not in sensor_ids_received]
-
-        for item in missing_items:
-            item['writeStatus'] = "Timeout reached"
-
-        if len(missing_items) == 0:
-            logger.info('There are no missing items.')
-        else:
-            logger.info(f'There are {len(missing_items)} missing items.')
-
-        self._disconnect()
-
-        return pandas.DataFrame(self.sensor_results), pandas.DataFrame(missing_items)
-
-    def subscribe(self, topic: str):
-        """Subscribes to topic.
-
-        Parameters
-        ----------
-        topic : str
-            Topic for Websocket subscription.
-        """
-        self.mqttc.subscribe(topic)
-
-    def _disconnect(self):
-        """Disconnect connection to MQTT Message Broker.
-        """
-        self.mqttc.disconnect()
-        self.mqttc.loop_stop()
-
-    def _process_acknowledgement(self, payload: dict):
-        """Process mqtt message for control result acknowledgement
-
-        Parameters
-        ----------
-        payload : dict
-            Message payload
-        """
-        status_messages = {
-            0: "Appliance doesn't have any of the sensors requested.",
-            1: "Appliance will control some of the sensors in the requested list.",
-            2: "Appliance will control all the sensors in the list."
-        }
-
-        status = payload.get("status", -1)
-        logger.info(
-            f'Control Request Acknowledgment. Mac={payload["mac"]} Status={status_messages.get(status, "Unknown status")}')
-
-        self.request_status = status
-
-    def _process_notification(self, payload: dict):
-        """Process mqtt message for control result notification
-
-        Parameters
-        ----------
-        payload : dict
-            Message payload
-        """
-        write_status = payload.get('status', -1)
-        write_status_description = {
-            0: 'Write success - session in progress',
-            1: 'Write success - session completed',
-            2: 'Write failed - session failed',
-            3: 'Read verification failed - session failed'
-        }.get(write_status, 'Unknown Write Status')
-
-        logger.info(
-            f'Sensor {payload["sensorId"]} Value={payload["presentValue"]}, Status={write_status_description}')
-
-        sensor_control_result = {
-            'sensorId': payload["sensorId"],
-            'controlValue': payload["controlValue"],
-            'presentValue': payload["presentValue"],
-            'status': write_status,
-        }
-
-        if not write_status == 0:
-            self.sensor_results.append(sensor_control_result)
-
-    def _build_control_request_payload(self, sensors: dict):
-        return {
-            'action': 'control-request',
-            'clientId': self.client_id,
-            'originator': self.email,
-            'sensors': sensors,
-            'sessionId': str(self.session_id)
-        }
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module containing the SwitchMQTT class and associated methods referenced by the controls module in the package. This
+module is not directly referenced by end users.
+"""
+import json
+import logging
+import sys
+import time
+from urllib.parse import urlparse
+import paho.mqtt.client as mqtt
+import pandas
+from ._constants import (CONTROL_REQUEST_ACTION_ACK, CONTROL_REQUEST_ACTION_RESULT, WS_DEFAULT_PORT,
+                         WS_MQTT_CONNECTION_TIMEOUT, WS_MQTT_WAIT_TIME_INTERVAL)
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  %(name)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+global message_received
+
+
+class SwitchMQTT:
+    def __init__(self, host_address: str, host_port: int, username: str, password: str, session_id: str,
+                 client_id: str, email: str, project_id: str, installation_id: str):
+        """Constructor for MQTT Message Broker client.
+
+        Parameters
+        ----------
+        host_address : str
+            MQTT message broker host address.
+        host_port :int
+            MQTT message broker port. Defaults to 443.
+        username : str
+            MQTT credentials - username.
+        password :str
+            MQTT credentials - password.
+        session_id : str
+            Unique Session Id for connection.
+        client_id : str
+            Unique Client Id for connection.
+        email : str
+            User email requesting Control.
+        project_id : str
+            Portfolio to connect for control request.
+        installation_id : str
+            Installation to connect for control request.
+        """
+        self.host_address = host_address
+        self.port = WS_DEFAULT_PORT
+        self.client_id = client_id.lower()
+        self.session_id = session_id
+        self.email = email
+        self.project_id = project_id
+        self.installation_id = installation_id
+        self.is_connected = False
+        self.request_status = 0
+        self.connection_timeout = WS_MQTT_CONNECTION_TIMEOUT
+
+        self.sensor_count = 0
+        self.sensor_results: list[dict] = []
+
+        self.mqttc = mqtt.Client(client_id=client_id, transport='websockets')
+        self.mqttc.tls_set()
+        self.mqttc.username_pw_set(username, password)
+        self.mqttc.on_connect = self._on_connect
+        self.mqttc.on_message = self._on_message
+
+    def _on_connect(self, client, userdata, flags, rc):
+        """Event callback when connecting to the MQTT Message Broker.
+        """
+        if rc == 0:
+            logger.info(
+                f"Connected to MQTT broker: {self.host_address} port:{self.port}")
+
+            topics_to_subscribe = [
+                self.client_id,
+                f'control-result/pid/{self.project_id}/site/{self.installation_id}'
+            ]
+
+            for topic in topics_to_subscribe:
+                logger.info(f'Subscribing to topic: {topic}')
+                self.subscribe(topic)
+
+            self.is_connected = True
+        else:
+            logger.error(f"Connection failed with code {rc}")
+
+    def _on_message(self, client, userdata, message):
+        """Event callback when a message received by the client.
+        """
+        payload_str = message.payload.decode()
+        payload = json.loads(payload_str)
+
+        topic = message.topic
+        action = payload.get('action')
+
+        if topic == self.client_id and action == CONTROL_REQUEST_ACTION_ACK:
+            self._process_acknowledgement(payload=payload)
+        elif topic == f'control-result/pid/{self.project_id}/site/{self.installation_id}' and action == CONTROL_REQUEST_ACTION_RESULT:
+            self._process_notification(payload=payload)
+
+    def connect(self, timeout: int = WS_MQTT_CONNECTION_TIMEOUT) -> bool:
+        """Initiate connection to MQTT Broker.
+
+        Parameters
+        ----------
+        timeout : int, Optional
+            Timeout in seconds for trying to connect to MQTT Message Broker.
+                Defaults to WS_MQTT_CONNECTION_TIMEOUT.
+        """
+        self.connection_timeout = timeout
+        url = urlparse(self.host_address)
+
+        logger.info(
+            f'Attempting connection to MQTT broker with client_id={self.client_id} on host={self.host_address} port={self.port}')
+        logger.info(f'hostname={url.hostname}')
+
+        self.mqttc.connect(host=url.hostname, port=self.port)
+        self.mqttc.loop_start()
+
+        start_time = time.time()
+        while not self.is_connected and time.time() - start_time < timeout:
+            time.sleep(WS_MQTT_WAIT_TIME_INTERVAL)
+
+        return self.is_connected
+
+    def send_control_request(self, sensors: list[dict]):
+        """Sends control-request action to Switch MQTT Broker to request control
+
+        Parameters
+        ----------
+        topic : str
+            Message broker topic to pulish message to
+        sensors : dict
+            Sensors to request control and update value
+        """
+        logger.info(f'Sending Control Request for {sensors}')
+
+        self.sensor_count = len(sensors)
+        topic = f'control/pid/{self.project_id}/site/{self.installation_id}'
+
+        payload = self._build_control_request_payload(sensors=sensors)
+        payload_json = json.dumps(payload)
+
+        logger.info(f'Payload = {payload_json}')
+        self.mqttc.publish(topic=topic, payload=payload_json)
+
+        start_time = time.time()
+        while len(self.sensor_results) < self.sensor_count and time.time() - start_time < self.connection_timeout:
+            time.sleep(WS_MQTT_WAIT_TIME_INTERVAL)
+
+        if len(self.sensor_results) == self.sensor_count:
+            logger.info("All Sensor Control Request received.")
+        else:
+            logger.info("Timeout reached.")
+
+        logger.info('Checking missing items...')
+
+        sensor_ids_received = set(item['sensorId']
+                                  for item in self.sensor_results)
+
+        missing_items = [item for item in sensors if item['sensorId']
+                         not in sensor_ids_received]
+
+        for item in missing_items:
+            item['writeStatus'] = "Timeout reached"
+
+        if len(missing_items) == 0:
+            logger.info('There are no missing items.')
+        else:
+            logger.info(f'There are {len(missing_items)} missing items.')
+
+        self._disconnect()
+
+        return pandas.DataFrame(self.sensor_results), pandas.DataFrame(missing_items)
+
+    def subscribe(self, topic: str):
+        """Subscribes to topic.
+
+        Parameters
+        ----------
+        topic : str
+            Topic for Websocket subscription.
+        """
+        self.mqttc.subscribe(topic)
+
+    def _disconnect(self):
+        """Disconnect connection to MQTT Message Broker.
+        """
+        self.mqttc.disconnect()
+        self.mqttc.loop_stop()
+
+    def _process_acknowledgement(self, payload: dict):
+        """Process mqtt message for control result acknowledgement
+
+        Parameters
+        ----------
+        payload : dict
+            Message payload
+        """
+        status_messages = {
+            0: "Appliance doesn't have any of the sensors requested.",
+            1: "Appliance will control some of the sensors in the requested list.",
+            2: "Appliance will control all the sensors in the list."
+        }
+
+        status = payload.get("status", -1)
+        logger.info(
+            f'Control Request Acknowledgment. Mac={payload["mac"]} Status={status_messages.get(status, "Unknown status")}')
+
+        self.request_status = status
+
+    def _process_notification(self, payload: dict):
+        """Process mqtt message for control result notification
+
+        Parameters
+        ----------
+        payload : dict
+            Message payload
+        """
+        write_status = payload.get('status', -1)
+        write_status_description = {
+            0: 'Write success - session in progress',
+            1: 'Write success - session completed',
+            2: 'Write failed - session failed',
+            3: 'Read verification failed - session failed'
+        }.get(write_status, 'Unknown Write Status')
+
+        logger.info(
+            f'Sensor {payload["sensorId"]} Value={payload["presentValue"]}, Status={write_status_description}')
+
+        sensor_control_result = {
+            'sensorId': payload["sensorId"],
+            'controlValue': payload["controlValue"],
+            'presentValue': payload["presentValue"],
+            'status': write_status,
+        }
+
+        if not write_status == 0:
+            self.sensor_results.append(sensor_control_result)
+
+    def _build_control_request_payload(self, sensors: dict):
+        return {
+            'action': 'control-request',
+            'clientId': self.client_id,
+            'originator': self.email,
+            'sensors': sensors,
+            'sessionId': str(self.session_id)
+        }
```

### Comparing `switch_api-0.5.4b2/switch_api/controls/controls.py` & `switch_api-0.5.4b3/switch_api/controls/controls.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,265 +1,265 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for sending control request of sensors.
-"""
-
-import json
-import logging
-import os
-import sys
-import time
-from typing import Union, Optional
-import uuid
-import pandas
-import requests
-from ._constants import IOT_RESPONSE_ERROR, IOT_RESPONSE_SUCCESS, WS_DEFAULT_PORT, WS_MQTT_CONNECTION_TIMEOUT, WS_MQTT_DEFAULT_MAX_TIMEOUT, WS_MQTT_WAIT_TIME_INTERVAL
-from ._mqtt import SwitchMQTT
-from .._utils._utils import ApiInputs, _with_func_attrs, is_valid_uuid
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  %(name)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-global _control_api_endpoint
-global _control_ws_host
-global _control_ws_port
-global _control_ws_username
-global _control_ws_password
-global _control_ws_max_timeout
-
-_control_api_endpoint = ''
-_control_ws_host = ''
-_control_ws_port = WS_DEFAULT_PORT
-_control_ws_username = ''
-_control_ws_password = ''
-_control_ws_max_timeout = WS_MQTT_DEFAULT_MAX_TIMEOUT
-
-
-def set_control_variables(api_endpoint: str, ws_host: str, user_name: str, password: str,
-                          ws_port: int = WS_DEFAULT_PORT, max_timeout: int = WS_MQTT_DEFAULT_MAX_TIMEOUT):
-    """Set Control Variables
-
-    Set Control Variables needed to enable control request to MQTT Broker when running locally.
-
-    In Production, these are pulled from the deployment environment variables.
-
-    Parameters
-    ----------
-    api_endpoint : str
-        Platform IoT API Endpoint.
-    host : str
-        Host URL for MQTT connection. This needs to be datacenter specfic URL.
-    port : int
-        MQTT message broker port. Defaults to 443.
-    user_name : str
-        Username for MQTT connection
-    password: str
-        Password for MQTT connection
-    max_timeout : int
-        Max timeout set for the controls module. Defaults to 30 seconds.
-    """
-    global _control_api_endpoint
-    global _control_ws_host
-    global _control_ws_port
-    global _control_ws_username
-    global _control_ws_password
-    global _control_ws_max_timeout
-
-    # Check if endpoint is a valid URL
-    if not api_endpoint.startswith('https://'):
-        raise ValueError(
-            "Invalid IoT API Endpoint. The IoT host should start with 'https://'.")
-
-    # Check if host is a valid URL
-    if not ws_host.startswith('wss://'):
-        raise ValueError(
-            "Invalid IoT Websocket MQTT Host. The IoT host should start with 'wss://'.")
-
-    # Check if user_name and password are not empty
-    if not user_name:
-        raise ValueError("user_name cannot be empty.")
-    if not password:
-        raise ValueError("password cannot be empty.")
-
-    # Check if max_timeout is greated than 0
-    if max_timeout < 1:
-        raise ValueError("max_timeout should be greater than 0.")
-
-    # Set global variables
-    _control_api_endpoint = api_endpoint
-    _control_ws_host = ws_host
-    _control_ws_port = ws_port
-    _control_ws_username = user_name
-    _control_ws_password = password
-    _control_ws_max_timeout = max_timeout
-
-
-@_with_func_attrs(df_required_columns=['ObjectPropertyId', 'Value', 'TTL'])
-def submit_control(api_inputs: ApiInputs, installation_id: Union[uuid.UUID, str], df: pandas.DataFrame, has_priority: bool, session_id: uuid.UUID, timeout: int = WS_MQTT_CONNECTION_TIMEOUT):
-    """Submit control of sensor(s)
-
-    Required fields are:
-
-    - ObjectPropertyId
-    - Value
-    - TTL
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    df : pandas.DataFrame
-        List of Sensors for control request.
-    has_priority : bool
-        Flag if dataframe passes contains has_priority column.
-    session_id : uuid.UUID., Optional
-        Session Id to reuse when communicating with IoT Endpoint and MQTT Broker
-    timeout : int, Optional:
-        Default value is 30 seconds. Value must be between 1 and max control timeout set in the control variables.
-            When value is set to 0 it defaults to max timeout value.
-            When value is above max timeout value it defaults to max timeout value.
-
-    Returns
-    -------
-    tuple
-        control_response  = is the list of sensors that are acknowledged and process by the MQTTT message broker
-        missing_response = is the list of sensors that are sensors that were caught by the connection time_out
-            default to 30 secs - meaning the response were no longer waited to be received by the python package. 
-            Increasing the time out can potentially help with this.
-    """
-    global _control_api_endpoint
-    global _control_ws_host
-    global _control_ws_port
-    global _control_ws_username
-    global _control_ws_password
-    global _control_ws_max_timeout
-
-    data_frame = df.copy()
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using the API.")
-        return 'Invalid api_inputs.', pandas.DataFrame()
-
-    if not is_valid_uuid(installation_id):
-        logger.error("Installation Id is not a valid UUID.")
-        return 'Invalid installation_id.', pandas.DataFrame()
-
-    if data_frame.empty:
-        logger.error("Dataframe is empty.")
-        return 'Empty dataframe.', pandas.DataFrame()
-
-    if timeout < 0:
-        logger.error(
-            f"Invalid timeout value. Timeout should be between 0 and {_control_ws_max_timeout}. Setting to zero will default to max timeout.")
-        return 'Invalid timeout.', pandas.DataFrame()
-
-    if timeout > _control_ws_max_timeout:
-        logger.critical(
-            f'Timeout is greater than Max Timeout value. Setting timeout to Max Timeout Value instead.')
-        timeout = _control_ws_max_timeout
-
-    if timeout == 0:
-        timeout = _control_ws_max_timeout
-
-    if not is_valid_uuid(session_id):
-        session_id = uuid.uuid4()
-
-    required_columns = getattr(submit_control, 'df_required_columns')
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set().issubset(data_frame.columns):
-        logger.exception('Missing required column(s): %s', set(
-            required_columns).difference(proposed_columns))
-        return 'control.submit_control(): dataframe must contain the following columns: ' + ', '.join(
-            required_columns), pandas.DataFrame()
-
-    control_columns_required = ['ObjectPropertyId', 'Value', 'TTL', 'Priority']
-    data_frame.drop(data_frame.columns.difference(
-        control_columns_required), axis=1, inplace=True)
-
-    # We convert these columns to the required payload property names
-    data_frame = data_frame.rename(columns={'ObjectPropertyId': 'id',
-                                            'Value': 'v', 'TTL': 'dsecs'})
-
-    if has_priority:
-        if not 'Priority' in data_frame:
-            logger.error(
-                f"has_priority is set to True, but the dataframe does not have the column 'Priority'.")
-            return 'Missing Priority column', pandas.DataFrame()
-        else:
-            data_frame = data_frame.rename(columns={'Priority': 'p'})
-
-    json_payload = {
-        "sensors": data_frame.to_dict(orient='records'),
-        "email": api_inputs.email_address,
-        "userid": api_inputs.user_id,
-        "sessionId": str(session_id)
-    }
-
-    url = f"{_control_api_endpoint}/api/gateway/{str(installation_id)}/log-control-request"
-
-    headers = api_inputs.api_headers.default
-
-    logger.info("Sending Control Request to IoT API: POST %s", url)
-    logger.info("Control Request Session Id: %s", str(session_id))
-    logger.info("Control Request for User: %s=%s",
-                api_inputs.email_address, api_inputs.user_id)
-
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    response_object = json.loads(response.text)
-
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.",
-                     response.status_code, response.reason)
-        logger.error(response_object[IOT_RESPONSE_ERROR])
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s',
-                     response.request.url)
-        return response_status, pandas.DataFrame()
-
-    if not response_object[IOT_RESPONSE_SUCCESS]:
-        logger.error(response_object[IOT_RESPONSE_ERROR])
-        return response_object[IOT_RESPONSE_SUCCESS], pandas.DataFrame()
-
-    # Proceeds when the control request is successful
-    logger.info('IoT API Control Request is Successful.')
-
-    data_frame = df.copy()
-
-    data_frame = data_frame.rename(columns={'ObjectPropertyId': 'sensorId',
-                                            'Value': 'controlValue', 'TTL': 'duration'})
-
-    if has_priority:
-        if not 'Priority' in data_frame:
-            logger.error(
-                f"The dataframe does not have the column 'Priority'.")
-        else:
-            data_frame = data_frame.rename(columns={'Priority': 'priority'})
-
-    switch_mqtt = SwitchMQTT(host_address=_control_ws_host, host_port=_control_ws_port,
-                             username=_control_ws_username, password=_control_ws_password,
-                             session_id=session_id, client_id=api_inputs.user_id, email=api_inputs.email_address,
-                             project_id=api_inputs.api_project_id, installation_id=str(installation_id))
-
-    is_connected = switch_mqtt.connect(timeout=timeout)
-
-    if not is_connected:
-        logger.info("Could not connect to MQTT Broker.")
-        return 'Could not connect to MQTT Broker.', pandas.DataFrame()
-
-    control_response, missing_response = switch_mqtt.send_control_request(
-        sensors=data_frame.to_dict(orient='records'))
-
-    return control_response, missing_response
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for sending control request of sensors.
+"""
+
+import json
+import logging
+import os
+import sys
+import time
+from typing import Union, Optional
+import uuid
+import pandas
+import requests
+from ._constants import IOT_RESPONSE_ERROR, IOT_RESPONSE_SUCCESS, WS_DEFAULT_PORT, WS_MQTT_CONNECTION_TIMEOUT, WS_MQTT_DEFAULT_MAX_TIMEOUT, WS_MQTT_WAIT_TIME_INTERVAL
+from ._mqtt import SwitchMQTT
+from .._utils._utils import ApiInputs, _with_func_attrs, is_valid_uuid
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  %(name)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+global _control_api_endpoint
+global _control_ws_host
+global _control_ws_port
+global _control_ws_username
+global _control_ws_password
+global _control_ws_max_timeout
+
+_control_api_endpoint = ''
+_control_ws_host = ''
+_control_ws_port = WS_DEFAULT_PORT
+_control_ws_username = ''
+_control_ws_password = ''
+_control_ws_max_timeout = WS_MQTT_DEFAULT_MAX_TIMEOUT
+
+
+def set_control_variables(api_endpoint: str, ws_host: str, user_name: str, password: str,
+                          ws_port: int = WS_DEFAULT_PORT, max_timeout: int = WS_MQTT_DEFAULT_MAX_TIMEOUT):
+    """Set Control Variables
+
+    Set Control Variables needed to enable control request to MQTT Broker when running locally.
+
+    In Production, these are pulled from the deployment environment variables.
+
+    Parameters
+    ----------
+    api_endpoint : str
+        Platform IoT API Endpoint.
+    host : str
+        Host URL for MQTT connection. This needs to be datacenter specfic URL.
+    port : int
+        MQTT message broker port. Defaults to 443.
+    user_name : str
+        Username for MQTT connection
+    password: str
+        Password for MQTT connection
+    max_timeout : int
+        Max timeout set for the controls module. Defaults to 30 seconds.
+    """
+    global _control_api_endpoint
+    global _control_ws_host
+    global _control_ws_port
+    global _control_ws_username
+    global _control_ws_password
+    global _control_ws_max_timeout
+
+    # Check if endpoint is a valid URL
+    if not api_endpoint.startswith('https://'):
+        raise ValueError(
+            "Invalid IoT API Endpoint. The IoT host should start with 'https://'.")
+
+    # Check if host is a valid URL
+    if not ws_host.startswith('wss://'):
+        raise ValueError(
+            "Invalid IoT Websocket MQTT Host. The IoT host should start with 'wss://'.")
+
+    # Check if user_name and password are not empty
+    if not user_name:
+        raise ValueError("user_name cannot be empty.")
+    if not password:
+        raise ValueError("password cannot be empty.")
+
+    # Check if max_timeout is greated than 0
+    if max_timeout < 1:
+        raise ValueError("max_timeout should be greater than 0.")
+
+    # Set global variables
+    _control_api_endpoint = api_endpoint
+    _control_ws_host = ws_host
+    _control_ws_port = ws_port
+    _control_ws_username = user_name
+    _control_ws_password = password
+    _control_ws_max_timeout = max_timeout
+
+
+@_with_func_attrs(df_required_columns=['ObjectPropertyId', 'Value', 'TTL'])
+def submit_control(api_inputs: ApiInputs, installation_id: Union[uuid.UUID, str], df: pandas.DataFrame, has_priority: bool, session_id: uuid.UUID, timeout: int = WS_MQTT_CONNECTION_TIMEOUT):
+    """Submit control of sensor(s)
+
+    Required fields are:
+
+    - ObjectPropertyId
+    - Value
+    - TTL
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    df : pandas.DataFrame
+        List of Sensors for control request.
+    has_priority : bool
+        Flag if dataframe passes contains has_priority column.
+    session_id : uuid.UUID., Optional
+        Session Id to reuse when communicating with IoT Endpoint and MQTT Broker
+    timeout : int, Optional:
+        Default value is 30 seconds. Value must be between 1 and max control timeout set in the control variables.
+            When value is set to 0 it defaults to max timeout value.
+            When value is above max timeout value it defaults to max timeout value.
+
+    Returns
+    -------
+    tuple
+        control_response  = is the list of sensors that are acknowledged and process by the MQTTT message broker
+        missing_response = is the list of sensors that are sensors that were caught by the connection time_out
+            default to 30 secs - meaning the response were no longer waited to be received by the python package. 
+            Increasing the time out can potentially help with this.
+    """
+    global _control_api_endpoint
+    global _control_ws_host
+    global _control_ws_port
+    global _control_ws_username
+    global _control_ws_password
+    global _control_ws_max_timeout
+
+    data_frame = df.copy()
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using the API.")
+        return 'Invalid api_inputs.', pandas.DataFrame()
+
+    if not is_valid_uuid(installation_id):
+        logger.error("Installation Id is not a valid UUID.")
+        return 'Invalid installation_id.', pandas.DataFrame()
+
+    if data_frame.empty:
+        logger.error("Dataframe is empty.")
+        return 'Empty dataframe.', pandas.DataFrame()
+
+    if timeout < 0:
+        logger.error(
+            f"Invalid timeout value. Timeout should be between 0 and {_control_ws_max_timeout}. Setting to zero will default to max timeout.")
+        return 'Invalid timeout.', pandas.DataFrame()
+
+    if timeout > _control_ws_max_timeout:
+        logger.critical(
+            f'Timeout is greater than Max Timeout value. Setting timeout to Max Timeout Value instead.')
+        timeout = _control_ws_max_timeout
+
+    if timeout == 0:
+        timeout = _control_ws_max_timeout
+
+    if not is_valid_uuid(session_id):
+        session_id = uuid.uuid4()
+
+    required_columns = getattr(submit_control, 'df_required_columns')
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set().issubset(data_frame.columns):
+        logger.exception('Missing required column(s): %s', set(
+            required_columns).difference(proposed_columns))
+        return 'control.submit_control(): dataframe must contain the following columns: ' + ', '.join(
+            required_columns), pandas.DataFrame()
+
+    control_columns_required = ['ObjectPropertyId', 'Value', 'TTL', 'Priority']
+    data_frame.drop(data_frame.columns.difference(
+        control_columns_required), axis=1, inplace=True)
+
+    # We convert these columns to the required payload property names
+    data_frame = data_frame.rename(columns={'ObjectPropertyId': 'id',
+                                            'Value': 'v', 'TTL': 'dsecs'})
+
+    if has_priority:
+        if not 'Priority' in data_frame:
+            logger.error(
+                f"has_priority is set to True, but the dataframe does not have the column 'Priority'.")
+            return 'Missing Priority column', pandas.DataFrame()
+        else:
+            data_frame = data_frame.rename(columns={'Priority': 'p'})
+
+    json_payload = {
+        "sensors": data_frame.to_dict(orient='records'),
+        "email": api_inputs.email_address,
+        "userid": api_inputs.user_id,
+        "sessionId": str(session_id)
+    }
+
+    url = f"{_control_api_endpoint}/api/gateway/{str(installation_id)}/log-control-request"
+
+    headers = api_inputs.api_headers.default
+
+    logger.info("Sending Control Request to IoT API: POST %s", url)
+    logger.info("Control Request Session Id: %s", str(session_id))
+    logger.info("Control Request for User: %s=%s",
+                api_inputs.email_address, api_inputs.user_id)
+
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    response_object = json.loads(response.text)
+
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.",
+                     response.status_code, response.reason)
+        logger.error(response_object[IOT_RESPONSE_ERROR])
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s',
+                     response.request.url)
+        return response_status, pandas.DataFrame()
+
+    if not response_object[IOT_RESPONSE_SUCCESS]:
+        logger.error(response_object[IOT_RESPONSE_ERROR])
+        return response_object[IOT_RESPONSE_SUCCESS], pandas.DataFrame()
+
+    # Proceeds when the control request is successful
+    logger.info('IoT API Control Request is Successful.')
+
+    data_frame = df.copy()
+
+    data_frame = data_frame.rename(columns={'ObjectPropertyId': 'sensorId',
+                                            'Value': 'controlValue', 'TTL': 'duration'})
+
+    if has_priority:
+        if not 'Priority' in data_frame:
+            logger.error(
+                f"The dataframe does not have the column 'Priority'.")
+        else:
+            data_frame = data_frame.rename(columns={'Priority': 'priority'})
+
+    switch_mqtt = SwitchMQTT(host_address=_control_ws_host, host_port=_control_ws_port,
+                             username=_control_ws_username, password=_control_ws_password,
+                             session_id=session_id, client_id=api_inputs.user_id, email=api_inputs.email_address,
+                             project_id=api_inputs.api_project_id, installation_id=str(installation_id))
+
+    is_connected = switch_mqtt.connect(timeout=timeout)
+
+    if not is_connected:
+        logger.info("Could not connect to MQTT Broker.")
+        return 'Could not connect to MQTT Broker.', pandas.DataFrame()
+
+    control_response, missing_response = switch_mqtt.send_control_request(
+        sensors=data_frame.to_dict(orient='records'))
+
+    return control_response, missing_response
```

### Comparing `switch_api-0.5.4b2/switch_api/dataset/dataset.py` & `switch_api-0.5.4b3/switch_api/dataset/dataset.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,195 +1,195 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for interacting with Data Sets from the Switch Automation Platform.
-
-"""
-import pandas
-import requests
-import logging
-import sys
-from io import StringIO
-from .._utils._utils import ApiInputs
-from .._utils._constants import DATA_SET_QUERY_PARAMETER_TYPES
-
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-
-def get_folders(api_inputs: ApiInputs, folder_type="dataset"):
-    """Get dataset folders
-
-    Retrieves the list of DataSets folders for the given portfolio from the Switch Automation
-    platform.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    folder_type : str
-        Type of folder records to retrieve. (Default value = "dataset")
-
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response, df) - Returns the response status of the call & a dataframe containing the data returned by the call.
-
-    """
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    payload = {}
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/folders?type={folder_type}"
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-    df = pandas.read_json(response.text)
-    logger.info("API Call successful.")
-    return response_status, df
-
-
-def get_datasets_list(api_inputs: ApiInputs, include_path: bool = False, path_prefix="*"):
-    """Get list of data sets.
-
-    Retrieves the list of data set queries either for the entire project across all folders
-    (if `include_path`=False) or specific to the folder designated in the `path_prefix`
-    when `include_path` = True.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    include_path : bool, default = False
-        Include the folder path (Default value = False).
-    path_prefix : str
-        The folder path where queries should be returned from (Default value = "*").
-
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response, df) - Returns the response status of the call & a dataframe containing the data returned by the call.
-
-    """
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    payload = {}
-    headers = api_inputs.api_headers.default
-
-    if path_prefix != "*":
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/datasets?pathprefix=" \
-              f"{path_prefix.replace('/', '|')}&includepath={str(include_path)}"
-    else:
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/datasets?includepath=" \
-              f"{str(include_path)}"
-
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    logger.info("API Call successful.")
-
-    return response_status, df
-
-
-def get_data(api_inputs: ApiInputs, dataset_id, parameters: list[dict] = None):
-    """Retrieve data for the given Data Set.
-
-    Retrieves data from the Switch Automation Platform for the given data set.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    dataset_id : uuid.UUID
-        The identifier for the data set.
-    parameters : list[dict], Optional
-        Any parameters required to be passed to the data set.
-        Dict must contain `name`, `value`, and `type` items 
-
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response, df) - Returns the response status of the call & a dataframe containing the data returned by the call.
-    """
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/datasets/queries/{str(dataset_id)}/data"
-    logger.info("Sending request: POST %s", url)
-    logger.info('DataSets: Sending request for DatasetID: %s', dataset_id)
-
-    if parameters is None or len(parameters) <= 0:
-        parameters = []
-    else:
-        for param in parameters:
-            if not 'name' in param:
-                logger.error(f"Missing 'name' property in parameter {param}.")
-                return pandas.DataFrame()
-            if not 'value' in param:
-                logger.error(f"Missing 'value' property in parameter {param}.")
-                return pandas.DataFrame()
-            if not 'type' in param:
-                logger.error(f"Missing 'type' property in parameter {param}.")
-                return pandas.DataFrame()
-            if not param['type'] in DATA_SET_QUERY_PARAMETER_TYPES.__args__:
-                logger.error('type parameter must be set to one of the allowed values defined by the '
-                        'DATA_SET_QUERY_PARAMETER_TYPES literal: %s', DATA_SET_QUERY_PARAMETER_TYPES.__args__)
-                return pandas.DataFrame()
-
-    payload = {
-        'parameters': parameters
-    }
-
-    response = requests.post(url, json=payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('DataSets: no data returned for this call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(StringIO(response.text), dtype=False)
-    logger.info("API Call successful.")
-
-    return response_status, df
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for interacting with Data Sets from the Switch Automation Platform.
+
+"""
+import pandas
+import requests
+import logging
+import sys
+from io import StringIO
+from .._utils._utils import ApiInputs
+from .._utils._constants import DATA_SET_QUERY_PARAMETER_TYPES
+
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+
+def get_folders(api_inputs: ApiInputs, folder_type="dataset"):
+    """Get dataset folders
+
+    Retrieves the list of DataSets folders for the given portfolio from the Switch Automation
+    platform.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    folder_type : str
+        Type of folder records to retrieve. (Default value = "dataset")
+
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response, df) - Returns the response status of the call & a dataframe containing the data returned by the call.
+
+    """
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    payload = {}
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/folders?type={folder_type}"
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+    df = pandas.read_json(response.text)
+    logger.info("API Call successful.")
+    return response_status, df
+
+
+def get_datasets_list(api_inputs: ApiInputs, include_path: bool = False, path_prefix="*"):
+    """Get list of data sets.
+
+    Retrieves the list of data set queries either for the entire project across all folders
+    (if `include_path`=False) or specific to the folder designated in the `path_prefix`
+    when `include_path` = True.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    include_path : bool, default = False
+        Include the folder path (Default value = False).
+    path_prefix : str
+        The folder path where queries should be returned from (Default value = "*").
+
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response, df) - Returns the response status of the call & a dataframe containing the data returned by the call.
+
+    """
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    payload = {}
+    headers = api_inputs.api_headers.default
+
+    if path_prefix != "*":
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/datasets?pathprefix=" \
+              f"{path_prefix.replace('/', '|')}&includepath={str(include_path)}"
+    else:
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/datasets?includepath=" \
+              f"{str(include_path)}"
+
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, data=payload, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    logger.info("API Call successful.")
+
+    return response_status, df
+
+
+def get_data(api_inputs: ApiInputs, dataset_id, parameters: list[dict] = None):
+    """Retrieve data for the given Data Set.
+
+    Retrieves data from the Switch Automation Platform for the given data set.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    dataset_id : uuid.UUID
+        The identifier for the data set.
+    parameters : list[dict], Optional
+        Any parameters required to be passed to the data set.
+        Dict must contain `name`, `value`, and `type` items 
+
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response, df) - Returns the response status of the call & a dataframe containing the data returned by the call.
+    """
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/datasets/queries/{str(dataset_id)}/data"
+    logger.info("Sending request: POST %s", url)
+    logger.info('DataSets: Sending request for DatasetID: %s', dataset_id)
+
+    if parameters is None or len(parameters) <= 0:
+        parameters = []
+    else:
+        for param in parameters:
+            if not 'name' in param:
+                logger.error(f"Missing 'name' property in parameter {param}.")
+                return pandas.DataFrame()
+            if not 'value' in param:
+                logger.error(f"Missing 'value' property in parameter {param}.")
+                return pandas.DataFrame()
+            if not 'type' in param:
+                logger.error(f"Missing 'type' property in parameter {param}.")
+                return pandas.DataFrame()
+            if not param['type'] in DATA_SET_QUERY_PARAMETER_TYPES.__args__:
+                logger.error('type parameter must be set to one of the allowed values defined by the '
+                        'DATA_SET_QUERY_PARAMETER_TYPES literal: %s', DATA_SET_QUERY_PARAMETER_TYPES.__args__)
+                return pandas.DataFrame()
+
+    payload = {
+        'parameters': parameters
+    }
+
+    response = requests.post(url, json=payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('DataSets: no data returned for this call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(StringIO(response.text), dtype=False)
+    logger.info("API Call successful.")
+
+    return response_status, df
```

### Comparing `switch_api-0.5.4b2/switch_api/email/email_sender.py` & `switch_api-0.5.4b3/switch_api/email/email_sender.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for sending emails with attachments to active users within a portfolio in the Switch Automation Platform.
-"""
-
-import logging
-import sys
-from typing import Union
-import requests
-import mimetypes
-import os
-from .._utils._utils import ApiInputs
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-def send_email(
-        api_inputs: ApiInputs, body: str, subject: str,
-        to_recipients: list[str], cc_recipients: list[str] = [], bcc_recipients: list[str] = [],
-        attachments: list[str] = [], attachments_mime_types: list[str] = [], conversation_id: Union[str, None] = None):
-    """
-    Sends an email with attachments to the specified active users within a portfolio in the Switch Automation Platform.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        The API Inputs object.
-    body : str
-        The body of the email.
-    subject : str
-        The subject of the email.
-    to_recipients : list[str]
-        The list of recipient emails.
-    cc_recipients : list[str], optional
-        The list of cc recipient emails, by default None.
-    bcc_recipients : list[str], optional
-        The list of bcc recipient emails, by default None.
-    attachments : list[str], optional
-        The list of attachment paths, by default None.
-    attachments_mime_types : list[str], optional
-        The list of attachment mime types, by default None.
-        Use this parameter to specify the mime type of the attachments that are being guessed incorrectly.
-        Ensure length and order of this list matches the attachments list.
-        Provide None for the attachments that are being guessed correctly.
-    conversation_id : str, optional
-        Correlate related emails into a thread by specifying the same conversation id, by default None.
-
-    Returns
-    -------
-    bool
-        True if the email was sent successfully, False otherwise.
-
-    Examples
-    --------
-    >>> import switch_api as sw
-    >>> api_inputs = sw.initialize(api_project_id='<project_id>')
-    >>> body = "<div>Sending attachments</div>"
-    >>> subject = 'Test Email with Attachments'
-    >>> to_recipients = ["email@example.com"]
-    >>> # Optional attachments
-    >>> attachments = [
-        'Path/To/Attachment/File.<ext>',
-        'Path/To/Attachment/File2.<ext>'
-    ]
-    >>> send_email(api_inputs=api_inputs, body=body, subject=subject, to_recipients=to_recipients, attachments=attachments)
-    """
-
-    if not body:
-        logger.error("No body was provided")
-        return False
-
-    if not subject:
-        logger.error("No subject was provided")
-        return False
-
-    if to_recipients is None or len(to_recipients) == 0:
-        logger.error("No recipients were provided")
-        return False
-
-    logger.info(f'Sending email to recipients: {to_recipients}')
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/task-insights/email/send"
-
-    data = {
-        "Subject": subject,
-        "ContentHtml": body
-    }
-
-    if conversation_id:
-        data["ConversationId"] = conversation_id
-
-    for index, email in enumerate(to_recipients):
-        data[f"ToRecipients[{index}]"] = email
-
-    for index, email in enumerate(cc_recipients or []):
-        data[f"CcRecipients[{index}]"] = email
-
-    for index, email in enumerate(bcc_recipients or []):
-        data[f"BccRecipients[{index}]"] = email
-
-    files = []
-    for index, attachmentPath in enumerate(attachments or []):
-        filename = os.path.basename(attachmentPath)
-        mimetype = attachments_mime_types[index] \
-            if attachments_mime_types and len(attachments_mime_types) >= index \
-            else mimetypes.guess_type(filename)[0]
-
-        files.append(
-            ('Attachments', (filename, open(attachmentPath, 'rb'), mimetype))
-        )
-
-    headers = api_inputs.api_headers.default.copy()
-    del headers['Content-Type']
-
-    logger.info("Sending request: POST %s", url)
-    response = requests.post(url, headers=headers, files=files, data=data)
-
-    if response.status_code != 200:
-        logger.error(f"API Call was not successful. Response Status: {response.status_code}. Reason: {response.reason}. "
-                     f"Response: {response.text}")
-        return False
-
-    logger.info(response.text)
-    return True
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for sending emails with attachments to active users within a portfolio in the Switch Automation Platform.
+"""
+
+import logging
+import sys
+from typing import Union
+import requests
+import mimetypes
+import os
+from .._utils._utils import ApiInputs
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+def send_email(
+        api_inputs: ApiInputs, body: str, subject: str,
+        to_recipients: list[str], cc_recipients: list[str] = [], bcc_recipients: list[str] = [],
+        attachments: list[str] = [], attachments_mime_types: list[str] = [], conversation_id: Union[str, None] = None):
+    """
+    Sends an email with attachments to the specified active users within a portfolio in the Switch Automation Platform.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        The API Inputs object.
+    body : str
+        The body of the email.
+    subject : str
+        The subject of the email.
+    to_recipients : list[str]
+        The list of recipient emails.
+    cc_recipients : list[str], optional
+        The list of cc recipient emails, by default None.
+    bcc_recipients : list[str], optional
+        The list of bcc recipient emails, by default None.
+    attachments : list[str], optional
+        The list of attachment paths, by default None.
+    attachments_mime_types : list[str], optional
+        The list of attachment mime types, by default None.
+        Use this parameter to specify the mime type of the attachments that are being guessed incorrectly.
+        Ensure length and order of this list matches the attachments list.
+        Provide None for the attachments that are being guessed correctly.
+    conversation_id : str, optional
+        Correlate related emails into a thread by specifying the same conversation id, by default None.
+
+    Returns
+    -------
+    bool
+        True if the email was sent successfully, False otherwise.
+
+    Examples
+    --------
+    >>> import switch_api as sw
+    >>> api_inputs = sw.initialize(api_project_id='<project_id>')
+    >>> body = "<div>Sending attachments</div>"
+    >>> subject = 'Test Email with Attachments'
+    >>> to_recipients = ["email@example.com"]
+    >>> # Optional attachments
+    >>> attachments = [
+        'Path/To/Attachment/File.<ext>',
+        'Path/To/Attachment/File2.<ext>'
+    ]
+    >>> send_email(api_inputs=api_inputs, body=body, subject=subject, to_recipients=to_recipients, attachments=attachments)
+    """
+
+    if not body:
+        logger.error("No body was provided")
+        return False
+
+    if not subject:
+        logger.error("No subject was provided")
+        return False
+
+    if to_recipients is None or len(to_recipients) == 0:
+        logger.error("No recipients were provided")
+        return False
+
+    logger.info(f'Sending email to recipients: {to_recipients}')
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/task-insights/email/send"
+
+    data = {
+        "Subject": subject,
+        "ContentHtml": body
+    }
+
+    if conversation_id:
+        data["ConversationId"] = conversation_id
+
+    for index, email in enumerate(to_recipients):
+        data[f"ToRecipients[{index}]"] = email
+
+    for index, email in enumerate(cc_recipients or []):
+        data[f"CcRecipients[{index}]"] = email
+
+    for index, email in enumerate(bcc_recipients or []):
+        data[f"BccRecipients[{index}]"] = email
+
+    files = []
+    for index, attachmentPath in enumerate(attachments or []):
+        filename = os.path.basename(attachmentPath)
+        mimetype = attachments_mime_types[index] \
+            if attachments_mime_types and len(attachments_mime_types) >= index \
+            else mimetypes.guess_type(filename)[0]
+
+        files.append(
+            ('Attachments', (filename, open(attachmentPath, 'rb'), mimetype))
+        )
+
+    headers = api_inputs.api_headers.default.copy()
+    del headers['Content-Type']
+
+    logger.info("Sending request: POST %s", url)
+    response = requests.post(url, headers=headers, files=files, data=data)
+
+    if response.status_code != 200:
+        logger.error(f"API Call was not successful. Response Status: {response.status_code}. Reason: {response.reason}. "
+                     f"Response: {response.text}")
+        return False
+
+    logger.info(response.text)
+    return True
```

### Comparing `switch_api-0.5.4b2/switch_api/error_handlers/__init__.py` & `switch_api-0.5.4b3/switch_api/error_handlers/__init__.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-This module contains the helper functions for error handling.
-
-The module contains three functions:
-    - invalid_file_format() which should be used to validate the source file received against the expected schema and
-      post any issues identified to the data feed dashboard.
-
-    - post_errors() which is used to post errors (apart from those identified by the invalid_file_format() function) to
-      the data feed dashboard.
-
-    - validate_datetime() which checks whether the values of the datetime column(s) of the source file are valid. Any
-      datetime errors identified by this function should be passed to the post_errors() function.
-
-The validate_datetime() function can be used to validate the datetime column(s) of the source file. The output
-`df_invalid_datetime` from this function should be passed to the post_errors() function. For example,
-
->>> import pandas as pd
->>> import switch_api as sw
->>> api_inputs = sw.initialize(user_id, api_project_id) # set api_project_id to the relevant portfolio and user_id to
-                                                       # your own user identifier
->>> test_df = pd.DataFrame({'DateTime':['2021-06-01 00:00:00', '2021-06-01 00:15:00', '', '2021-06-01 00:45:00'],
-... 'Value':[10, 20, 30, 40], 'device_id':['xyz', 'xyz', 'xyz', 'xyz']})
->>> df_invalid_datetime, df = validate_datetime(df=test_df, datetime_col=['DateTime'], dt_fmt='%Y-%m-%d %H:%M:%S')
->>> if df_invalid_datetime.shape[0] != 0:
-...     sw.error_handlers.post_errors(api_inputs, df_invalid_datetime, error_type='DateTime')
-
-"""
-
-from .error_handlers import invalid_file_format, validate_datetime, post_errors, check_duplicates
-
-__all__ = ['validate_datetime', 'invalid_file_format', 'post_errors', 'check_duplicates']
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+This module contains the helper functions for error handling.
+
+The module contains three functions:
+    - invalid_file_format() which should be used to validate the source file received against the expected schema and
+      post any issues identified to the data feed dashboard.
+
+    - post_errors() which is used to post errors (apart from those identified by the invalid_file_format() function) to
+      the data feed dashboard.
+
+    - validate_datetime() which checks whether the values of the datetime column(s) of the source file are valid. Any
+      datetime errors identified by this function should be passed to the post_errors() function.
+
+The validate_datetime() function can be used to validate the datetime column(s) of the source file. The output
+`df_invalid_datetime` from this function should be passed to the post_errors() function. For example,
+
+>>> import pandas as pd
+>>> import switch_api as sw
+>>> api_inputs = sw.initialize(user_id, api_project_id) # set api_project_id to the relevant portfolio and user_id to
+                                                       # your own user identifier
+>>> test_df = pd.DataFrame({'DateTime':['2021-06-01 00:00:00', '2021-06-01 00:15:00', '', '2021-06-01 00:45:00'],
+... 'Value':[10, 20, 30, 40], 'device_id':['xyz', 'xyz', 'xyz', 'xyz']})
+>>> df_invalid_datetime, df = validate_datetime(df=test_df, datetime_col=['DateTime'], dt_fmt='%Y-%m-%d %H:%M:%S')
+>>> if df_invalid_datetime.shape[0] != 0:
+...     sw.error_handlers.post_errors(api_inputs, df_invalid_datetime, error_type='DateTime')
+
+"""
+
+from .error_handlers import invalid_file_format, validate_datetime, post_errors, check_duplicates
+
+__all__ = ['validate_datetime', 'invalid_file_format', 'post_errors', 'check_duplicates']
```

### Comparing `switch_api-0.5.4b2/switch_api/error_handlers/error_handlers.py` & `switch_api-0.5.4b3/switch_api/error_handlers/error_handlers.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,331 +1,331 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-This module contains the helper functions for error handling.
-
-The module contains three functions:
-    - invalid_file_format() which should be used to validate the source file received against the expected schema and
-      post any issues identified to the data feed dashboard.
-
-    - post_errors() which is used to post errors (apart from those identified by the invalid_file_format() function) to
-      the data feed dashboard.
-
-    - validate_datetime() which checks whether the values of the datetime column(s) of the source file are valid. Any
-      datetime errors identified by this function should be passed to the post_errors() function.
-
-The validate_datetime() function can be used to validate the datetime column(s) of the source file. The output
-`df_invalid_datetime` from this function should be passed to the post_errors() function. For example,
-
->>> import pandas as pd
->>> import switch_api as sw
->>> api_inputs = sw.initialize(api_project_id=api_project_id) # set api_project_id to the relevant portfolio
->>> test_df = pd.DataFrame({'DateTime':['2021-06-01 00:00:00', '2021-06-01 00:15:00', '', '2021-06-01 00:45:00'],
-... 'Value':[10, 20, 30, 40], 'device_id':['xyz', 'xyz', 'xyz', 'xyz']})
->>> df_invalid_datetime, df = validate_datetime(df=test_df, datetime_col=['DateTime'], dt_fmt='%Y-%m-%d %H:%M:%S')
->>> if df_invalid_datetime.shape[0] != 0:
-...     sw.error_handlers.post_errors(api_inputs, df_invalid_datetime, error_type='DateTime')
-
-"""
-import pandas
-import pandera
-import logging
-import requests
-import sys
-import json
-from typing import Optional, List, Union
-from .._utils._constants import ERROR_TYPE, PROCESS_STATUS
-from .._utils._utils import ApiInputs
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-def invalid_file_format(api_inputs: ApiInputs, schema: pandera.DataFrameSchema, raw_df: pandas.DataFrame):
-    """Validates the raw file format and posts any errors to data feed dashboard
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by call to initialize()
-    schema : pandera.DataFrameSchema
-        The defined data frame schema object to be used for validation.
-    raw_df : pandas.DataFrame
-        The raw dataframe created by reading the file.
-
-    Returns
-    -------
-    response_boolean: boolean
-        True or False indicating the success of the call
-
-    """
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    error_type = 'FileFormat'
-    process_status = 'Failed'
-
-    try:
-        validated_df = schema.validate(raw_df, lazy=True)
-        return validated_df
-    except pandera.errors.SchemaErrors as err:
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
-              f"{api_inputs.data_feed_id}/errors?status={process_status}&errorType={error_type}&statusId=" \
-              f"{api_inputs.data_feed_file_status_id}"
-        logger.info("Sending request: POST %s", url)
-        logger.error('Schema errors present: %s', err.failure_cases)
-
-        schema_error = err.failure_cases
-
-        response = requests.post(url, data=schema_error.to_json(orient='records'), headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        return response.text
-
-
-def post_errors(api_inputs: ApiInputs, errors: Union[pandas.DataFrame, str], error_type: ERROR_TYPE,
-                process_status: PROCESS_STATUS = 'ActionRequired'):
-    """Post errors to the Data Feed Dashboard
-
-    Post dataframe containing the errors of type ``error_type`` to the Data Feed Dashboard in the Switch Platform.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        The object returned by call to initialize() function.
-    errors : Union[pandas.DataFrame, str]
-        The dataframe containing the row(s) with errors or a string describing the error(s).
-    error_type : ERROR_TYPE
-        The type of error being posted to Data Feed Dashboard.
-    process_status: PROCESS_STATUS, optional
-        Set the status of the process to one of the allowed values specified by the PROCESS_STATUS literal
-        (Default value = 'ActionRequired').
-
-    Returns
-    -------
-    response_boolean: boolean
-        True or False indicating the success of the call
-
-    """
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if (api_inputs.data_feed_id == '00000000-0000-0000-0000-000000000000' or
-            api_inputs.data_feed_file_status_id == '00000000-0000-0000-0000-000000000000'):
-        logger.error("Post Errors can only be called in Production.")
-        return False
-
-    if not set([error_type]).issubset(set(ERROR_TYPE.__args__)):
-        logger.error('error_type parameter must be set to one of the allowed values defined by the '
-                     'ERROR_TYPE literal: %s', ERROR_TYPE.__args__)
-        return False
-
-    if not set([process_status]).issubset(set(PROCESS_STATUS.__args__)):
-        logger.error('process_status parameter must be set to one of the allowed values defined by the '
-                     'PROCESS_STATUS literal: %s', PROCESS_STATUS.__args__)
-        return False
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
-          f"{api_inputs.data_feed_id}/errors?status={process_status}&errorType={error_type}&statusId=" \
-          f"{api_inputs.data_feed_file_status_id}"
-    logger.info("Sending request: POST %s", url)
-
-    payload = ""
-    if (isinstance(errors, pandas.DataFrame)):
-        payload = json.dumps(json.loads(errors.to_json(orient='table'))['data'])
-    elif (isinstance(errors, str)):
-        payload = errors
-    else:
-        logger.error('errors_content parameter must be set to either a type of pandas.DataFrame or string')
-        return False
-
-    response = requests.post(url,
-                             data=payload,
-                             headers=headers)
-
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, False
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, False
-
-    logger.error(f'Directive:ProcessStatus={str(process_status)}')
-
-    return response.text
-
-
-def validate_datetime(df: pandas.DataFrame, datetime_col: Union[str, List[str]],
-                      dt_fmt: Optional[str] = None, errors: bool=False, api_inputs: Union[ApiInputs, None]=None):  # -> tuple[pandas.DataFrame, pandas.DataFrame]:
-    """Check for datetime errors.
-
-    Returns a tuple ``(df_invalid_datetime, df)``, where:
-        - ``df_invalid_datetime`` is a dataframe containing the extracted rows of the input df with invalid datetime
-        values.
-
-        - ``df`` is the original dataframe input after dropping any rows with datetime errors.
-
-    Parameters
-    ----------
-    df : pandas.DataFrame
-        The dataframe that contains the columns to be validated.
-    datetime_col: List[str]
-        List of column names that contain the datetime values to be validated. If passing a single column name in as a
-        string, it will be coerced to a list.
-    dt_fmt : Optional[str]
-        The expected format of the datetime columns to be coerced. The strftime to parse time, eg "%d/%m/%Y", note
-        that "%f" will parse all the way up to nanoseconds. See strftime documentation for more information on
-        choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior
-    errors : bool, optional
-        If True, will automatically post errors to Switch Automation platform. If False, no errors are posted.
-        Defaults to False.
-    api_inputs: Union[ApiInputs, None]=None
-        If post_errors=True, then need to provide api_inputs object. Otherwise, set api_inputs to None.
-
-    Returns
-    -------
-    df_invalid_datetime, df : tuple[pandas.DataFrame, pandas.DataFrame]
-        (df_invalid_datetime, df) - where: `df_invalid_date
-
-    Notes
-    -----
-    If the ``df_invalid_datetime`` dataframe is not empty, then the dataframe should be passed to the ``post_errors()``
-    function using ``error_type = 'DateTime'``. See example code below:
-
-    >>> import pandas as pd
-    >>> import switch_api as sw
-    >>> api_inputs = sw.initialize(api_project_id = api_project_id) # set api_project_id to the relevant portfolio
-    >>> test_df = pd.DataFrame({'DateTime':['2021-06-01 00:00:00', '2021-06-01 00:15:00', '', '2021-06-01 00:45:00'],
-    ... 'Value':[10, 20, 30, 40], 'device_id':['xyz', 'xyz', 'xyz', 'xyz']})
-    >>> df_invalid_datetime, df = validate_datetime(df=test_df, datetime_col=['DateTime'], dt_fmt='%Y-%m-%d %H:%M:%S')
-    >>> if df_invalid_datetime.shape[0] != 0:
-    ...     sw.error_handlers.post_errors(api_inputs, df_invalid_datetime, error_type='DateTime')
-
-    """
-
-    if type(datetime_col) == str:
-        datetime_col = [datetime_col]
-    elif type(datetime_col) == list:
-        datetime_col = datetime_col
-    else:
-        logger.error('datetime_col: Invalid format - datetime_col must be a string or list of strings.')
-
-    if errors==True and api_inputs is None:
-        logger.error("If post_errors set to True, then need to provide a valid api_inputs object. ")
-        return False
-
-    val_dt_cols = []
-    for i in datetime_col:
-        val_dt_cols.append(i + '_dt')
-    lst = [None] * len(datetime_col)
-
-    if dt_fmt is None:
-        for i in range(len(datetime_col)):
-            df[val_dt_cols[i]] = pandas.to_datetime((df[datetime_col[i]]), errors='coerce')
-            lst[i] = df[df[val_dt_cols[i]].isnull()]
-            df = df[df[val_dt_cols[i]].notnull()]
-            lst[i] = lst[i].drop(val_dt_cols[i], axis=1)
-            df = df.drop(val_dt_cols[i], axis=1)
-        df_invalid_datetime = pandas.concat(lst, axis=0)
-        df[datetime_col] = df[datetime_col].apply(lambda x: pandas.to_datetime(x))
-        logger.info('Row count with invalid datetime values: %s', df_invalid_datetime.shape[0])
-        logger.info('Row count with valid datetime values: %s', df.shape[0])
-
-        if errors==True and api_inputs is not None:
-            # send any identified invalid datetime records to Data Feed Dashboard
-            if df_invalid_datetime.shape[0] > 0 and df.shape[0] > 0:
-                post_errors(api_inputs, df_invalid_datetime, error_type='DateTime', process_status='ActionRequired')
-                logger.error('Invalid DateTime values: \n %s', df_invalid_datetime)
-            elif df_invalid_datetime.shape[0] > 0 and df.shape[0] == 0:
-                post_errors(api_inputs, df_invalid_datetime, error_type='DateTime', process_status='Failed')
-                logger.exception('ProcessStatus = Failed. No valid datetime values present in the file.')
-            elif df_invalid_datetime.shape[0] == 0 and df.shape[0] > 0:
-                logger.info('All datetime values in the file are valid. ')
-
-        return df_invalid_datetime, df
-    else:
-        for i in range(len(datetime_col)):
-            df[val_dt_cols[i]] = pandas.to_datetime((df[datetime_col[i]]), errors='coerce', format=dt_fmt)
-            lst[i] = df[df[val_dt_cols[i]].isnull()]
-            df = df[df[val_dt_cols[i]].notnull()]
-            lst[i] = lst[i].drop(val_dt_cols[i], axis=1)
-            df = df.drop(val_dt_cols[i], axis=1)
-        df_invalid_datetime = pandas.concat(lst, axis=0)
-        df[datetime_col] = df[datetime_col].apply(lambda x: pandas.to_datetime(x, format=dt_fmt))
-        logger.info('Row count with invalid datetime values: %s', df_invalid_datetime.shape[0])
-        logger.info('Row count with valid datetime values: %s', df.shape[0])
-
-        if errors==True and api_inputs is not None:
-            # send any identified invalid datetime records to Data Feed Dashboard
-            if df_invalid_datetime.shape[0] > 0 and df.shape[0] > 0:
-                post_errors(api_inputs, df_invalid_datetime, error_type='DateTime', process_status='ActionRequired')
-                logger.error('Invalid DateTime values: \n %s', df_invalid_datetime)
-            elif df_invalid_datetime.shape[0] > 0 and df.shape[0] == 0:
-                post_errors(api_inputs, df_invalid_datetime, error_type='DateTime', process_status='Failed')
-                logger.exception('ProcessStatus = Failed. No valid datetime values present in the file.')
-            elif df_invalid_datetime.shape[0] == 0 and df.shape[0] > 0:
-                logger.info('All datetime values in the file are valid. ')
-
-        return df_invalid_datetime, df
-
-
-def check_duplicates(api_inputs: ApiInputs, raw_df: pandas.DataFrame) -> tuple[bool, pandas.DataFrame]:
-    """Validates the raw file format and posts any errors to data feed dashboard
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by call to initialize()
-    raw_df : pandas.DataFrame
-        The raw dataframe to be checked for duplicate rows
-
-    Returns
-    -------
-    tuple[bool, pandas.DataFrame]
-        response_boolean : True or False indicating the success of the call.
-        response_dataframe : Either a dataframe containing the non-dupliate records (if response_boolean=True) or an
-        empty dataframe (if response_boolean=False)
-
-    """
-    duplicate_records = raw_df[raw_df.duplicated()]
-
-    raw_df = raw_df.drop_duplicates()
-
-    if duplicate_records.shape[0] > 0 and raw_df.shape[0] > 0:
-        post_errors(api_inputs=api_inputs, errors=duplicate_records,
-                                      error_type='DuplicateRecords', process_status='ActionRequired')
-        logger.error('Duplicate records present in file:\n %s', duplicate_records)
-        return True, raw_df
-    elif duplicate_records.shape[0] > 0 and raw_df.shape[0] == 0:
-        post_errors(api_inputs=api_inputs, errors=duplicate_records,
-                                      error_type='DuplicateRecords', process_status='Failed')
-        logger.exception('ProcessStatus = Failed. All records are duplicates.')
-        return False, pandas.DataFrame()
-    elif duplicate_records.shape[0] == 0 and raw_df.shape[0] > 0:
-        logger.info('No duplicate records present in the file. ')
-        return True, raw_df
-
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+This module contains the helper functions for error handling.
+
+The module contains three functions:
+    - invalid_file_format() which should be used to validate the source file received against the expected schema and
+      post any issues identified to the data feed dashboard.
+
+    - post_errors() which is used to post errors (apart from those identified by the invalid_file_format() function) to
+      the data feed dashboard.
+
+    - validate_datetime() which checks whether the values of the datetime column(s) of the source file are valid. Any
+      datetime errors identified by this function should be passed to the post_errors() function.
+
+The validate_datetime() function can be used to validate the datetime column(s) of the source file. The output
+`df_invalid_datetime` from this function should be passed to the post_errors() function. For example,
+
+>>> import pandas as pd
+>>> import switch_api as sw
+>>> api_inputs = sw.initialize(api_project_id=api_project_id) # set api_project_id to the relevant portfolio
+>>> test_df = pd.DataFrame({'DateTime':['2021-06-01 00:00:00', '2021-06-01 00:15:00', '', '2021-06-01 00:45:00'],
+... 'Value':[10, 20, 30, 40], 'device_id':['xyz', 'xyz', 'xyz', 'xyz']})
+>>> df_invalid_datetime, df = validate_datetime(df=test_df, datetime_col=['DateTime'], dt_fmt='%Y-%m-%d %H:%M:%S')
+>>> if df_invalid_datetime.shape[0] != 0:
+...     sw.error_handlers.post_errors(api_inputs, df_invalid_datetime, error_type='DateTime')
+
+"""
+import pandas
+import pandera
+import logging
+import requests
+import sys
+import json
+from typing import Optional, List, Union
+from .._utils._constants import ERROR_TYPE, PROCESS_STATUS
+from .._utils._utils import ApiInputs
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+def invalid_file_format(api_inputs: ApiInputs, schema: pandera.DataFrameSchema, raw_df: pandas.DataFrame):
+    """Validates the raw file format and posts any errors to data feed dashboard
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by call to initialize()
+    schema : pandera.DataFrameSchema
+        The defined data frame schema object to be used for validation.
+    raw_df : pandas.DataFrame
+        The raw dataframe created by reading the file.
+
+    Returns
+    -------
+    response_boolean: boolean
+        True or False indicating the success of the call
+
+    """
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    error_type = 'FileFormat'
+    process_status = 'Failed'
+
+    try:
+        validated_df = schema.validate(raw_df, lazy=True)
+        return validated_df
+    except pandera.errors.SchemaErrors as err:
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
+              f"{api_inputs.data_feed_id}/errors?status={process_status}&errorType={error_type}&statusId=" \
+              f"{api_inputs.data_feed_file_status_id}"
+        logger.info("Sending request: POST %s", url)
+        logger.error('Schema errors present: %s', err.failure_cases)
+
+        schema_error = err.failure_cases
+
+        response = requests.post(url, data=schema_error.to_json(orient='records'), headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        return response.text
+
+
+def post_errors(api_inputs: ApiInputs, errors: Union[pandas.DataFrame, str], error_type: ERROR_TYPE,
+                process_status: PROCESS_STATUS = 'ActionRequired'):
+    """Post errors to the Data Feed Dashboard
+
+    Post dataframe containing the errors of type ``error_type`` to the Data Feed Dashboard in the Switch Platform.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        The object returned by call to initialize() function.
+    errors : Union[pandas.DataFrame, str]
+        The dataframe containing the row(s) with errors or a string describing the error(s).
+    error_type : ERROR_TYPE
+        The type of error being posted to Data Feed Dashboard.
+    process_status: PROCESS_STATUS, optional
+        Set the status of the process to one of the allowed values specified by the PROCESS_STATUS literal
+        (Default value = 'ActionRequired').
+
+    Returns
+    -------
+    response_boolean: boolean
+        True or False indicating the success of the call
+
+    """
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if (api_inputs.data_feed_id == '00000000-0000-0000-0000-000000000000' or
+            api_inputs.data_feed_file_status_id == '00000000-0000-0000-0000-000000000000'):
+        logger.error("Post Errors can only be called in Production.")
+        return False
+
+    if not set([error_type]).issubset(set(ERROR_TYPE.__args__)):
+        logger.error('error_type parameter must be set to one of the allowed values defined by the '
+                     'ERROR_TYPE literal: %s', ERROR_TYPE.__args__)
+        return False
+
+    if not set([process_status]).issubset(set(PROCESS_STATUS.__args__)):
+        logger.error('process_status parameter must be set to one of the allowed values defined by the '
+                     'PROCESS_STATUS literal: %s', PROCESS_STATUS.__args__)
+        return False
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
+          f"{api_inputs.data_feed_id}/errors?status={process_status}&errorType={error_type}&statusId=" \
+          f"{api_inputs.data_feed_file_status_id}"
+    logger.info("Sending request: POST %s", url)
+
+    payload = ""
+    if (isinstance(errors, pandas.DataFrame)):
+        payload = json.dumps(json.loads(errors.to_json(orient='table'))['data'])
+    elif (isinstance(errors, str)):
+        payload = errors
+    else:
+        logger.error('errors_content parameter must be set to either a type of pandas.DataFrame or string')
+        return False
+
+    response = requests.post(url,
+                             data=payload,
+                             headers=headers)
+
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, False
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, False
+
+    logger.error(f'Directive:ProcessStatus={str(process_status)}')
+
+    return response.text
+
+
+def validate_datetime(df: pandas.DataFrame, datetime_col: Union[str, List[str]],
+                      dt_fmt: Optional[str] = None, errors: bool=False, api_inputs: Union[ApiInputs, None]=None):  # -> tuple[pandas.DataFrame, pandas.DataFrame]:
+    """Check for datetime errors.
+
+    Returns a tuple ``(df_invalid_datetime, df)``, where:
+        - ``df_invalid_datetime`` is a dataframe containing the extracted rows of the input df with invalid datetime
+        values.
+
+        - ``df`` is the original dataframe input after dropping any rows with datetime errors.
+
+    Parameters
+    ----------
+    df : pandas.DataFrame
+        The dataframe that contains the columns to be validated.
+    datetime_col: List[str]
+        List of column names that contain the datetime values to be validated. If passing a single column name in as a
+        string, it will be coerced to a list.
+    dt_fmt : Optional[str]
+        The expected format of the datetime columns to be coerced. The strftime to parse time, eg "%d/%m/%Y", note
+        that "%f" will parse all the way up to nanoseconds. See strftime documentation for more information on
+        choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior
+    errors : bool, optional
+        If True, will automatically post errors to Switch Automation platform. If False, no errors are posted.
+        Defaults to False.
+    api_inputs: Union[ApiInputs, None]=None
+        If post_errors=True, then need to provide api_inputs object. Otherwise, set api_inputs to None.
+
+    Returns
+    -------
+    df_invalid_datetime, df : tuple[pandas.DataFrame, pandas.DataFrame]
+        (df_invalid_datetime, df) - where: `df_invalid_date
+
+    Notes
+    -----
+    If the ``df_invalid_datetime`` dataframe is not empty, then the dataframe should be passed to the ``post_errors()``
+    function using ``error_type = 'DateTime'``. See example code below:
+
+    >>> import pandas as pd
+    >>> import switch_api as sw
+    >>> api_inputs = sw.initialize(api_project_id = api_project_id) # set api_project_id to the relevant portfolio
+    >>> test_df = pd.DataFrame({'DateTime':['2021-06-01 00:00:00', '2021-06-01 00:15:00', '', '2021-06-01 00:45:00'],
+    ... 'Value':[10, 20, 30, 40], 'device_id':['xyz', 'xyz', 'xyz', 'xyz']})
+    >>> df_invalid_datetime, df = validate_datetime(df=test_df, datetime_col=['DateTime'], dt_fmt='%Y-%m-%d %H:%M:%S')
+    >>> if df_invalid_datetime.shape[0] != 0:
+    ...     sw.error_handlers.post_errors(api_inputs, df_invalid_datetime, error_type='DateTime')
+
+    """
+
+    if type(datetime_col) == str:
+        datetime_col = [datetime_col]
+    elif type(datetime_col) == list:
+        datetime_col = datetime_col
+    else:
+        logger.error('datetime_col: Invalid format - datetime_col must be a string or list of strings.')
+
+    if errors==True and api_inputs is None:
+        logger.error("If post_errors set to True, then need to provide a valid api_inputs object. ")
+        return False
+
+    val_dt_cols = []
+    for i in datetime_col:
+        val_dt_cols.append(i + '_dt')
+    lst = [None] * len(datetime_col)
+
+    if dt_fmt is None:
+        for i in range(len(datetime_col)):
+            df[val_dt_cols[i]] = pandas.to_datetime((df[datetime_col[i]]), errors='coerce')
+            lst[i] = df[df[val_dt_cols[i]].isnull()]
+            df = df[df[val_dt_cols[i]].notnull()]
+            lst[i] = lst[i].drop(val_dt_cols[i], axis=1)
+            df = df.drop(val_dt_cols[i], axis=1)
+        df_invalid_datetime = pandas.concat(lst, axis=0)
+        df[datetime_col] = df[datetime_col].apply(lambda x: pandas.to_datetime(x))
+        logger.info('Row count with invalid datetime values: %s', df_invalid_datetime.shape[0])
+        logger.info('Row count with valid datetime values: %s', df.shape[0])
+
+        if errors==True and api_inputs is not None:
+            # send any identified invalid datetime records to Data Feed Dashboard
+            if df_invalid_datetime.shape[0] > 0 and df.shape[0] > 0:
+                post_errors(api_inputs, df_invalid_datetime, error_type='DateTime', process_status='ActionRequired')
+                logger.error('Invalid DateTime values: \n %s', df_invalid_datetime)
+            elif df_invalid_datetime.shape[0] > 0 and df.shape[0] == 0:
+                post_errors(api_inputs, df_invalid_datetime, error_type='DateTime', process_status='Failed')
+                logger.exception('ProcessStatus = Failed. No valid datetime values present in the file.')
+            elif df_invalid_datetime.shape[0] == 0 and df.shape[0] > 0:
+                logger.info('All datetime values in the file are valid. ')
+
+        return df_invalid_datetime, df
+    else:
+        for i in range(len(datetime_col)):
+            df[val_dt_cols[i]] = pandas.to_datetime((df[datetime_col[i]]), errors='coerce', format=dt_fmt)
+            lst[i] = df[df[val_dt_cols[i]].isnull()]
+            df = df[df[val_dt_cols[i]].notnull()]
+            lst[i] = lst[i].drop(val_dt_cols[i], axis=1)
+            df = df.drop(val_dt_cols[i], axis=1)
+        df_invalid_datetime = pandas.concat(lst, axis=0)
+        df[datetime_col] = df[datetime_col].apply(lambda x: pandas.to_datetime(x, format=dt_fmt))
+        logger.info('Row count with invalid datetime values: %s', df_invalid_datetime.shape[0])
+        logger.info('Row count with valid datetime values: %s', df.shape[0])
+
+        if errors==True and api_inputs is not None:
+            # send any identified invalid datetime records to Data Feed Dashboard
+            if df_invalid_datetime.shape[0] > 0 and df.shape[0] > 0:
+                post_errors(api_inputs, df_invalid_datetime, error_type='DateTime', process_status='ActionRequired')
+                logger.error('Invalid DateTime values: \n %s', df_invalid_datetime)
+            elif df_invalid_datetime.shape[0] > 0 and df.shape[0] == 0:
+                post_errors(api_inputs, df_invalid_datetime, error_type='DateTime', process_status='Failed')
+                logger.exception('ProcessStatus = Failed. No valid datetime values present in the file.')
+            elif df_invalid_datetime.shape[0] == 0 and df.shape[0] > 0:
+                logger.info('All datetime values in the file are valid. ')
+
+        return df_invalid_datetime, df
+
+
+def check_duplicates(api_inputs: ApiInputs, raw_df: pandas.DataFrame) -> tuple[bool, pandas.DataFrame]:
+    """Validates the raw file format and posts any errors to data feed dashboard
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by call to initialize()
+    raw_df : pandas.DataFrame
+        The raw dataframe to be checked for duplicate rows
+
+    Returns
+    -------
+    tuple[bool, pandas.DataFrame]
+        response_boolean : True or False indicating the success of the call.
+        response_dataframe : Either a dataframe containing the non-dupliate records (if response_boolean=True) or an
+        empty dataframe (if response_boolean=False)
+
+    """
+    duplicate_records = raw_df[raw_df.duplicated()]
+
+    raw_df = raw_df.drop_duplicates()
+
+    if duplicate_records.shape[0] > 0 and raw_df.shape[0] > 0:
+        post_errors(api_inputs=api_inputs, errors=duplicate_records,
+                                      error_type='DuplicateRecords', process_status='ActionRequired')
+        logger.error('Duplicate records present in file:\n %s', duplicate_records)
+        return True, raw_df
+    elif duplicate_records.shape[0] > 0 and raw_df.shape[0] == 0:
+        post_errors(api_inputs=api_inputs, errors=duplicate_records,
+                                      error_type='DuplicateRecords', process_status='Failed')
+        logger.exception('ProcessStatus = Failed. All records are duplicates.')
+        return False, pandas.DataFrame()
+    elif duplicate_records.shape[0] == 0 and raw_df.shape[0] > 0:
+        logger.info('No duplicate records present in the file. ')
+        return True, raw_df
+
```

### Comparing `switch_api-0.5.4b2/switch_api/extensions/__init__.py` & `switch_api-0.5.4b3/switch_api/extensions/__init__.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for providing support for extensions to tasks.
-Allows sharing of code between tasks.
-"""
-
-from .extensions import (provide, get_extension_fields, replace_extension_imports, 
-                         replace_extensions_imports, tokenize_extension_imports)
-
-from .field_meta import FieldMeta
-
-from .helpers import find_cycles_by_field_type, convert_to_serializeable_fields, has_extensions_support
-
-from .pipeline import ExtensionTask
-
-__all__ = ['ExtensionTask', 'provide', 'get_extension_fields', 'replace_extension_imports', 
-           'replace_extensions_imports', 'tokenize_extension_imports', 'find_cycles_by_field_type',
-           'convert_to_serializeable_fields', 'has_extensions_support', 'FieldMeta']
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for providing support for extensions to tasks.
+Allows sharing of code between tasks.
+"""
+
+from .extensions import (provide, get_extension_fields, replace_extension_imports, 
+                         replace_extensions_imports, tokenize_extension_imports)
+
+from .field_meta import FieldMeta
+
+from .helpers import find_cycles_by_field_type, convert_to_serializeable_fields, has_extensions_support
+
+from .pipeline import ExtensionTask
+
+__all__ = ['ExtensionTask', 'provide', 'get_extension_fields', 'replace_extension_imports', 
+           'replace_extensions_imports', 'tokenize_extension_imports', 'find_cycles_by_field_type',
+           'convert_to_serializeable_fields', 'has_extensions_support', 'FieldMeta']
```

### Comparing `switch_api-0.5.4b2/switch_api/extensions/extensions.py` & `switch_api-0.5.4b3/switch_api/extensions/extensions.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,236 +1,236 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-
-"""Module providing support for extensions to the Task classes."""
-
-from typing import List, Tuple, Union, get_type_hints
-
-from .pipeline import ExtensionTask
-from .field_meta import FieldMeta
-from .helpers import get_code, convert_to_serializeable_fields, find_cycles_by_field_type, has_annotations, HasExtensionClsAttributeName
-
-
-def provide(field: Union[str, List[str]]):
-    """Decorator used to provide an extension to the Task class 
-    by setting an attribute on the class matching field
-
-    Parameters
-    ----------
-    field: str | list
-        The field on the class to inject the extension into.
-
-    Returns
-    -------
-    function
-        The decorator function.
-
-    """
-
-    def decorator(cls):
-        if not cls or not isinstance(cls, type):
-            raise TypeError("The provided class must be a valid class")
-        
-        if issubclass(cls, ExtensionTask):
-            raise TypeError(f"""
-                Extensions cannot be provided to other extensions at this time. 
-                Please check for nested extensions in class '{cls.__name__}'.
-                Nested Extensions are extensions which depend on other extensions.
-                """)
-
-        # Load the module from the local file system
-        type_hints = get_type_hints(cls)
-        
-        if isinstance(field, str):
-            __provide(cls, type_hints, field)
-        elif isinstance(field, list):
-            field_list = filter(lambda x: x and isinstance(x, str), field)
-            for f in field_list:
-                __provide(cls, type_hints, f)
-
-        return cls
-
-    def __provide(cls, type_hints: dict[str, any], inner_field: str):
-        if inner_field not in type_hints:
-            raise TypeError(f"Provided field '{inner_field}' must exist as a field on the class {cls.__name__}")
-        
-        module_type = type_hints[inner_field]
-
-        if not issubclass(module_type, ExtensionTask):
-            raise TypeError(f"Provided field '{inner_field}' must have a return type that inherits from ExtensionTask")
-
-        module = module_type()
-
-        # Set the module as an attribute of the class
-        setattr(cls, inner_field, module)
-
-        # Set attribute on the class to identify that it has extensions
-        if not hasattr(cls, HasExtensionClsAttributeName):
-            setattr(cls, HasExtensionClsAttributeName, True)
-
-    return decorator
-
-
-def get_extension_fields(cls):
-    """Get a list of the injected fields on the class.
-    
-    Parameters
-    ----------
-    cls : type
-        The class to get the injected fields from.
-
-    Returns
-    -------
-    list
-        A list of the injected fields on the class.
-
-    """
-
-    injected_fields: list[FieldMeta] = []
-
-    if not has_annotations(cls): # Check if the class has type annotations
-        return injected_fields
-
-    for field_name in cls.__annotations__:
-        if hasattr(cls, field_name):
-            field_type = cls.__annotations__[field_name]
-
-            if issubclass(field_type, ExtensionTask):
-
-                # Add variable called version that returns the version of the injected module
-                field = getattr(cls, field_name)
-                injected_fields.append(FieldMeta(
-                    field_name=field_name,
-                    field_type=field_type,
-                    extension_name=field_type.__name__, # Extension class name
-                    id=getattr(field, "id"),
-                    version=getattr(field, "version"),
-                    description=getattr(field, "description"),
-                    author=getattr(field, "author"),
-                    nested_extensions=get_extension_fields(field_type)
-                ))
-
-    return injected_fields
-
-
-def replace_extension_imports(task: ExtensionTask, replacements: List[Tuple[str, str]]) -> str:
-    """
-    Replace the import statements for the injected extensions given replacements.
-
-    Parameters
-    ----------
-
-    task : ExtensionTask
-
-    replacements : List[Tuple[str, str]]
-        A list of tuples containing the old path and the new path.
-        Example: 
-            replacements = [
-                ('my_extension.MyExtension', 'my_extension_second')
-            ]
-
-    Returns
-    -------
-    str
-        The code of the task with the replaced import statements.
-    """
-
-
-    # Get a list of all the injected fields in the code
-    extension_fields = get_extension_fields(task)
-
-    serializeable_fields = convert_to_serializeable_fields(extension_fields)
-
-    cycles = find_cycles_by_field_type(serializeable_fields)
-
-    if cycles:
-        raise Exception(f"Found cycles in the extension dependencies: {cycles}")
-
-    # Get the code of the task
-    code = get_code(task)
-
-    # Loop through all the replacements and check if any of them match the type of an injected field
-    for old_path, new_path in replacements:
-        for field in extension_fields:
-            field_type = field.field_type
-            if field_type.__module__ == old_path:
-                # Replace the import statement with the new path
-                code = code.replace(f"from {field_type.__module__}", f"from {new_path}")
-
-    return code
-
-def replace_extensions_imports(task: ExtensionTask, extensions_dir: str = 'extensions') -> str:
-    """
-    Replaces all extensions import statements to the format of <extensions_dir>.<extension_name>
-
-    For example:
-        from extensions.SimpleExtension import SimpleExtension
-
-    Parameters
-    ----------
-
-    task : ExtensionTask
-
-    extensions_dir : str
-
-    Returns
-    -------
-    str
-        The code of the task with the replaced import statements.
-    """
-
-
-    # Get a list of all the injected fields in the code
-    extension_fields = get_extension_fields(task)
-
-    serializeable_fields = convert_to_serializeable_fields(extension_fields)
-
-    cycles = find_cycles_by_field_type(serializeable_fields)
-
-    if cycles:
-        raise Exception(f"Found cycles in the extension dependencies: {cycles}")
-
-    replacements = []
-
-    for field in extension_fields:
-        field_type = field.field_type
-
-        replacements.append((field_type.__module__, extensions_dir + '.' + field_type.__qualname__))
-
-    return replace_extension_imports(task, replacements)
-
-def tokenize_extension_imports(task: ExtensionTask) -> str:
-    """
-    Replace the import statements for the injected extensions with tokens to be replaced later.
-
-    Parameters
-    ----------
-
-    task : ExtensionTask
-
-    extensions_dir : str
-
-    Returns
-    -------
-    str
-        The code of the task with the tokenized import statements.
-    """
-
-
-    # Get a list of all the injected fields in the code
-    injected_fields = get_extension_fields(task)
-
-    # Get the code of the task
-    code = get_code(task)
-
-    # Loop through all the replacements and check if any of them match the type of an injected field
-    for field in injected_fields:
-        field_type = field.field_type
-
-        # Replace the import statement with the new path
-        code = code.replace(f"from {field_type.__module__} import {field_type.__qualname__}",
-                            f"from {{sw_extensions_base_dir}}.{field_type.__qualname__} import {field_type.__qualname__}")
-
-    return code
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+
+"""Module providing support for extensions to the Task classes."""
+
+from typing import List, Tuple, Union, get_type_hints
+
+from .pipeline import ExtensionTask
+from .field_meta import FieldMeta
+from .helpers import get_code, convert_to_serializeable_fields, find_cycles_by_field_type, has_annotations, HasExtensionClsAttributeName
+
+
+def provide(field: Union[str, List[str]]):
+    """Decorator used to provide an extension to the Task class 
+    by setting an attribute on the class matching field
+
+    Parameters
+    ----------
+    field: str | list
+        The field on the class to inject the extension into.
+
+    Returns
+    -------
+    function
+        The decorator function.
+
+    """
+
+    def decorator(cls):
+        if not cls or not isinstance(cls, type):
+            raise TypeError("The provided class must be a valid class")
+        
+        if issubclass(cls, ExtensionTask):
+            raise TypeError(f"""
+                Extensions cannot be provided to other extensions at this time. 
+                Please check for nested extensions in class '{cls.__name__}'.
+                Nested Extensions are extensions which depend on other extensions.
+                """)
+
+        # Load the module from the local file system
+        type_hints = get_type_hints(cls)
+        
+        if isinstance(field, str):
+            __provide(cls, type_hints, field)
+        elif isinstance(field, list):
+            field_list = filter(lambda x: x and isinstance(x, str), field)
+            for f in field_list:
+                __provide(cls, type_hints, f)
+
+        return cls
+
+    def __provide(cls, type_hints: dict[str, any], inner_field: str):
+        if inner_field not in type_hints:
+            raise TypeError(f"Provided field '{inner_field}' must exist as a field on the class {cls.__name__}")
+        
+        module_type = type_hints[inner_field]
+
+        if not issubclass(module_type, ExtensionTask):
+            raise TypeError(f"Provided field '{inner_field}' must have a return type that inherits from ExtensionTask")
+
+        module = module_type()
+
+        # Set the module as an attribute of the class
+        setattr(cls, inner_field, module)
+
+        # Set attribute on the class to identify that it has extensions
+        if not hasattr(cls, HasExtensionClsAttributeName):
+            setattr(cls, HasExtensionClsAttributeName, True)
+
+    return decorator
+
+
+def get_extension_fields(cls):
+    """Get a list of the injected fields on the class.
+    
+    Parameters
+    ----------
+    cls : type
+        The class to get the injected fields from.
+
+    Returns
+    -------
+    list
+        A list of the injected fields on the class.
+
+    """
+
+    injected_fields: list[FieldMeta] = []
+
+    if not has_annotations(cls): # Check if the class has type annotations
+        return injected_fields
+
+    for field_name in cls.__annotations__:
+        if hasattr(cls, field_name):
+            field_type = cls.__annotations__[field_name]
+
+            if issubclass(field_type, ExtensionTask):
+
+                # Add variable called version that returns the version of the injected module
+                field = getattr(cls, field_name)
+                injected_fields.append(FieldMeta(
+                    field_name=field_name,
+                    field_type=field_type,
+                    extension_name=field_type.__name__, # Extension class name
+                    id=getattr(field, "id"),
+                    version=getattr(field, "version"),
+                    description=getattr(field, "description"),
+                    author=getattr(field, "author"),
+                    nested_extensions=get_extension_fields(field_type)
+                ))
+
+    return injected_fields
+
+
+def replace_extension_imports(task: ExtensionTask, replacements: List[Tuple[str, str]]) -> str:
+    """
+    Replace the import statements for the injected extensions given replacements.
+
+    Parameters
+    ----------
+
+    task : ExtensionTask
+
+    replacements : List[Tuple[str, str]]
+        A list of tuples containing the old path and the new path.
+        Example: 
+            replacements = [
+                ('my_extension.MyExtension', 'my_extension_second')
+            ]
+
+    Returns
+    -------
+    str
+        The code of the task with the replaced import statements.
+    """
+
+
+    # Get a list of all the injected fields in the code
+    extension_fields = get_extension_fields(task)
+
+    serializeable_fields = convert_to_serializeable_fields(extension_fields)
+
+    cycles = find_cycles_by_field_type(serializeable_fields)
+
+    if cycles:
+        raise Exception(f"Found cycles in the extension dependencies: {cycles}")
+
+    # Get the code of the task
+    code = get_code(task)
+
+    # Loop through all the replacements and check if any of them match the type of an injected field
+    for old_path, new_path in replacements:
+        for field in extension_fields:
+            field_type = field.field_type
+            if field_type.__module__ == old_path:
+                # Replace the import statement with the new path
+                code = code.replace(f"from {field_type.__module__}", f"from {new_path}")
+
+    return code
+
+def replace_extensions_imports(task: ExtensionTask, extensions_dir: str = 'extensions') -> str:
+    """
+    Replaces all extensions import statements to the format of <extensions_dir>.<extension_name>
+
+    For example:
+        from extensions.SimpleExtension import SimpleExtension
+
+    Parameters
+    ----------
+
+    task : ExtensionTask
+
+    extensions_dir : str
+
+    Returns
+    -------
+    str
+        The code of the task with the replaced import statements.
+    """
+
+
+    # Get a list of all the injected fields in the code
+    extension_fields = get_extension_fields(task)
+
+    serializeable_fields = convert_to_serializeable_fields(extension_fields)
+
+    cycles = find_cycles_by_field_type(serializeable_fields)
+
+    if cycles:
+        raise Exception(f"Found cycles in the extension dependencies: {cycles}")
+
+    replacements = []
+
+    for field in extension_fields:
+        field_type = field.field_type
+
+        replacements.append((field_type.__module__, extensions_dir + '.' + field_type.__qualname__))
+
+    return replace_extension_imports(task, replacements)
+
+def tokenize_extension_imports(task: ExtensionTask) -> str:
+    """
+    Replace the import statements for the injected extensions with tokens to be replaced later.
+
+    Parameters
+    ----------
+
+    task : ExtensionTask
+
+    extensions_dir : str
+
+    Returns
+    -------
+    str
+        The code of the task with the tokenized import statements.
+    """
+
+
+    # Get a list of all the injected fields in the code
+    injected_fields = get_extension_fields(task)
+
+    # Get the code of the task
+    code = get_code(task)
+
+    # Loop through all the replacements and check if any of them match the type of an injected field
+    for field in injected_fields:
+        field_type = field.field_type
+
+        # Replace the import statement with the new path
+        code = code.replace(f"from {field_type.__module__} import {field_type.__qualname__}",
+                            f"from {{sw_extensions_base_dir}}.{field_type.__qualname__} import {field_type.__qualname__}")
+
+    return code
```

### Comparing `switch_api-0.5.4b2/switch_api/extensions/field_meta.py` & `switch_api-0.5.4b3/switch_api/extensions/field_meta.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-import json
-from typing import List
-import uuid
-
-
-class FieldMeta:
-    """A class for storing metadata about a field on a class.
-
-    Attributes
-    ----------
-    field_name : str
-        The name of the field.
-    field_type : type
-        The type of the field.
-    extension_name : str
-        The name of the extension.
-    id : uuid.UUID
-        The unique identifier of the extension.
-    version : str
-        The version of the extension.
-    description : str
-        A brief description of the extension.
-    author : str
-        The author of the extension.
-    nested_extensions : List[dict]
-        A list of nested extensions.
-
-    """
-
-    def __init__(self, 
-                 field_name: str, field_type: type, extension_name: str, 
-                 id: uuid.UUID, version: str, description: str, author: str, 
-                 nested_extensions: List[dict]):
-        self.field_name = field_name
-        self.field_type = field_type
-        self.extension_name = extension_name
-        self.id = id
-        self.version = version
-        self.description = description
-        self.author = author
-        self.nested_extensions = nested_extensions
-
-    def to_dict(self):
-        return {
-            "field_name": self.field_name,
-            "field_type": self.field_type.__name__,
-            "extension_name": self.extension_name,
-            "id": self.id,
-            "version": self.version,
-            "description": self.description,
-            "author": self.author,
-            "nested_extensions": [x.to_dict() for x in self.nested_extensions]
-        }
-
-    def to_json(self):
+import json
+from typing import List
+import uuid
+
+
+class FieldMeta:
+    """A class for storing metadata about a field on a class.
+
+    Attributes
+    ----------
+    field_name : str
+        The name of the field.
+    field_type : type
+        The type of the field.
+    extension_name : str
+        The name of the extension.
+    id : uuid.UUID
+        The unique identifier of the extension.
+    version : str
+        The version of the extension.
+    description : str
+        A brief description of the extension.
+    author : str
+        The author of the extension.
+    nested_extensions : List[dict]
+        A list of nested extensions.
+
+    """
+
+    def __init__(self, 
+                 field_name: str, field_type: type, extension_name: str, 
+                 id: uuid.UUID, version: str, description: str, author: str, 
+                 nested_extensions: List[dict]):
+        self.field_name = field_name
+        self.field_type = field_type
+        self.extension_name = extension_name
+        self.id = id
+        self.version = version
+        self.description = description
+        self.author = author
+        self.nested_extensions = nested_extensions
+
+    def to_dict(self):
+        return {
+            "field_name": self.field_name,
+            "field_type": self.field_type.__name__,
+            "extension_name": self.extension_name,
+            "id": self.id,
+            "version": self.version,
+            "description": self.description,
+            "author": self.author,
+            "nested_extensions": [x.to_dict() for x in self.nested_extensions]
+        }
+
+    def to_json(self):
         return json.dumps(self.to_dict())
```

### Comparing `switch_api-0.5.4b2/switch_api/initialize.py` & `switch_api-0.5.4b3/switch_api/initialize.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,85 +1,85 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-import sys
-import uuid
-import logging
-from ._authentication._authentication import get_switch_credentials
-from ._utils._utils import ApiInputs, ApiHeaders
-from ._utils._constants import SWITCH_ENVIRONMENT
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-# consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-def initialize(api_project_id: uuid.UUID, environment: SWITCH_ENVIRONMENT = 'Production') -> ApiInputs:
-    """
-    Initialize session with API when running scripts on your local machine.
-
-    Parameters
-    ----------
-    api_project_id : uuid.UUID
-        The ApiProjectID for the portfolio.
-    environment: SWITCH_ENVIRONMENT
-        (Optional) The Switch API environment to interact with
-
-    Returns
-    -------
-    ApiInputs
-        Returns the ApiInputs namedtuple that is required as an input to other functions.
-    """
-
-    try:
-        uuid.UUID(api_project_id)
-    except:
-        logger.error(
-            f"Invalid value provided '{api_project_id}'- please provide a valid api_project_id. ")
-        return False
-
-    creds = get_switch_credentials(environment, api_project_id)
-
-    logger.info("Successfully initialized.")
-    api_base_url = f"https://{creds.api_endpoint}/api/1.0"
-    # api_base_url = f"https://localhost:5001/api/1.0"
-    api_projects_endpoint = f"{api_base_url}/projects"
-
-    token = creds.api_token
-
-    data_feed_id = '00000000-0000-0000-0000-000000000000'
-    data_feed_file_status_id = '00000000-0000-0000-0000-000000000000'
-
-    api_default = {
-        'Content-Type': 'application/json; charset=utf-8',
-        'Ocp-Apim-Subscription-Key': creds.subscription_key,
-        'Authorization': f'bearer {token}'
-    }
-
-    operation_metadata = {
-        'DataFeedId': data_feed_id,
-        'DataFeedFileStatusId': data_feed_file_status_id
-    }
-
-    # API Header specific for Integration API endpoints
-    api_integration = {
-        'Content-Type': 'application/json; charset=utf-8',
-        'Ocp-Apim-Subscription-Key': creds.subscription_key,
-        'Authorization': f'bearer {token}',
-        'X-Operation-Metadata': str(operation_metadata)
-    }
-
-    api_headers = ApiHeaders(default=api_default, integration=api_integration)
-
-    return ApiInputs(email_address=creds.email, user_id=creds.user_id, api_project_id=api_project_id,
-                     data_feed_id=data_feed_id, data_feed_file_status_id=data_feed_file_status_id, bearer_token=token,
-                     api_base_url=api_base_url, api_projects_endpoint=api_projects_endpoint,
-                     subscription_key=creds.subscription_key, api_headers=api_headers)
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+import sys
+import uuid
+import logging
+from ._authentication._authentication import get_switch_credentials
+from ._utils._utils import ApiInputs, ApiHeaders
+from ._utils._constants import SWITCH_ENVIRONMENT
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+# consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+def initialize(api_project_id: uuid.UUID, environment: SWITCH_ENVIRONMENT = 'Production', custom_port: int = 51796) -> ApiInputs:
+    """
+    Initialize session with API when running scripts on your local machine.
+
+    Parameters
+    ----------
+    api_project_id : uuid.UUID
+        The ApiProjectID for the portfolio.
+    environment: SWITCH_ENVIRONMENT
+        (Optional) The Switch API environment to interact with
+
+    Returns
+    -------
+    ApiInputs
+        Returns the ApiInputs namedtuple that is required as an input to other functions.
+    """
+
+    try:
+        uuid.UUID(api_project_id)
+    except:
+        logger.error(
+            f"Invalid value provided '{api_project_id}'- please provide a valid api_project_id. ")
+        return False
+
+    creds = get_switch_credentials(environment, api_project_id, custom_port)
+
+    logger.info("Successfully initialized.")
+    api_base_url = f"https://{creds.api_endpoint}/api/1.0"
+    # api_base_url = f"https://localhost:5001/api/1.0"
+    api_projects_endpoint = f"{api_base_url}/projects"
+
+    token = creds.api_token
+
+    data_feed_id = '00000000-0000-0000-0000-000000000000'
+    data_feed_file_status_id = '00000000-0000-0000-0000-000000000000'
+
+    api_default = {
+        'Content-Type': 'application/json; charset=utf-8',
+        'Ocp-Apim-Subscription-Key': creds.subscription_key,
+        'Authorization': f'bearer {token}'
+    }
+
+    operation_metadata = {
+        'DataFeedId': data_feed_id,
+        'DataFeedFileStatusId': data_feed_file_status_id
+    }
+
+    # API Header specific for Integration API endpoints
+    api_integration = {
+        'Content-Type': 'application/json; charset=utf-8',
+        'Ocp-Apim-Subscription-Key': creds.subscription_key,
+        'Authorization': f'bearer {token}',
+        'X-Operation-Metadata': str(operation_metadata)
+    }
+
+    api_headers = ApiHeaders(default=api_default, integration=api_integration)
+
+    return ApiInputs(email_address=creds.email, user_id=creds.user_id, api_project_id=api_project_id,
+                     data_feed_id=data_feed_id, data_feed_file_status_id=data_feed_file_status_id, bearer_token=token,
+                     api_base_url=api_base_url, api_projects_endpoint=api_projects_endpoint,
+                     subscription_key=creds.subscription_key, api_headers=api_headers)
```

### Comparing `switch_api-0.5.4b2/switch_api/integration/__init__.py` & `switch_api-0.5.4b3/switch_api/integration/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,23 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for integrating asset creation, asset updates, data ingestion, etc into the Switch Automation Platform.
-"""
-
-from .integration import (upsert_data, upsert_sites, upsert_workorders, upsert_device_sensors, upsert_device_sensors_ext,
-                          upsert_timeseries_ds, replace_data, append_data, upsert_file_row_count,
-                          upsert_discovered_records, upsert_timeseries, upsert_tags, upsert_device_metadata,
-                          upsert_reservations)
-
-from .helpers import (get_sites, get_tag_groups, get_metadata_keys, get_data, get_device_sensors, get_templates,
-                      get_units_of_measure, get_states_by_country, get_operation_state, get_equipment_classes,
-                      load_data, get_timezones, amortise_across_days, get_metadata_where_clause, connect_to_sql)
-
-__all__ = ['upsert_data', 'upsert_sites', 'upsert_workorders', 'upsert_device_sensors', 'upsert_device_sensors_ext',
-           'upsert_timeseries_ds', 'replace_data', 'append_data', 'get_sites', 'get_tag_groups', 'get_metadata_keys',
-           'get_data', 'get_device_sensors', 'get_templates', 'get_units_of_measure', 'get_states_by_country',
-           'get_operation_state', 'get_equipment_classes', 'load_data', 'upsert_file_row_count',
-           'upsert_discovered_records', 'upsert_timeseries', 'upsert_tags', 'upsert_device_metadata', 'get_timezones',
-           'amortise_across_days', 'get_metadata_where_clause', 'connect_to_sql', 'upsert_reservations']
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for integrating asset creation, asset updates, data ingestion, etc into the Switch Automation Platform.
+"""
+
+from .integration import (upsert_data, upsert_sites, upsert_workorders, upsert_device_sensors, upsert_device_sensors_ext,
+                          upsert_timeseries_ds, replace_data, append_data, upsert_file_row_count,
+                          upsert_discovered_records, upsert_timeseries, upsert_tags, upsert_device_metadata, upsert_device_sensors_iq)
+
+from .helpers import (get_sites, get_tag_groups, get_metadata_keys, get_data, get_device_sensors, get_templates,
+                      get_units_of_measure, get_states_by_country, get_operation_state, get_equipment_classes,
+                      load_data, get_timezones, amortise_across_days, get_metadata_where_clause, connect_to_sql)
+
+__all__ = ['upsert_data', 'upsert_sites', 'upsert_workorders', 'upsert_device_sensors', 'upsert_device_sensors_ext',
+           'upsert_timeseries_ds', 'replace_data', 'append_data', 'get_sites', 'get_tag_groups', 'get_metadata_keys',
+           'get_data', 'get_device_sensors', 'get_templates', 'get_units_of_measure', 'get_states_by_country',
+           'get_operation_state', 'get_equipment_classes', 'load_data', 'upsert_file_row_count',
+           'upsert_discovered_records', 'upsert_timeseries', 'upsert_tags', 'upsert_device_metadata', 'get_timezones',
+           'amortise_across_days', 'get_metadata_where_clause', 'connect_to_sql', 'upsert_device_sensors_iq']
```

### Comparing `switch_api-0.5.4b2/switch_api/integration/_utils.py` & `switch_api-0.5.4b3/switch_api/integration/_utils.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,347 +1,347 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module containing the helper functions useful when integrating asset creation, asset updates, data ingestion, etc
-into the Switch Automation Platform.
-"""
-import json
-import pandas
-import requests
-import datetime
-import logging
-import sys
-import re
-from .._utils._constants import SUPPORT_PAYLOAD_TYPE  # , QUERY_LANGUAGE
-from .._utils._utils import ApiInputs, requests_retry_session, is_valid_uuid
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-def _timezones(api_inputs: ApiInputs):
-    """Get timezones
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-
-
-    """
-    # payload = {}
-    headers = api_inputs.api_headers.default
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    # upload Blobs to folder
-    url = f"{api_inputs.api_base_url}/timezones/all"
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text, orient='index')
-    df.rename(columns={0: 'TimezoneName'}, inplace=True)
-    df['TimezoneId'] = df.index
-
-    return df
-
-
-def _timezone_offsets(api_inputs: ApiInputs, date_from: datetime.date, date_to: datetime.date,
-                      installation_id_list: list):
-    """Get timezones
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    date_from : datetime.date
-        First (earliest) record
-    date_to : datetime.date
-        Last (latest) record
-    installation_id_list : list
-        List of InstallationIDs
-
-    Returns
-    -------
-    df : pandas.DataFrame
-        A dataframe containing the timezone offsets per InstallationID - if multiple timezone offsets occur for the
-        submitted period, there will be a row per offset date range. E.g. for a timezone with daylight savings
-
-    """
-    payload = {
-        "dateFrom": date_from.isoformat(),
-        "dateTo": date_to.isoformat(),
-        "installationIds": installation_id_list
-    }
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_base_url}/timezones/offsets"
-    logger.info("Sending request: POST %s", url)
-
-    try:
-        response = requests_retry_session(method_whitelist=['POST']).post(url, json=payload, headers=headers)
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        response_data_frame = pandas.read_json(response.text)
-        return response_data_frame
-    except Exception as ex:
-        logger.error("API Call was not successful.", ex)
-        return pandas.DataFrame()
-
-
-def _timezone_dst_offsets(api_inputs: ApiInputs, date_from: datetime.date, date_to: datetime.date,
-                          installation_id_list: list = None, timezone_name: str = None):
-    """Get timezones
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    date_from : datetime.date
-        First (earliest) record
-    date_to : datetime.date
-        Last (latest) record
-    installation_id_list : list
-        List of InstallationIDs
-    timezone_name: str
-        Specific timezone name to retrieve DST offsets.
-        Defaults to None.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-        A dataframe containing the timezone offsets per InstallationID - if multiple timezone offsets occur for the
-        submitted period, there will be a row per offset date range. E.g. for a timezone with daylight savings
-
-    """
-    payload = {
-        "dateFrom": date_from.isoformat(),
-        "dateTo": date_to.isoformat(),
-        "installationIds": installation_id_list,
-        "timezoneName": timezone_name
-    }
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if timezone_name == '':
-        logger.error("timezone_name parameter cannot be an empty string.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_base_url}/timezones/dst-transitions"
-    if not timezone_name is None:
-        url = f"{api_inputs.api_base_url}/timezones/name/dst-transitions"
-
-    logger.info("Sending request: POST %s", url)
-
-    try:
-        response = requests_retry_session(method_whitelist=['POST']).post(url, json=payload, headers=headers)
-
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return pandas.DataFrame()
-
-        timezone_intervals = json.loads(response.text)
-
-        if not timezone_intervals or ('dstTransitions' in timezone_intervals and len(timezone_intervals['dstTransitions']) == 0):
-            logger.error('No Timezone DST Intervals returned for this API call. %s', response.request.url)
-            return pandas.DataFrame()
-
-        site_timezones = pandas.DataFrame.from_dict(timezone_intervals['installationTimezoneList'])
-        timezone_offsets = pandas.DataFrame.from_dict(timezone_intervals['dstTransitions'])
-        timezone_offsets = timezone_offsets.explode('dstIntervals')
-        timezone_offsets = timezone_offsets.reset_index(drop=True).join(timezone_offsets.reset_index(drop=True)
-                                                                        .dstIntervals.apply(pandas.Series)).drop(columns=['dstIntervals', 'timezoneName'])
-        if timezone_name is None:
-            return site_timezones.merge(timezone_offsets, on='timezoneId')
-        else:
-            return timezone_offsets
-    except Exception as ex:
-        logger.error("API Call was not successful.", ex)
-        return pandas.DataFrame()
-
-def _upsert_entities_affected_count(api_inputs: ApiInputs, entities_affected_count: int):
-    """Updates data feed and data feed file status entities affected count.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    entities_affected_count : int
-        Count of affected entities - i.e. how many records are being upserted, replaced or appended.
-
-    Returns
-    -------
-
-    """
-
-    if entities_affected_count is None:
-        entities_affected_count = 0
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if not is_valid_uuid(api_inputs.data_feed_id):
-        logger.error("Entities Affected Count can only be upserted when called in Production.")
-        return False
-
-    if not is_valid_uuid(api_inputs.data_feed_file_status_id):
-        logger.error("Entities Affected Count can only be upserted when called in Production.")
-        return False
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
-          f"{api_inputs.data_feed_id}/file-status/{api_inputs.data_feed_file_status_id}/entities-affected/" \
-          f"{entities_affected_count}"
-
-    response = requests.request("PUT", url, timeout=20, headers=headers)
-
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-    elif response.status_code == 200:
-        logger.info("Entities Affected Count successfully upserted. ")
-
-
-def _adx_support(api_inputs: ApiInputs, payload_type: SUPPORT_PAYLOAD_TYPE):
-    """
-        Call ADX Support endpoint to trigger SQL->ADX sync
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    payload_type : SUPPORT_PAYLOAD_TYPE
-        Payload Type to sync ADX
-
-    Returns
-    -------
-
-
-    """
-    headers = api_inputs.api_headers.default
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if payload_type == None:
-        logger.error("You must provide the ADX Support Payload Type to SYNC ADX.")
-        return pandas.DataFrame()
-
-    if not set([payload_type]).issubset(set(SUPPORT_PAYLOAD_TYPE.__args__)):
-        logger.error('payload_type parameter must be set to one of the allowed values defined by the '
-                     'SUPPORT_PAYLOAD_TYPE literal: %s', SUPPORT_PAYLOAD_TYPE.__args__)
-        return pandas.DataFrame()
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/sync/{payload_type}"
-
-    response = requests.request("PUT", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    logger.info("API Call successful.")
-
-    return response_status, response.text
-
-
-def _extract_sql_credentials(cs: str) -> tuple:
-    """
-        Extract SQL Credentials from string
-
-        Parameters
-        ----------
-        cs : str
-            String containing sql credentials to be parsed.
-
-        Returns
-        -------
-        sql_server_name, sql_server_user_name, sql_server_password : tuple
-            Tuple containing SQL Server Name, UserName & Pwd
-
-        """
-    # Future: More elegant extraction.
-    try:
-        sql_server_name = re.search(r'Data Source=([a-zA-Z0-9.]+)\;', cs).group().replace('Data Source=', '')
-        sql_server_user_name = re.search(r'User ID=([a-zA-Z0-9.]+)\;', cs).group().replace('User ID=', '')
-        sql_server_password = re.search(r'Password=([a-zA-Z0-9@$#%^&*().]+)\;', cs).group().replace('Password=', '')
-    except AttributeError:
-        sql_server_name = None
-        sql_server_user_name = None
-        sql_server_password = None
-
-    return sql_server_name, sql_server_user_name, sql_server_password
-
-
-def _get_sql_connection_string(api_inputs: ApiInputs) -> str:
-    """
-        Get SQL Connection String
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-
-        Returns
-        -------
-        connection_string: str
-            String containing sql connection string.
-
-        """
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/sql-read-connection"
-    headers = api_inputs.api_headers.default
-    response = requests.get(url, headers=headers)
-    connection_string = response.text
-    return connection_string
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module containing the helper functions useful when integrating asset creation, asset updates, data ingestion, etc
+into the Switch Automation Platform.
+"""
+import json
+import pandas
+import requests
+import datetime
+import logging
+import sys
+import re
+from .._utils._constants import SUPPORT_PAYLOAD_TYPE  # , QUERY_LANGUAGE
+from .._utils._utils import ApiInputs, requests_retry_session, is_valid_uuid
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+def _timezones(api_inputs: ApiInputs):
+    """Get timezones
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+
+
+    """
+    # payload = {}
+    headers = api_inputs.api_headers.default
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    # upload Blobs to folder
+    url = f"{api_inputs.api_base_url}/timezones/all"
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text, orient='index')
+    df.rename(columns={0: 'TimezoneName'}, inplace=True)
+    df['TimezoneId'] = df.index
+
+    return df
+
+
+def _timezone_offsets(api_inputs: ApiInputs, date_from: datetime.date, date_to: datetime.date,
+                      installation_id_list: list):
+    """Get timezones
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    date_from : datetime.date
+        First (earliest) record
+    date_to : datetime.date
+        Last (latest) record
+    installation_id_list : list
+        List of InstallationIDs
+
+    Returns
+    -------
+    df : pandas.DataFrame
+        A dataframe containing the timezone offsets per InstallationID - if multiple timezone offsets occur for the
+        submitted period, there will be a row per offset date range. E.g. for a timezone with daylight savings
+
+    """
+    payload = {
+        "dateFrom": date_from.isoformat(),
+        "dateTo": date_to.isoformat(),
+        "installationIds": installation_id_list
+    }
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_base_url}/timezones/offsets"
+    logger.info("Sending request: POST %s", url)
+
+    try:
+        response = requests_retry_session(method_whitelist=['POST']).post(url, json=payload, headers=headers)
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        response_data_frame = pandas.read_json(response.text)
+        return response_data_frame
+    except Exception as ex:
+        logger.error("API Call was not successful.", ex)
+        return pandas.DataFrame()
+
+
+def _timezone_dst_offsets(api_inputs: ApiInputs, date_from: datetime.date, date_to: datetime.date,
+                          installation_id_list: list = None, timezone_name: str = None):
+    """Get timezones
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    date_from : datetime.date
+        First (earliest) record
+    date_to : datetime.date
+        Last (latest) record
+    installation_id_list : list
+        List of InstallationIDs
+    timezone_name: str
+        Specific timezone name to retrieve DST offsets.
+        Defaults to None.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+        A dataframe containing the timezone offsets per InstallationID - if multiple timezone offsets occur for the
+        submitted period, there will be a row per offset date range. E.g. for a timezone with daylight savings
+
+    """
+    payload = {
+        "dateFrom": date_from.isoformat(),
+        "dateTo": date_to.isoformat(),
+        "installationIds": installation_id_list,
+        "timezoneName": timezone_name
+    }
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if timezone_name == '':
+        logger.error("timezone_name parameter cannot be an empty string.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_base_url}/timezones/dst-transitions"
+    if not timezone_name is None:
+        url = f"{api_inputs.api_base_url}/timezones/name/dst-transitions"
+
+    logger.info("Sending request: POST %s", url)
+
+    try:
+        response = requests_retry_session(method_whitelist=['POST']).post(url, json=payload, headers=headers)
+
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return pandas.DataFrame()
+
+        timezone_intervals = json.loads(response.text)
+
+        if not timezone_intervals or ('dstTransitions' in timezone_intervals and len(timezone_intervals['dstTransitions']) == 0):
+            logger.error('No Timezone DST Intervals returned for this API call. %s', response.request.url)
+            return pandas.DataFrame()
+
+        site_timezones = pandas.DataFrame.from_dict(timezone_intervals['installationTimezoneList'])
+        timezone_offsets = pandas.DataFrame.from_dict(timezone_intervals['dstTransitions'])
+        timezone_offsets = timezone_offsets.explode('dstIntervals')
+        timezone_offsets = timezone_offsets.reset_index(drop=True).join(timezone_offsets.reset_index(drop=True)
+                                                                        .dstIntervals.apply(pandas.Series)).drop(columns=['dstIntervals', 'timezoneName'])
+        if timezone_name is None:
+            return site_timezones.merge(timezone_offsets, on='timezoneId')
+        else:
+            return timezone_offsets
+    except Exception as ex:
+        logger.error("API Call was not successful.", ex)
+        return pandas.DataFrame()
+
+def _upsert_entities_affected_count(api_inputs: ApiInputs, entities_affected_count: int):
+    """Updates data feed and data feed file status entities affected count.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    entities_affected_count : int
+        Count of affected entities - i.e. how many records are being upserted, replaced or appended.
+
+    Returns
+    -------
+
+    """
+
+    if entities_affected_count is None:
+        entities_affected_count = 0
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if not is_valid_uuid(api_inputs.data_feed_id):
+        logger.error("Entities Affected Count can only be upserted when called in Production.")
+        return False
+
+    if not is_valid_uuid(api_inputs.data_feed_file_status_id):
+        logger.error("Entities Affected Count can only be upserted when called in Production.")
+        return False
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
+          f"{api_inputs.data_feed_id}/file-status/{api_inputs.data_feed_file_status_id}/entities-affected/" \
+          f"{entities_affected_count}"
+
+    response = requests.request("PUT", url, timeout=20, headers=headers)
+
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+    elif response.status_code == 200:
+        logger.info("Entities Affected Count successfully upserted. ")
+
+
+def _adx_support(api_inputs: ApiInputs, payload_type: SUPPORT_PAYLOAD_TYPE):
+    """
+        Call ADX Support endpoint to trigger SQL->ADX sync
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    payload_type : SUPPORT_PAYLOAD_TYPE
+        Payload Type to sync ADX
+
+    Returns
+    -------
+
+
+    """
+    headers = api_inputs.api_headers.default
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if payload_type == None:
+        logger.error("You must provide the ADX Support Payload Type to SYNC ADX.")
+        return pandas.DataFrame()
+
+    if not set([payload_type]).issubset(set(SUPPORT_PAYLOAD_TYPE.__args__)):
+        logger.error('payload_type parameter must be set to one of the allowed values defined by the '
+                     'SUPPORT_PAYLOAD_TYPE literal: %s', SUPPORT_PAYLOAD_TYPE.__args__)
+        return pandas.DataFrame()
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/sync/{payload_type}"
+
+    response = requests.request("PUT", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    logger.info("API Call successful.")
+
+    return response_status, response.text
+
+
+def _extract_sql_credentials(cs: str) -> tuple:
+    """
+        Extract SQL Credentials from string
+
+        Parameters
+        ----------
+        cs : str
+            String containing sql credentials to be parsed.
+
+        Returns
+        -------
+        sql_server_name, sql_server_user_name, sql_server_password : tuple
+            Tuple containing SQL Server Name, UserName & Pwd
+
+        """
+    # Future: More elegant extraction.
+    try:
+        sql_server_name = re.search(r'Data Source=([a-zA-Z0-9.]+)\;', cs).group().replace('Data Source=', '')
+        sql_server_user_name = re.search(r'User ID=([a-zA-Z0-9.]+)\;', cs).group().replace('User ID=', '')
+        sql_server_password = re.search(r'Password=([a-zA-Z0-9@$#%^&*().]+)\;', cs).group().replace('Password=', '')
+    except AttributeError:
+        sql_server_name = None
+        sql_server_user_name = None
+        sql_server_password = None
+
+    return sql_server_name, sql_server_user_name, sql_server_password
+
+
+def _get_sql_connection_string(api_inputs: ApiInputs) -> str:
+    """
+        Get SQL Connection String
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+
+        Returns
+        -------
+        connection_string: str
+            String containing sql connection string.
+
+        """
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/sql-read-connection"
+    headers = api_inputs.api_headers.default
+    response = requests.get(url, headers=headers)
+    connection_string = response.text
+    return connection_string
```

### Comparing `switch_api-0.5.4b2/switch_api/integration/helpers.py` & `switch_api-0.5.4b3/switch_api/integration/helpers.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,960 +1,960 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module containing the helper functions useful when integrating asset creation, asset updates, data ingestion, etc
-into the Switch Automation Platform.
-"""
-import json
-import sys
-import pandas
-import pandas as pd
-import requests
-import logging
-import uuid
-import pyodbc
-from typing import Union
-from ._utils import _get_sql_connection_string, _extract_sql_credentials
-from .._utils._constants import QUERY_LANGUAGE
-from .._utils._constants import RESPONSE_TYPE, AMORTISATION_METHOD
-from .._utils._utils import ApiInputs, _column_name_cap, requests_retry_session, requests_retry_session2
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-def get_operation_state(upload_id: uuid.UUID, api_inputs: ApiInputs):
-    """Get operation state
-
-    Parameters
-    ----------
-    upload_id: uuid.UUID
-        uploadId returned from the Data Operation
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-
-
-    """
-    # payload = {}
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    # upload Blobs to folder
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/operation-state?operationId={upload_id}"
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    df = df.drop(columns=['NodeId', 'RootActivityId', 'Principal', 'User', 'Database'])
-    return df
-
-
-def load_data(dev_mode_path, api_inputs: ApiInputs):
-    """Load data
-
-    Parameters
-    ----------
-    dev_mode_path :
-
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    pandas.DataFrame
-
-    """
-    if api_inputs.api_projects_endpoint == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    df = pandas.read_csv(dev_mode_path)
-
-    return df
-
-
-def data_table_exists(table_name: str, api_inputs: ApiInputs):
-    """Validate if data table exists.
-
-    Parameters
-    ----------
-    table_name: str :
-        Table name to validate.
-    api_inputs: ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    bool
-        True if the datatable exists. False if the table does not exist.
-
-    """
-    # payload = {}
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/table?name={table_name}"
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status
-
-    if response.text == 'true':
-        logger.info("Response status: %s", response_status)
-        logger.info("Data table '%s' exists.", table_name)
-        return True
-    else:
-        logger.info("Response status: %s", response_status)
-        logger.info("Data table '%s' does not exist.", table_name)
-        return False
-
-
-def get_sites(api_inputs: ApiInputs, include_tag_groups: Union[list, bool] = False,
-              sql_where_clause: str = None, top_count: int = None, include_removed_sites: bool = False, retries: int=0,
-              backoff_factor: Union[int, float] = 0.3):
-    """Retrieve site information.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    include_tag_groups : Union[list, bool], default = False
-        If False, no tag groups are included. If True, all tag groups will be returned. Else, if list, the Tag Groups
-        in the list are retrieved as columns.
-    sql_where_clause : str, default = None
-        Optional `WHERE` clause in SQL syntax. Use field names only and do not include the "WHERE".
-    top_count: int, default = None
-        For use during testing to limit the number of records returned.
-    include_removed_sites: bool, default = False
-        Whether or not to include sites marked as "IsRemoved" in the returned dataframe. If True, removed sites are
-        included. If False, they are not. Defaults to False.
-    retries :0 < int < 10
-        Number of retries performed before returning last retry instance's response status. Max retries = 10.
-        Defaults to 0.
-    backoff_factor : Union[int, float]
-        If retries > 0, a backoff factor to apply between attempts after the second try (most errors are resolved
-        immediately by a second try without a delay).
-        {backoff factor} * (2 ** ({retry instance} - 1)) seconds
-        Defaults to 0.3
-
-
-    Returns
-    -------
-    df : pandas.DataFrame
-
-    """
-
-    if top_count is None:
-        top_count = 0
-
-    tags_mode = False
-    tag_groups = []
-    if type(include_tag_groups) is list:
-        tags_mode = True
-        tag_groups = include_tag_groups
-    elif type(include_tag_groups) is bool:
-        tag_groups = []
-        tags_mode = include_tag_groups
-
-    if sql_where_clause is not None:
-        if sql_where_clause.startswith('WHERE') or sql_where_clause.startswith('where'):
-            sql_where_clause = sql_where_clause.removeprefix('WHERE')
-            sql_where_clause = sql_where_clause.removeprefix('where')
-
-    payload = {
-        "tagsMode": tags_mode,
-        "includeTagColumns": tag_groups,
-        "sqlWhereClause": sql_where_clause,
-        "topCount": top_count,
-        "includeDeletedSites": include_removed_sites,
-    }
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/installations/sites-ingestion"
-    logger.info("Sending request: POST %s", url)
-
-    # response = requests.post(url, json=payload, headers=headers)
-    # response_status = '{} {}'.format(response.status_code, response.reason)
-    # if response.status_code != 200:
-    #     logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-    #                  response.reason)
-    #     return response_status
-    # elif len(response.text) == 0:
-    #     logger.error('No data returned for this API call. %s', response.request.url)
-    #     return response_status
-    #
-    # df = pandas.read_json(response.text, dtype={'InstallationCode': str})
-    #
-    # return df
-
-    try:
-        response = requests_retry_session2(
-            retries=retries, backoff_factor=backoff_factor,
-            status_forcelist=[408, 429, 500, 502, 503, 504]).post(url, json=payload, headers=headers)
-        retries = requests_retry_session2.retries
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-
-        if response.status_code != 200:
-            logger.error(f"API Call was not successful. Max retries reached: {retries}. Response Status: "
-                         f"{response.status_code}. Reason: {response.reason}.")
-            return response_status
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status
-
-        df = pandas.read_json(response.text, dtype={'InstallationCode': str})
-        return df
-    except Exception as ex:
-        logger.error(f"API Call was not successful. {str(ex)}")
-        return pandas.DataFrame()
-
-
-def get_device_sensors(api_inputs: ApiInputs, include_tag_groups: Union[list, bool] = False,
-                       include_metadata_keys: Union[list, bool] = False, sql_where_clause: str = None,
-                       top_count: int = None, retries: int=0, backoff_factor: Union[int, float] = 0.3):
-    """Retrieve device and sensor information.
-
-    Optionally include all or a subset of tag groups and/or metadata keys depending on the configuration of the
-    `include_tag_groups` and `include_metadata_keys` parameters. Whilst testing, there is the option to limit the number
-    of records returned via the `top_count` parameter. If this parameter is not set, then the function will return all
-    records.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    include_tag_groups : Union[list, bool], default = False
-        If False, no tag groups are included. If True, all tag groups will be returned. Else, if list, the Tag Groups
-        in the list are retrieved as columns.
-    include_metadata_keys : Union[list, bool], default = False
-        If False, no metadata keys are included. If True, all metadata keys will be returned. Else, if list,
-        the metadata keys in the list are retrieved as columns.
-    sql_where_clause : str, optional
-        optional `WHERE` clause in SQL syntax.
-    top_count: int, default = None
-        For use during testing to limit the number of records returned.
-    retries : int
-        Number of retries performed before returning last retry instance's response status.  Max retries = 10.
-        Defaults to 5.
-    backoff_factor : Union[int, float]
-        If retries > 0, a backoff factor to apply between attempts after the second try (most errors are resolved
-        immediately by a second try without a delay).
-        {backoff factor} * (2 ** ({number of total retries} - 1)) seconds
-        Defaults to 0.3
-
-    Returns
-    -------
-    df : pandas.DataFrame
-
-    """
-    if (include_tag_groups is True or type(include_tag_groups) == list) and \
-            (include_metadata_keys is True or type(include_metadata_keys) == list):
-        logger.exception('Tags and Metadata cannot be returned in a single call. Please set either include_tag_groups '
-                         'or include_metadata_keys to False. ')
-        return
-
-    if type(include_metadata_keys)==list:
-        portfolio_metadata_keys = get_metadata_keys(api_inputs=api_inputs)
-        if set(include_metadata_keys).issubset(portfolio_metadata_keys['MetadataKey'].values):
-            logger.info('The required Metadata Keys exist in this portfolio. ')
-        else:
-            logger.exception('The required Metadata Keys do not exist in this portfolio. Unable to retrieve devices and '
-                         'sensors. ')
-            return pd.DataFrame()
-
-    if type(include_tag_groups)==list:
-        portfolio_tag_groups = get_tag_groups(api_inputs=api_inputs)
-        if set(include_tag_groups).issubset(portfolio_tag_groups['TagGroup'].values):
-            logger.info('The required Tag Groups exist in this portfolio. ')
-        else:
-            logger.exception('The required Tag Groups do not exist in this portfolio. Unable to retrieve devices and '
-                         'sensors. ')
-            return pd.DataFrame()
-
-
-    limit = 50000
-    offset = 0
-    is_finished = False
-
-    if top_count is None:
-        top_count = 0
-
-    if top_count > 0:
-        limit = 0
-
-    tags_mode = False
-    tag_groups = []
-    if type(include_tag_groups) is list:
-        tags_mode = True
-        tag_groups = include_tag_groups
-    elif type(include_tag_groups) is bool:
-        tag_groups = []
-        tags_mode = include_tag_groups
-
-    metadata_mode = False
-    metadata_keys = []
-    if type(include_metadata_keys) is list:
-        metadata_mode = True
-        metadata_keys = include_metadata_keys
-    elif type(include_metadata_keys) is bool:
-        metadata_keys = []
-        metadata_mode = include_metadata_keys
-
-    payload = {
-        "tagsMode": tags_mode,
-        "metadataMode": metadata_mode,
-        "includeTagColumns": tag_groups,
-        "includeMetadataColumns": metadata_keys,
-        "sqlWhereClause": sql_where_clause,
-        "topCount": top_count,
-        "limit": limit,
-        "offset": offset
-    }
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/device-sensors"
-    logger.info("Sending request: POST %s", url)
-
-    df_master = pandas.DataFrame()
-    # holder of paginated dataframes
-    df_data_chunks = []
-    # Loop through pagination until encountered error message or empty Next URL
-    while is_finished == False:
-        df_data = ''
-        # df_message = ''
-
-        try:
-            response = requests_retry_session2(
-                retries=retries, backoff_factor=backoff_factor,
-                status_forcelist=[408, 429, 500, 502, 503, 504]).post(url, json=payload, headers=headers)
-            retries = requests_retry_session2.retries
-
-            # response = requests.post(url, json=payload, headers=headers)
-            response_status = '{} {}'.format(response.status_code, response.reason)
-
-            if response.status_code != 200:
-                logger.error(f"API Call was not successful. Max retries reached: {retries}. Response Status: "
-                             f"{response.status_code}. Reason: {response.reason}.")
-                return response_status
-            elif len(response.text) == 0:
-                logger.error('No data returned for this API call. %s', response.request.url)
-                return response_status
-
-            response_dict = json.loads(response.text)
-
-            if 'Message' in response_dict.keys() and len(response_dict['Message']) != 0 and response_dict['Message'] != '':
-                logger.error(response_dict['Message'])
-                is_finished = True
-                return response_status, response_dict['Message']
-
-            if 'Data' in response_dict.keys() and len(response_dict['Data'])==0 and len(df_data_chunks)==0:
-                logger.error(f'No data returned for this API call. {response.request.url}')
-                is_finished = True
-                return pandas.DataFrame()
-            elif 'Data' in response_dict.keys() and len(response_dict['Data']) != 0:
-                df_data = pandas.DataFrame(response_dict['Data'])
-                df_data_chunks.append(df_data)
-
-            if 'NextUrl' in response_dict.keys() and len(response_dict['NextUrl']) != 0:
-                payload = json.loads(response_dict['NextUrl'])
-            else:
-                is_finished = True
-                df_master = pandas.concat(df_data_chunks, ignore_index=True)
-                df_master.rename(columns={"InstallationID": "InstallationId", "ObjectPropertyID": "ObjectPropertyId"},
-                                 inplace=True)
-                return df_master
-        except Exception as ex:
-            logger.error(f"API Call was not successful. {str(ex)}")
-            is_finished = True
-            return pandas.DataFrame()
-
-    return df_master
-
-
-def get_data(query_text, api_inputs: ApiInputs, query_language: QUERY_LANGUAGE = "kql",
-             response_type: RESPONSE_TYPE = 'dataframe', retries: int = 0, backoff_factor: Union[int, float] = 2):
-    """Retrieve data.
-
-    Parameters
-    ----------
-    query_text : str
-        SQL statement used to retrieve data.
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    query_language : QUERY_LANGUAGE, optional
-        The query language the query_text is written in (Default value = 'sql').
-    response_type : RESPONSE_TYPE, optional
-        The type of the response to be provided - either dataframe or json
-    retries :0 < int < 10
-        Number of retries performed before returning last retry instance's response status. Max retries = 10.
-        Defaults to 5.
-    backoff_factor : Union[int, float]
-        If retries > 0, a backoff factor to apply between attempts after the second try (most errors are resolved
-        immediately by a second try without a delay).
-        {backoff factor} * (2 ** ({number of total retries} - 1)) seconds
-        Defaults to 2
-
-    Returns
-    -------
-    df : pandas.DataFrame
-
-    """
-    payload = {
-        "queryText": query_text,
-        "queryLanguage": str(query_language)
-    }
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    if not set([query_language]).issubset(set(QUERY_LANGUAGE.__args__)):
-        logger.error('query_language parameter must be set to one of the allowed values defined by the '
-                     'QUERY_LANGUAGE literal: %s', QUERY_LANGUAGE.__args__)
-        return False
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data"
-    logger.info("Sending request: POST %s", url)
-
-    # response = requests.post(url, json=payload, headers=headers)
-    # response_status = '{} {}'.format(response.status_code, response.reason)
-    # if response.status_code != 200:
-    #     logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-    #                  response.reason)
-    #     return response_status
-    # elif len(response.text) == 0:
-    #     logger.error('No data returned for this API call. %s', response.request.url)
-    #     return response_status
-    #
-    # if response_type == 'dataframe':
-    #     return pandas.read_json(response.text)
-    # elif response_type == 'json':
-    #     return json.loads(response.text)
-    # else:
-    #     return response.text
-    try:
-        response = requests_retry_session2(
-            retries=retries, backoff_factor=backoff_factor,
-            status_forcelist=[408, 429, 500, 502, 503, 504]).post(url, json=payload, headers=headers)
-        retries=requests_retry_session2.retries
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error(f"API Call was not successful. Max retries reached: {retries}. Response Status: {response.status_code}. Reason: {response.reason}.")
-            return response_status
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status
-
-        if response_type == 'dataframe':
-            response_data_frame = pandas.read_json(response.text)
-            return pandas.read_json(response.text)
-        elif response_type == 'json':
-            return json.loads(response.text)
-        else:
-            return response.text
-    except Exception as ex:
-        logger.error(f"API Call was not successful. {str(ex)}")
-        return pandas.DataFrame()
-
-
-def get_states_by_country(api_inputs: ApiInputs, country: str):
-    """Get list of States for selected country.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        country: str
-            Country to lookup states for.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Data frame containing the states for the given country.
-
-        """
-    # payload = {}
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_base_url}/country/{country}/states"
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    df.columns = _column_name_cap(df.columns)
-
-    return df
-
-
-def get_templates(api_inputs: ApiInputs, object_property_type: str = None):
-    """Get list of Templates by Type.
-
-    Also retrieves the default unit of measure for the given template.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    object_property_type : str, Optional
-        The object property type to filter to.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-        Data frame containing the templates by type including the default unit of measure.
-
-    """
-    # payload = {}
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = ''
-    if object_property_type is None:
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/templates"
-    elif object_property_type is not None:
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/templates?type=" \
-              f"{object_property_type}"
-
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    df.columns = _column_name_cap(df.columns)
-
-    return df
-
-
-def get_tag_groups(api_inputs: ApiInputs):
-    """Get the sensor-level tag groups present for a portfolio.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-        Data frame containing the tag groups present in the given portfolio.
-
-    """
-    # payload = {}
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/tags/tag-groups"
-
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    df.columns = ['TagGroup']
-
-    return df
-
-
-def get_metadata_keys(api_inputs: ApiInputs):
-    """Get the device-level metadata keys for a portfolio.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-        Data frame containing the metadata keys present in the portfolio.
-
-    """
-    # payload = {}
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/metadata/all-type"
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    if df.shape[1] == 0:
-        df = pd.DataFrame({'MetadataKey':[]})
-    else:
-        df.columns = ['MetadataKey']
-
-    return df
-
-
-def get_units_of_measure(api_inputs: ApiInputs, object_property_type: str = None):
-    """Get list of units of measure by type.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    object_property_type : str, Optional
-        The ObjectPropertyType to filter on.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-        Data frame containing the units of measure by type.
-
-    """
-    # payload = {}
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = ''
-
-    if object_property_type is None:
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/units"
-    elif object_property_type is not None:
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/units?type=" \
-              f"{object_property_type}"
-
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    df.columns = _column_name_cap(df.columns)
-
-    return df
-
-
-def get_equipment_classes(api_inputs: ApiInputs):
-    """Get list of Equipment Classes.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-        Data frame containing the equipment classes.
-
-    """
-    # payload = {}
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/installation/" \
-          f"00000000-0000-0000-0000-000000000000/equipment/integration-classes"
-
-    logger.info("Sending request: GET %s", url)
-
-    if api_inputs.api_projects_endpoint == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    df.columns = ['EquipmentClass']
-
-    return df
-
-
-def get_timezones(api_inputs: ApiInputs):
-    """Get timezones
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    df : pandas.DataFrame
-    """
-    headers = api_inputs.api_headers.default
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    url = f"{api_inputs.api_base_url}/timezones/all"
-    logger.info("Sending request: GET %s", url)
-
-    response = requests.request("GET", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    df = pandas.read_json(response.text)
-    df.columns = _column_name_cap(df.columns)
-
-    return df
-
-
-def connect_to_sql(api_inputs: ApiInputs):
-    """Create a pyodbc connection to SQL
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-
-    Returns
-    -------
-    Union[pyodbc.Connection,bool]
-        If successful, returns a pyodbc.Connection object for the SQL database. If unsuccessful, return False.
-
-    """
-    cs = _get_sql_connection_string(api_inputs=api_inputs)
-    sql_server_name, sql_server_user_name, sql_server_password = _extract_sql_credentials(cs)
-
-    if set([sql_server_name, sql_server_user_name, sql_server_password]).issubset(set([None])):
-        logger.error('Failed to create SQL connection object: Unable to retrieve SQL credentials via API. ')
-        return False
-
-    conn = pyodbc.connect(
-        f'Driver={{ODBC Driver 17 for SQL Server}};SERVER={sql_server_name}'
-        f';DATABASE=Switch;UID={sql_server_user_name};PWD={sql_server_password};ApplicationIntent=ReadOnly',
-        readonly=True)
-
-    return conn
-
-# def get_portfolios(api_inputs: ApiInputs, search_term: str = None):
-#     """Get list of units of measure by type.
-#
-#     Parameters
-#     ----------
-#     api_inputs : ApiInputs
-#         Object returned by initialize() function.
-#     search_term : str
-#         The search_term used to filter list of portfolios to be retrieved.
-#
-#     Returns
-#     -------
-#     df : pandas.DataFrame
-#         A list of portfolios.
-#
-#     """
-#     payload = {}
-#     headers = {
-#         'x-functions-key': api_inputs.api_key,
-#         'Content-Type': 'application/json; charset=utf-8',
-#         'user-key': api_inputs.user_id
-#     }
-#
-#     if api_inputs.datacentre == '' or api_inputs.api_key == '':
-#         logger.error("You must call initialize() before using API.")
-#         return pandas.DataFrame()
-#
-#     if search_term is None:
-#         search_term = '*'
-#
-#     url = api_prefix + "Portfolios/" + search_term
-#     response = requests.request("GET", url, timeout=20, headers=headers)
-#     response_status = '{} {}'.format(response.status_code, response.reason)
-#     if response.status_code != 200:
-#         logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-#                      response.reason)
-#         return response_status, pandas.DataFrame()
-#     elif len(response.text) == 0:
-#         logger.error('No data returned for this API call. %s', response.request.url)
-#         return response_status, pandas.DataFrame()
-#
-#     df = pandas.read_json(response.text)
-#
-#     return df
-
-
-def amortise_across_days(raw_df: pandas.DataFrame, start_date_col:str, end_date_col: str, value_col: str,
-                         amortise_method: AMORTISATION_METHOD = "Exclusive"):
-    """Amortise data across days in period.
-
-    The period is defined by the `start_date_col` and `end_date_col`.
-
-    Parameters
-    ----------
-    raw_df : pandas.DataFrame
-        The raw dataframe to be amortised.
-    start_date_col : str
-        The name of the column containing the start date of the period
-    end_date_col : str
-        The name of the column containing the end date of the period
-    value_col : str
-        The name of the column containing the numeric data to be amortised across the period.
-    amortise_method : AMORTISATION_METHOD, optional
-        Whether the amortisation should be inclusive or exclusive of the end date.
-
-    Returns
-    -------
-    amortised_df : pandas.DataFrame
-        Returned dataframe will contain three additional columns: num_days, amortised_value, Timestamp
-    """
-
-    if not set([start_date_col, end_date_col, value_col]).issubset(raw_df.columns):
-        col_lookup = pandas.DataFrame({'ColumnType': ['start_date_col', 'end_date_col', 'value_col'],
-                                     'ColumnName': [start_date_col, end_date_col, value_col]})
-        logger.exception(f"The raw_df does not contain column(s) defined for the input parameter(s): "
-                         f"{col_lookup[col_lookup.ColumnName.isin(list(set([start_date_col, end_date_col, value_col]).difference(raw_df.columns)))].ColumnType.values.tolist()}")
-        return False
-
-    start = start_date_col
-    end = end_date_col
-
-    if amortise_method == "Exclusive":
-        raw_df['num_days'] = (raw_df[end] - raw_df[start]).dt.days
-        raw_df['amortised_value'] = raw_df[value_col] / raw_df['num_days']
-
-        def make_list(row):
-            # make list of dates between Start Date & End Date (exclusive of the End Date).
-            return pd.date_range(start=row[start], end=row[end], freq='D').to_list()[:-1]
-
-        raw_df['Timestamp'] = raw_df[[start, end]].apply(lambda x: make_list(x), axis=1)
-        explode_df = raw_df.explode(column='Timestamp')
-
-        return explode_df
-    elif amortise_method == "Inclusive":
-        raw_df['num_days'] = (raw_df[end] - raw_df[start]).dt.days + 1
-        raw_df['amortised_value'] = raw_df[value_col] / raw_df['num_days']
-
-        def make_list(row):
-            return pd.date_range(start=row[start], end=row[end], freq='D').to_list()
-
-        raw_df['Timestamp'] = raw_df[[start, end]].apply(lambda x: make_list(x), axis=1)
-        explode_df = raw_df.explode(column='Timestamp')
-
-        return explode_df
-
-
-def get_metadata_where_clause(meta_columns:list):
-    sql_where_clause = ''.join(
-        ['([' + col + '] IS NOT NULL) AND '
-         if (col != meta_columns[-1])
-         else '([' + col + '] IS NOT NULL)' for col in
-         meta_columns])
-
-    return sql_where_clause
-
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module containing the helper functions useful when integrating asset creation, asset updates, data ingestion, etc
+into the Switch Automation Platform.
+"""
+import json
+import sys
+import pandas
+import pandas as pd
+import requests
+import logging
+import uuid
+import pyodbc
+from typing import Union
+from ._utils import _get_sql_connection_string, _extract_sql_credentials
+from .._utils._constants import QUERY_LANGUAGE
+from .._utils._constants import RESPONSE_TYPE, AMORTISATION_METHOD
+from .._utils._utils import ApiInputs, _column_name_cap, requests_retry_session, requests_retry_session2
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+def get_operation_state(upload_id: uuid.UUID, api_inputs: ApiInputs):
+    """Get operation state
+
+    Parameters
+    ----------
+    upload_id: uuid.UUID
+        uploadId returned from the Data Operation
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+
+
+    """
+    # payload = {}
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    # upload Blobs to folder
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/operation-state?operationId={upload_id}"
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    df = df.drop(columns=['NodeId', 'RootActivityId', 'Principal', 'User', 'Database'])
+    return df
+
+
+def load_data(dev_mode_path, api_inputs: ApiInputs):
+    """Load data
+
+    Parameters
+    ----------
+    dev_mode_path :
+
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    pandas.DataFrame
+
+    """
+    if api_inputs.api_projects_endpoint == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    df = pandas.read_csv(dev_mode_path)
+
+    return df
+
+
+def data_table_exists(table_name: str, api_inputs: ApiInputs):
+    """Validate if data table exists.
+
+    Parameters
+    ----------
+    table_name: str :
+        Table name to validate.
+    api_inputs: ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    bool
+        True if the datatable exists. False if the table does not exist.
+
+    """
+    # payload = {}
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/table?name={table_name}"
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status
+
+    if response.text == 'true':
+        logger.info("Response status: %s", response_status)
+        logger.info("Data table '%s' exists.", table_name)
+        return True
+    else:
+        logger.info("Response status: %s", response_status)
+        logger.info("Data table '%s' does not exist.", table_name)
+        return False
+
+
+def get_sites(api_inputs: ApiInputs, include_tag_groups: Union[list, bool] = False,
+              sql_where_clause: str = None, top_count: int = None, include_removed_sites: bool = False, retries: int=0,
+              backoff_factor: Union[int, float] = 0.3):
+    """Retrieve site information.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    include_tag_groups : Union[list, bool], default = False
+        If False, no tag groups are included. If True, all tag groups will be returned. Else, if list, the Tag Groups
+        in the list are retrieved as columns.
+    sql_where_clause : str, default = None
+        Optional `WHERE` clause in SQL syntax. Use field names only and do not include the "WHERE".
+    top_count: int, default = None
+        For use during testing to limit the number of records returned.
+    include_removed_sites: bool, default = False
+        Whether or not to include sites marked as "IsRemoved" in the returned dataframe. If True, removed sites are
+        included. If False, they are not. Defaults to False.
+    retries :0 < int < 10
+        Number of retries performed before returning last retry instance's response status. Max retries = 10.
+        Defaults to 0.
+    backoff_factor : Union[int, float]
+        If retries > 0, a backoff factor to apply between attempts after the second try (most errors are resolved
+        immediately by a second try without a delay).
+        {backoff factor} * (2 ** ({retry instance} - 1)) seconds
+        Defaults to 0.3
+
+
+    Returns
+    -------
+    df : pandas.DataFrame
+
+    """
+
+    if top_count is None:
+        top_count = 0
+
+    tags_mode = False
+    tag_groups = []
+    if type(include_tag_groups) is list:
+        tags_mode = True
+        tag_groups = include_tag_groups
+    elif type(include_tag_groups) is bool:
+        tag_groups = []
+        tags_mode = include_tag_groups
+
+    if sql_where_clause is not None:
+        if sql_where_clause.startswith('WHERE') or sql_where_clause.startswith('where'):
+            sql_where_clause = sql_where_clause.removeprefix('WHERE')
+            sql_where_clause = sql_where_clause.removeprefix('where')
+
+    payload = {
+        "tagsMode": tags_mode,
+        "includeTagColumns": tag_groups,
+        "sqlWhereClause": sql_where_clause,
+        "topCount": top_count,
+        "includeDeletedSites": include_removed_sites,
+    }
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/installations/sites-ingestion"
+    logger.info("Sending request: POST %s", url)
+
+    # response = requests.post(url, json=payload, headers=headers)
+    # response_status = '{} {}'.format(response.status_code, response.reason)
+    # if response.status_code != 200:
+    #     logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+    #                  response.reason)
+    #     return response_status
+    # elif len(response.text) == 0:
+    #     logger.error('No data returned for this API call. %s', response.request.url)
+    #     return response_status
+    #
+    # df = pandas.read_json(response.text, dtype={'InstallationCode': str})
+    #
+    # return df
+
+    try:
+        response = requests_retry_session2(
+            retries=retries, backoff_factor=backoff_factor,
+            status_forcelist=[408, 429, 500, 502, 503, 504]).post(url, json=payload, headers=headers)
+        retries = requests_retry_session2.retries
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+
+        if response.status_code != 200:
+            logger.error(f"API Call was not successful. Max retries reached: {retries}. Response Status: "
+                         f"{response.status_code}. Reason: {response.reason}.")
+            return response_status
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status
+
+        df = pandas.read_json(response.text, dtype={'InstallationCode': str})
+        return df
+    except Exception as ex:
+        logger.error(f"API Call was not successful. {str(ex)}")
+        return pandas.DataFrame()
+
+
+def get_device_sensors(api_inputs: ApiInputs, include_tag_groups: Union[list, bool] = False,
+                       include_metadata_keys: Union[list, bool] = False, sql_where_clause: str = None,
+                       top_count: int = None, retries: int=0, backoff_factor: Union[int, float] = 0.3):
+    """Retrieve device and sensor information.
+
+    Optionally include all or a subset of tag groups and/or metadata keys depending on the configuration of the
+    `include_tag_groups` and `include_metadata_keys` parameters. Whilst testing, there is the option to limit the number
+    of records returned via the `top_count` parameter. If this parameter is not set, then the function will return all
+    records.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    include_tag_groups : Union[list, bool], default = False
+        If False, no tag groups are included. If True, all tag groups will be returned. Else, if list, the Tag Groups
+        in the list are retrieved as columns.
+    include_metadata_keys : Union[list, bool], default = False
+        If False, no metadata keys are included. If True, all metadata keys will be returned. Else, if list,
+        the metadata keys in the list are retrieved as columns.
+    sql_where_clause : str, optional
+        optional `WHERE` clause in SQL syntax.
+    top_count: int, default = None
+        For use during testing to limit the number of records returned.
+    retries : int
+        Number of retries performed before returning last retry instance's response status.  Max retries = 10.
+        Defaults to 5.
+    backoff_factor : Union[int, float]
+        If retries > 0, a backoff factor to apply between attempts after the second try (most errors are resolved
+        immediately by a second try without a delay).
+        {backoff factor} * (2 ** ({number of total retries} - 1)) seconds
+        Defaults to 0.3
+
+    Returns
+    -------
+    df : pandas.DataFrame
+
+    """
+    if (include_tag_groups is True or type(include_tag_groups) == list) and \
+            (include_metadata_keys is True or type(include_metadata_keys) == list):
+        logger.exception('Tags and Metadata cannot be returned in a single call. Please set either include_tag_groups '
+                         'or include_metadata_keys to False. ')
+        return
+
+    if type(include_metadata_keys)==list:
+        portfolio_metadata_keys = get_metadata_keys(api_inputs=api_inputs)
+        if set(include_metadata_keys).issubset(portfolio_metadata_keys['MetadataKey'].values):
+            logger.info('The required Metadata Keys exist in this portfolio. ')
+        else:
+            logger.exception('The required Metadata Keys do not exist in this portfolio. Unable to retrieve devices and '
+                         'sensors. ')
+            return pd.DataFrame()
+
+    if type(include_tag_groups)==list:
+        portfolio_tag_groups = get_tag_groups(api_inputs=api_inputs)
+        if set(include_tag_groups).issubset(portfolio_tag_groups['TagGroup'].values):
+            logger.info('The required Tag Groups exist in this portfolio. ')
+        else:
+            logger.exception('The required Tag Groups do not exist in this portfolio. Unable to retrieve devices and '
+                         'sensors. ')
+            return pd.DataFrame()
+
+
+    limit = 50000
+    offset = 0
+    is_finished = False
+
+    if top_count is None:
+        top_count = 0
+
+    if top_count > 0:
+        limit = 0
+
+    tags_mode = False
+    tag_groups = []
+    if type(include_tag_groups) is list:
+        tags_mode = True
+        tag_groups = include_tag_groups
+    elif type(include_tag_groups) is bool:
+        tag_groups = []
+        tags_mode = include_tag_groups
+
+    metadata_mode = False
+    metadata_keys = []
+    if type(include_metadata_keys) is list:
+        metadata_mode = True
+        metadata_keys = include_metadata_keys
+    elif type(include_metadata_keys) is bool:
+        metadata_keys = []
+        metadata_mode = include_metadata_keys
+
+    payload = {
+        "tagsMode": tags_mode,
+        "metadataMode": metadata_mode,
+        "includeTagColumns": tag_groups,
+        "includeMetadataColumns": metadata_keys,
+        "sqlWhereClause": sql_where_clause,
+        "topCount": top_count,
+        "limit": limit,
+        "offset": offset
+    }
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/device-sensors"
+    logger.info("Sending request: POST %s", url)
+
+    df_master = pandas.DataFrame()
+    # holder of paginated dataframes
+    df_data_chunks = []
+    # Loop through pagination until encountered error message or empty Next URL
+    while is_finished == False:
+        df_data = ''
+        # df_message = ''
+
+        try:
+            response = requests_retry_session2(
+                retries=retries, backoff_factor=backoff_factor,
+                status_forcelist=[408, 429, 500, 502, 503, 504]).post(url, json=payload, headers=headers)
+            retries = requests_retry_session2.retries
+
+            # response = requests.post(url, json=payload, headers=headers)
+            response_status = '{} {}'.format(response.status_code, response.reason)
+
+            if response.status_code != 200:
+                logger.error(f"API Call was not successful. Max retries reached: {retries}. Response Status: "
+                             f"{response.status_code}. Reason: {response.reason}.")
+                return response_status
+            elif len(response.text) == 0:
+                logger.error('No data returned for this API call. %s', response.request.url)
+                return response_status
+
+            response_dict = json.loads(response.text)
+
+            if 'Message' in response_dict.keys() and len(response_dict['Message']) != 0 and response_dict['Message'] != '':
+                logger.error(response_dict['Message'])
+                is_finished = True
+                return response_status, response_dict['Message']
+
+            if 'Data' in response_dict.keys() and len(response_dict['Data'])==0 and len(df_data_chunks)==0:
+                logger.error(f'No data returned for this API call. {response.request.url}')
+                is_finished = True
+                return pandas.DataFrame()
+            elif 'Data' in response_dict.keys() and len(response_dict['Data']) != 0:
+                df_data = pandas.DataFrame(response_dict['Data'])
+                df_data_chunks.append(df_data)
+
+            if 'NextUrl' in response_dict.keys() and len(response_dict['NextUrl']) != 0:
+                payload = json.loads(response_dict['NextUrl'])
+            else:
+                is_finished = True
+                df_master = pandas.concat(df_data_chunks, ignore_index=True)
+                df_master.rename(columns={"InstallationID": "InstallationId", "ObjectPropertyID": "ObjectPropertyId"},
+                                 inplace=True)
+                return df_master
+        except Exception as ex:
+            logger.error(f"API Call was not successful. {str(ex)}")
+            is_finished = True
+            return pandas.DataFrame()
+
+    return df_master
+
+
+def get_data(query_text, api_inputs: ApiInputs, query_language: QUERY_LANGUAGE = "kql",
+             response_type: RESPONSE_TYPE = 'dataframe', retries: int = 0, backoff_factor: Union[int, float] = 2):
+    """Retrieve data.
+
+    Parameters
+    ----------
+    query_text : str
+        SQL statement used to retrieve data.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    query_language : QUERY_LANGUAGE, optional
+        The query language the query_text is written in (Default value = 'sql').
+    response_type : RESPONSE_TYPE, optional
+        The type of the response to be provided - either dataframe or json
+    retries :0 < int < 10
+        Number of retries performed before returning last retry instance's response status. Max retries = 10.
+        Defaults to 5.
+    backoff_factor : Union[int, float]
+        If retries > 0, a backoff factor to apply between attempts after the second try (most errors are resolved
+        immediately by a second try without a delay).
+        {backoff factor} * (2 ** ({number of total retries} - 1)) seconds
+        Defaults to 2
+
+    Returns
+    -------
+    df : pandas.DataFrame
+
+    """
+    payload = {
+        "queryText": query_text,
+        "queryLanguage": str(query_language)
+    }
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    if not set([query_language]).issubset(set(QUERY_LANGUAGE.__args__)):
+        logger.error('query_language parameter must be set to one of the allowed values defined by the '
+                     'QUERY_LANGUAGE literal: %s', QUERY_LANGUAGE.__args__)
+        return False
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data"
+    logger.info("Sending request: POST %s", url)
+
+    # response = requests.post(url, json=payload, headers=headers)
+    # response_status = '{} {}'.format(response.status_code, response.reason)
+    # if response.status_code != 200:
+    #     logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+    #                  response.reason)
+    #     return response_status
+    # elif len(response.text) == 0:
+    #     logger.error('No data returned for this API call. %s', response.request.url)
+    #     return response_status
+    #
+    # if response_type == 'dataframe':
+    #     return pandas.read_json(response.text)
+    # elif response_type == 'json':
+    #     return json.loads(response.text)
+    # else:
+    #     return response.text
+    try:
+        response = requests_retry_session2(
+            retries=retries, backoff_factor=backoff_factor,
+            status_forcelist=[408, 429, 500, 502, 503, 504]).post(url, json=payload, headers=headers)
+        retries=requests_retry_session2.retries
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error(f"API Call was not successful. Max retries reached: {retries}. Response Status: {response.status_code}. Reason: {response.reason}.")
+            return response_status
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status
+
+        if response_type == 'dataframe':
+            response_data_frame = pandas.read_json(response.text)
+            return pandas.read_json(response.text)
+        elif response_type == 'json':
+            return json.loads(response.text)
+        else:
+            return response.text
+    except Exception as ex:
+        logger.error(f"API Call was not successful. {str(ex)}")
+        return pandas.DataFrame()
+
+
+def get_states_by_country(api_inputs: ApiInputs, country: str):
+    """Get list of States for selected country.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        country: str
+            Country to lookup states for.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Data frame containing the states for the given country.
+
+        """
+    # payload = {}
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_base_url}/country/{country}/states"
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    df.columns = _column_name_cap(df.columns)
+
+    return df
+
+
+def get_templates(api_inputs: ApiInputs, object_property_type: str = None):
+    """Get list of Templates by Type.
+
+    Also retrieves the default unit of measure for the given template.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    object_property_type : str, Optional
+        The object property type to filter to.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+        Data frame containing the templates by type including the default unit of measure.
+
+    """
+    # payload = {}
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = ''
+    if object_property_type is None:
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/templates"
+    elif object_property_type is not None:
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/templates?type=" \
+              f"{object_property_type}"
+
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    df.columns = _column_name_cap(df.columns)
+
+    return df
+
+
+def get_tag_groups(api_inputs: ApiInputs):
+    """Get the sensor-level tag groups present for a portfolio.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+        Data frame containing the tag groups present in the given portfolio.
+
+    """
+    # payload = {}
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/tags/tag-groups"
+
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    df.columns = ['TagGroup']
+
+    return df
+
+
+def get_metadata_keys(api_inputs: ApiInputs):
+    """Get the device-level metadata keys for a portfolio.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+        Data frame containing the metadata keys present in the portfolio.
+
+    """
+    # payload = {}
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/metadata/all-type"
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    if df.shape[1] == 0:
+        df = pd.DataFrame({'MetadataKey':[]})
+    else:
+        df.columns = ['MetadataKey']
+
+    return df
+
+
+def get_units_of_measure(api_inputs: ApiInputs, object_property_type: str = None):
+    """Get list of units of measure by type.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    object_property_type : str, Optional
+        The ObjectPropertyType to filter on.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+        Data frame containing the units of measure by type.
+
+    """
+    # payload = {}
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = ''
+
+    if object_property_type is None:
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/units"
+    elif object_property_type is not None:
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/units?type=" \
+              f"{object_property_type}"
+
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    df.columns = _column_name_cap(df.columns)
+
+    return df
+
+
+def get_equipment_classes(api_inputs: ApiInputs):
+    """Get list of Equipment Classes.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+        Data frame containing the equipment classes.
+
+    """
+    # payload = {}
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/installation/" \
+          f"00000000-0000-0000-0000-000000000000/equipment/integration-classes"
+
+    logger.info("Sending request: GET %s", url)
+
+    if api_inputs.api_projects_endpoint == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    df.columns = ['EquipmentClass']
+
+    return df
+
+
+def get_timezones(api_inputs: ApiInputs):
+    """Get timezones
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    df : pandas.DataFrame
+    """
+    headers = api_inputs.api_headers.default
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    url = f"{api_inputs.api_base_url}/timezones/all"
+    logger.info("Sending request: GET %s", url)
+
+    response = requests.request("GET", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    df = pandas.read_json(response.text)
+    df.columns = _column_name_cap(df.columns)
+
+    return df
+
+
+def connect_to_sql(api_inputs: ApiInputs):
+    """Create a pyodbc connection to SQL
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    Union[pyodbc.Connection,bool]
+        If successful, returns a pyodbc.Connection object for the SQL database. If unsuccessful, return False.
+
+    """
+    cs = _get_sql_connection_string(api_inputs=api_inputs)
+    sql_server_name, sql_server_user_name, sql_server_password = _extract_sql_credentials(cs)
+
+    if set([sql_server_name, sql_server_user_name, sql_server_password]).issubset(set([None])):
+        logger.error('Failed to create SQL connection object: Unable to retrieve SQL credentials via API. ')
+        return False
+
+    conn = pyodbc.connect(
+        f'Driver={{ODBC Driver 17 for SQL Server}};SERVER={sql_server_name}'
+        f';DATABASE=Switch;UID={sql_server_user_name};PWD={sql_server_password};ApplicationIntent=ReadOnly',
+        readonly=True)
+
+    return conn
+
+# def get_portfolios(api_inputs: ApiInputs, search_term: str = None):
+#     """Get list of units of measure by type.
+#
+#     Parameters
+#     ----------
+#     api_inputs : ApiInputs
+#         Object returned by initialize() function.
+#     search_term : str
+#         The search_term used to filter list of portfolios to be retrieved.
+#
+#     Returns
+#     -------
+#     df : pandas.DataFrame
+#         A list of portfolios.
+#
+#     """
+#     payload = {}
+#     headers = {
+#         'x-functions-key': api_inputs.api_key,
+#         'Content-Type': 'application/json; charset=utf-8',
+#         'user-key': api_inputs.user_id
+#     }
+#
+#     if api_inputs.datacentre == '' or api_inputs.api_key == '':
+#         logger.error("You must call initialize() before using API.")
+#         return pandas.DataFrame()
+#
+#     if search_term is None:
+#         search_term = '*'
+#
+#     url = api_prefix + "Portfolios/" + search_term
+#     response = requests.request("GET", url, timeout=20, headers=headers)
+#     response_status = '{} {}'.format(response.status_code, response.reason)
+#     if response.status_code != 200:
+#         logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+#                      response.reason)
+#         return response_status, pandas.DataFrame()
+#     elif len(response.text) == 0:
+#         logger.error('No data returned for this API call. %s', response.request.url)
+#         return response_status, pandas.DataFrame()
+#
+#     df = pandas.read_json(response.text)
+#
+#     return df
+
+
+def amortise_across_days(raw_df: pandas.DataFrame, start_date_col:str, end_date_col: str, value_col: str,
+                         amortise_method: AMORTISATION_METHOD = "Exclusive"):
+    """Amortise data across days in period.
+
+    The period is defined by the `start_date_col` and `end_date_col`.
+
+    Parameters
+    ----------
+    raw_df : pandas.DataFrame
+        The raw dataframe to be amortised.
+    start_date_col : str
+        The name of the column containing the start date of the period
+    end_date_col : str
+        The name of the column containing the end date of the period
+    value_col : str
+        The name of the column containing the numeric data to be amortised across the period.
+    amortise_method : AMORTISATION_METHOD, optional
+        Whether the amortisation should be inclusive or exclusive of the end date.
+
+    Returns
+    -------
+    amortised_df : pandas.DataFrame
+        Returned dataframe will contain three additional columns: num_days, amortised_value, Timestamp
+    """
+
+    if not set([start_date_col, end_date_col, value_col]).issubset(raw_df.columns):
+        col_lookup = pandas.DataFrame({'ColumnType': ['start_date_col', 'end_date_col', 'value_col'],
+                                     'ColumnName': [start_date_col, end_date_col, value_col]})
+        logger.exception(f"The raw_df does not contain column(s) defined for the input parameter(s): "
+                         f"{col_lookup[col_lookup.ColumnName.isin(list(set([start_date_col, end_date_col, value_col]).difference(raw_df.columns)))].ColumnType.values.tolist()}")
+        return False
+
+    start = start_date_col
+    end = end_date_col
+
+    if amortise_method == "Exclusive":
+        raw_df['num_days'] = (raw_df[end] - raw_df[start]).dt.days
+        raw_df['amortised_value'] = raw_df[value_col] / raw_df['num_days']
+
+        def make_list(row):
+            # make list of dates between Start Date & End Date (exclusive of the End Date).
+            return pd.date_range(start=row[start], end=row[end], freq='D').to_list()[:-1]
+
+        raw_df['Timestamp'] = raw_df[[start, end]].apply(lambda x: make_list(x), axis=1)
+        explode_df = raw_df.explode(column='Timestamp')
+
+        return explode_df
+    elif amortise_method == "Inclusive":
+        raw_df['num_days'] = (raw_df[end] - raw_df[start]).dt.days + 1
+        raw_df['amortised_value'] = raw_df[value_col] / raw_df['num_days']
+
+        def make_list(row):
+            return pd.date_range(start=row[start], end=row[end], freq='D').to_list()
+
+        raw_df['Timestamp'] = raw_df[[start, end]].apply(lambda x: make_list(x), axis=1)
+        explode_df = raw_df.explode(column='Timestamp')
+
+        return explode_df
+
+
+def get_metadata_where_clause(meta_columns:list):
+    sql_where_clause = ''.join(
+        ['([' + col + '] IS NOT NULL) AND '
+         if (col != meta_columns[-1])
+         else '([' + col + '] IS NOT NULL)' for col in
+         meta_columns])
+
+    return sql_where_clause
+
```

### Comparing `switch_api-0.5.4b2/switch_api/integration/integration.py` & `switch_api-0.5.4b3/switch_api/integration/integration.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,2292 +1,2016 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for integrating asset creation, asset updates, data ingestion, etc into the Switch Automation Platform.
-"""
-import gc
-import re
-import sys
-from typing import Union, List
-import pandas
-import pandas as pd
-from ..integration.helpers import get_timezones
-import pandera
-import json
-import requests
-import datetime
-import logging
-import uuid
-from .._utils._platform import _get_structure, Blob
-from .._utils._utils import (ApiInputs, _with_func_attrs, _column_name_cap, _work_order_schema, _reservation_schema,
-                             convert_bytes, IngestionMode)
-from .._utils._constants import TAG_LEVEL
-from ..integration._utils import _timezone_offsets, _upsert_entities_affected_count, _adx_support, _timezone_dst_offsets
-from .._utils._constants import ADX_TABLE_DEF_TYPES, RESOURCE_TYPE, RESERVATION_STATUS
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(stream=sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-@_with_func_attrs(df_required_columns=['ApiProjectId', 'InstallationId', 'NetworkDeviceId', 'UserId', 'BatchNo',
-                                       'DriverUniqueId', 'Timestamp', 'DiscoveredValue', 'DriverClassName', 'JobId'],
-                  df_optional_columns=['DriverDeviceType', 'ObjectPropertyTemplateName', 'UnitOfMeasureAbbrev',
-                                       'DeviceName', 'DisplayName', 'EquipmentType', 'EquipmentLabel', ])
-def upsert_discovered_records(df: pandas.DataFrame, api_inputs: ApiInputs, discovery_properties_columns: list,
-                              device_tag_columns: list = None, sensor_tag_columns: list = None,
-                              metadata_columns: list = None, override_existing: bool = False):  # , ontology_tag_columns: list = None):
-    """Upsert discovered records to populate Build - Discovery & Selection UI.
-
-    Parameters
-    ----------
-    df : pandas.DataFrame
-        The dataframe containing the discovered records including the minimum required set of columns.
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    discovery_properties_columns : list
-        List of the discovery property columns returned by 3rd party API.
-    device_tag_columns : list, default = None
-        List of columns that represent device-level tag group(s) (Default value = None)
-    sensor_tag_columns : list, default = None
-        List of column names in input `df` that represent sensor-level tag group(s) (Default value = None).
-    metadata_columns : list, default = None
-        List of column names in input `df` that represent device-level metadata key(s) (Default value = None).
-    override_existing : bool, default = False
-        Flag if it the values passed to df will override existing integration records. Only valid if running locally,
-        not on a deployed task where it is triggered via UI.
-    # ontology_tag_columns : list, default = None
-    #     List of BRICK schema or Haystack tags that apply to a given point (Default value = None).
-
-    Returns
-    -------
-    tuple[pandas.DataFrame, pandas.DataFrame]
-        (response_df, errors_df) - Returns the response dataframe and the dataframe containing the parsed errors text
-        (if no errors, then empty dataframe).
-
-    """
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    data_frame = df.copy()
-
-    required_columns = ['ApiProjectId', 'InstallationId', 'NetworkDeviceId', 'UserId', 'BatchNo', 'DriverUniqueId',
-                        'Timestamp', 'DiscoveredValue', 'DriverClassName', 'JobId']
-    optional_columns = ['DriverDeviceType', 'ObjectPropertyTemplateName', 'UnitOfMeasureAbbrev', 'DeviceName',
-                        'DisplayName', 'EquipmentType', 'EquipmentLabel', ]
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(proposed_columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        error= 'integration.upsert_discovered_records() - df must contain the following columns: ' + ', '.join(
-            required_columns) + '. Optional Columns include: ' + ', '.join(optional_columns)
-        return error
-
-    missing_optional_columns = set(optional_columns) - set(proposed_columns)
-    for missing_column in missing_optional_columns:
-        data_frame[missing_column] = None
-
-    if discovery_properties_columns is not None and not set(discovery_properties_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected discovery property column(s): %s',
-                         set(discovery_properties_columns).difference(proposed_columns))
-        error = 'Integration.upsert_discovered_records(): df must contain the following discovery property column(s): ' + \
-               ', '.join(discovery_properties_columns)
-        return error
-    elif discovery_properties_columns is None:
-        logger.exception('Missing expected discovery property column(s): %s',
-                         set(discovery_properties_columns).difference(proposed_columns))
-        error= 'Integration.upsert_discovered_records(): df must contain the following discovery property' \
-               ' columns: ' + ', '.join(discovery_properties_columns)
-        return error
-
-    if device_tag_columns is not None and not set(device_tag_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected device tag column(s): %s',
-                         set(device_tag_columns).difference(proposed_columns))
-        error = 'Integration.upsert_discovered_records(): df expected to contain the following device tag ' \
-               'column(s): ' + ', '.join(device_tag_columns)
-        return error
-    elif device_tag_columns is None:
-        device_tag_columns = []
-
-    if sensor_tag_columns is not None and not set(sensor_tag_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected sensor tag column(s): %s',
-                         set(sensor_tag_columns).difference(proposed_columns))
-        error = 'Integration.upsert_discovered_records(): df expected to contain the following sensor tag ' \
-               'column(s): ' + ', '.join(sensor_tag_columns)
-        return error
-    elif sensor_tag_columns is None:
-        sensor_tag_columns = []
-
-    if metadata_columns is not None and not set(metadata_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected metadata column(s): %s',
-                         set(metadata_columns).difference(proposed_columns))
-        error = 'Integration.upsert_discovered_records(): df expected to contain the following metadata ' \
-               'column(s): ' + ', '.join(metadata_columns)
-        return error
-    elif metadata_columns is None:
-        metadata_columns = []
-
-    expected_cols = required_columns + optional_columns + discovery_properties_columns + device_tag_columns + sensor_tag_columns + metadata_columns
-    if set(expected_cols).symmetric_difference(data_frame.columns).__len__() != 0:
-        if not set(data_frame.columns).issubset(expected_cols):
-            logger.exception(f'Additional column(s) present in df outside those defined in the '
-                             f'integration.upsert_discovered_records.df_required_columns & '
-                             f'integration.upsert_discovered_records.df_optional_columns and those passed to the '
-                             f'discovery_properties_columns, device_tag_columns, sensor_tag_columns, '
-                             f'metadata_columns input arguments: {set(data_frame.columns).difference(expected_cols)}' )
-            error = (f'Integration.upsert_discovered_records(): df contains additional columns outside those '
-                    f'defined in the integration.upsert_discovered_records.df_required_columns &'
-                    f'integration.upsert_discovered_records.df_optional_columns and those defined in the '
-                    f'discovery_properties_columns, device_tag_columns, sensor_tag_columns,metadata_columns input '
-                    f'arguments. \nPlease remove the unexpected column(s) from the input df provided or update the other '
-                    f'input arguments to include the unexpected column(s). '
-                    f'\nThe unexpected columns are: {", ".join(set(data_frame.columns).difference(expected_cols))}.  ')
-            return error
-
-    # if ontology_tag_columns is not None and not set(ontology_tag_columns).issubset(data_frame.columns):
-    #     logger.exception('Missing expected ontology tag column(s): %s',
-    #                      set(ontology_tag_columns).difference(proposed_columns))
-    #     return 'Integration.upsert_discovered_records(): data_frame expected to contain the following ontology tag ' \
-    #            'column(s): ' + ', '.join(ontology_tag_columns)
-    # elif ontology_tag_columns is None:
-    #     ontology_tag_columns = []
-
-    # convert timestamp format to required format
-    data_frame.Timestamp = data_frame.Timestamp.dt.strftime("%Y-%m-%dT%H:%M:%S.%fZ")
-    data_frame = data_frame.rename(columns={'DiscoveredValue': 'CurrentValue'})
-
-    column_dict = {'DiscoveryProperties': discovery_properties_columns + ['Timestamp'],
-                   'DeviceTags': device_tag_columns,
-                   'SensorTags': sensor_tag_columns,
-                   'Metadata': metadata_columns,
-                   # 'OntologyTags': ontology_tag_columns
-                   }
-
-    def set_properties(raw_df: pandas.DataFrame, column_dict: dict):
-        for key, value in column_dict.items():
-
-            def update_values(row):
-                j_row = row[value].to_dict()
-                return j_row
-
-            if len(value) > 0:
-                raw_df[key] = raw_df.apply(update_values, axis=1)
-            else:
-                raw_df[key] = None
-
-        return raw_df
-
-    data_frame = set_properties(raw_df=data_frame, column_dict=column_dict)
-    data_frame = data_frame.drop(columns=discovery_properties_columns + ['Timestamp'] + device_tag_columns +
-                                 sensor_tag_columns + metadata_columns  # + ontology_tag_columns
-                                 )
-    data_frame = data_frame.assign(OntologyTags=None)
-
-    final_req_cols = ['ApiProjectId', 'InstallationId', 'NetworkDeviceId', 'DriverClassName', 'UserId', 'BatchNo',
-                      'DriverUniqueId', 'CurrentValue', 'DriverDeviceType', 'ObjectPropertyTemplateName',
-                      'UnitOfMeasureAbbrev', 'DeviceName', 'DisplayName', 'EquipmentType', 'EquipmentLabel',
-                      'DeviceTags', 'SensorTags', 'OntologyTags', 'Metadata', 'DiscoveryProperties', 'JobId']
-
-    if set(data_frame.columns.tolist()).issubset(final_req_cols):
-        batch_size = 50
-        chunk_list = []
-        payload_error_list = []
-        grouped_df = data_frame.reset_index(drop=True).groupby(by=lambda x: x // batch_size, axis=0)
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/integrations/driver-discovery"
-        headers = api_inputs.api_headers.default
-
-        logger.info(f"Input has been batched into {grouped_df.ngroups} group(s). ")
-
-        for name, group in grouped_df:
-            discovery_payload = group.groupby(
-                by=['ApiProjectId', 'InstallationId', 'NetworkDeviceId', 'DriverClassName', 'UserId', 'BatchNo', 'JobId']).apply(
-                lambda x: x[['DriverUniqueId', 'CurrentValue', 'DriverDeviceType', 'ObjectPropertyTemplateName',
-                             'UnitOfMeasureAbbrev', 'DeviceName', 'DisplayName', 'EquipmentType', 'EquipmentLabel',
-                             'DeviceTags', 'SensorTags', 'OntologyTags', 'Metadata', 'DiscoveryProperties']].to_dict(
-                    orient='records')).reset_index().rename(columns={0: 'Sensors'}).to_json(orient='records')
-
-            # remove outer [] from json
-            discovery_payload = re.sub(r"^[\[]", '', re.sub(r"[\]]$", '', discovery_payload))
-            # The code snippet is attempting to load a JSON string `discovery_payload` into a Python
-            # dictionary using the `json.loads()` function.
-            discovery_payload = json.loads(discovery_payload)
-            if api_inputs.data_feed_file_status_id != '00000000-0000-0000-0000-000000000000':
-                discovery_payload['IsOverride'] = False
-            else:
-                discovery_payload['IsOverride'] = override_existing
-            discovery_payload = json.dumps(discovery_payload)
-
-            logger.info(f"Upserting discovery record group {name} of {grouped_df.ngroups - 1}. ")
-            response = requests.post(url=url, headers=headers, data=discovery_payload)
-
-            response_status = '{} {}'.format(response.status_code, response.reason)
-            if response.status_code != 200 and len(response.text) > 0:
-                logger.error(f"API Call was not successful. Response Status: {response.status_code}. "
-                             f"Reason: {response.reason}. Error Text: {response.text}")
-                payload_error_list += [{'Chunk': name, 'Payload': discovery_payload}]
-                if response.text.startswith('{'):
-                    response_content = response.json()
-                    chunk_list += [{'Chunk': name, 'response_status': response.status_code,
-                                    'response_reason': response.reason, 'error_code': response_content['ErrorCode'],
-                                    'errors': response_content['Errors']}]
-                elif response.text.startswith('"'):
-                    chunk_list += [{'Chunk': name, 'response_status': response.status_code,
-                                    'response_reason': response.reason, 'errors': response.json()}]
-                else:
-                    chunk_list += [{'Chunk': name, 'response_status': response.status_code,
-                                    'response_reason': response.reason, 'errors': response.text}]
-            elif response.status_code != 200 and len(response.text) == 0:
-                logger.error(f"API Call was not successful. Response Status: {response.status_code}. "
-                             f"Reason: {response.reason}. ")
-                payload_error_list += [{'Chunk': name, 'Payload': discovery_payload}]
-                chunk_list += [{'Chunk': name, 'response_status': response.status_code,
-                                'response_reason': response.reason}]
-            elif response.status_code == 200:
-                logger.info(f"API Call was successful. ")
-                chunk_list += [{'Chunk': name, 'response_status': response.status_code,
-                                'response_reason': response.reason}]
-
-        upsert_response_df = pandas.DataFrame(chunk_list)
-
-        if 0 < len(payload_error_list) <= grouped_df.ngroups:
-            payload_error_df = pandas.DataFrame(payload_error_list)
-            logger.error(f"Errors on upsert of discovered records. ")
-            return upsert_response_df, payload_error_df
-
-    return upsert_response_df, pandas.DataFrame()
-
-
-@_with_func_attrs(df_required_columns=['InstallationCode', 'DeviceCode', 'DeviceName', 'SensorName', 'SensorTemplate',
-                                       'SensorUnitOfMeasure', 'EquipmentClass', 'EquipmentLabel'])
-def upsert_device_sensors(df: pandas.DataFrame, api_inputs: ApiInputs, tag_columns: list = None,
-                          metadata_columns: list = None, save_additional_columns_as_slices: bool = False):
-    """Upsert device(s) and sensor(s)
-
-    Required fields are:
-
-    - InstallationCode
-    - DeviceCode
-    - DeviceName
-    - SensorName
-    - SensorTemplate
-    - SensorUnitOfMeasure
-    - EquipmentClass
-    - EquipmentLabel
-
-    Parameters
-    ----------
-    df: pandas.DataFrame
-        The asset register created by the driver including the minimum required set of columns.
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    tag_columns : list, default = None
-        Columns of dataframe that contain tags (Default value = None).
-    metadata_columns : list, default = None
-        Column(s) of dataframe that contain device-level metadata (Default value = None).
-    save_additional_columns_as_slices : bool, default = False
-        Whether additional columns should be saved as slices (Default value = False).
-
-    Returns
-    -------
-    tuple[list, pandas.DataFrame]
-        (response_status_list, upsert_response_df) - Returns the list of response statuses and the dataframe containing
-        the parsed response text.
-
-    """
-    pd.set_option('display.max_columns', 10)
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    data_frame = df.copy()
-
-    required_columns = ['InstallationCode', 'DeviceCode', 'DeviceName', 'SensorName', 'SensorTemplate',
-                        'SensorUnitOfMeasure', 'EquipmentClass', 'EquipmentLabel']
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(data_frame.columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        return 'Integration.upsert_device_sensors(): data_frame must contain the following columns: ' + ', '.join(
-            required_columns)
-
-    if tag_columns is not None and not set(tag_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected tag column(s): %s', set(tag_columns).difference(proposed_columns))
-        return 'Integration.upsert_device_sensors(): data_frame expected to contain the following tag column(s): ' + \
-               ', '.join(tag_columns)
-    elif tag_columns is None:
-        tag_columns = []
-
-    if metadata_columns is not None and not set(metadata_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected metadata column(s): %s', set(metadata_columns).difference(proposed_columns))
-        return 'Integration.upsert_device_sensors(): data_frame expected to contain the following metadata ' \
-               'column(s): ' + ', '.join(metadata_columns)
-    elif metadata_columns is None:
-        metadata_columns = []
-
-    slice_columns = set(proposed_columns).difference(set(required_columns)) - set(tag_columns) - set(metadata_columns)
-    slice_columns = list(slice_columns)
-    slices_data_frame = pandas.DataFrame()
-
-    if len(slice_columns) > 0 or len(tag_columns) > 0 or len(metadata_columns) > 0:
-        def update_values(row, mode):
-            if mode == 'A':
-                j_row = row[slice_columns].to_json()
-                if j_row == '{}':
-                    j_row = ''
-                return str(j_row)
-            elif mode == 'B':
-                j_row = row[tag_columns].to_json()
-                if j_row == '{}':
-                    j_row = ''
-                return str(j_row)
-            else:
-                j_row = row[metadata_columns].to_json()
-                if j_row == '{}':
-                    j_row = ''
-                return str(j_row)
-
-        data_frame['Slices'] = data_frame.apply(update_values, args="A", axis=1)
-        data_frame['TagsJson'] = data_frame.apply(update_values, args="B", axis=1)
-        data_frame['MetadataJson'] = data_frame.apply(update_values, args="C", axis=1)
-        data_frame = data_frame.drop(columns=slice_columns)
-        slices_data_frame = data_frame[['DeviceCode', 'Slices']]
-
-    headers = api_inputs.api_headers.integration
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion"
-
-    data_frame_grpd = data_frame.groupby(['InstallationCode', 'DeviceCode'])
-    chunk_list = []
-    for name, group in data_frame_grpd:
-        logger.info("Sending request: POST %s", url)
-        logger.info('Upserting data for InstallationCode = %s and DeviceCode = %s', str(name[0]), str(name[1]))
-        # logger.info('Sensor count to upsert: %s', str(group.shape[0]))
-
-        response = requests.post(url, data=group.to_json(orient='records'), headers=headers)
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        logger.info("Response status: %s", response_status)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1, 'SensorCountToUpsert': str(group.shape[0]),
-                            'response_status': response_status, 'response_df': pandas.DataFrame(),
-                            'invalid_rows': pandas.DataFrame()}]
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1, 'SensorCountToUpsert': str(group.shape[0]),
-                            'response_status': response_status, 'response_df': pandas.DataFrame(),
-                            'invalid_rows': pandas.DataFrame()}]
-        elif response.status_code == 200 and len(response.text) > 0:
-            response_data_frame = pandas.read_json(response.text)
-            logger.info('Dataframe response row count = %s', str(response_data_frame.shape[0]))
-            if response_data_frame.shape[1] > 0:
-                response_data_frame = response_data_frame.assign(InstallationCode=str(name[0]),
-                                                                 SensorCount=group.shape[0])
-                invalid_rows = response_data_frame[response_data_frame['status'] != 'Ok']
-                if invalid_rows.shape[0] > 0:
-                    logger.error("The following rows contain invalid data: \n %s", invalid_rows)
-                    chunk_list += [
-                        {'Chunk': name, 'DeviceCountToUpsert': 1,
-                         'SensorCountToUpsert': str(group.shape[0]),
-                         'response_status': response_status,
-                         'response_df': response_data_frame[response_data_frame['status'] == 'Ok'],
-                         'invalid_rows': invalid_rows}]
-                else:
-                    chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1,
-                                    'SensorCountToUpsert': str(group.shape[0]),
-                                    'response_status': response_status, 'response_df': response_data_frame,
-                                    'invalid_rows': invalid_rows}]
-
-    upsert_response_df = pandas.DataFrame()
-    upsert_invalid_rows_df = pandas.DataFrame()
-    upsert_response_status_list = []
-    for i in range(len(chunk_list)):
-        upsert_response_df = pandas.concat([upsert_response_df, chunk_list[i]['response_df']], axis=0, ignore_index=True)
-        upsert_invalid_rows_df = pandas.concat([upsert_invalid_rows_df, chunk_list[i]['invalid_rows']], axis=0, ignore_index=True)
-        upsert_response_status_list += [chunk_list[i]['response_status']]
-
-    if save_additional_columns_as_slices and slices_data_frame.shape[0] > 0:
-        slices_merged = pandas.merge(left=upsert_response_df, right=slices_data_frame, left_on='deviceCode',
-                                     right_on='DeviceCode')
-        slices_data_frame = slices_merged[['deviceId', 'Slices']]
-        slices_data_frame = slices_data_frame.rename(columns={'deviceId': 'DeviceId'})
-        upsert_data(slices_data_frame, api_inputs=api_inputs, key_columns=['DeviceId'], table_name='DeviceSensorSlices',
-                    is_slices_table=True)
-
-    upsert_response_df.columns = _column_name_cap(columns=upsert_response_df.columns)
-    _upsert_entities_affected_count(api_inputs=api_inputs,
-                                    entities_affected_count=upsert_response_df['SensorCount'].sum())
-    _adx_support(api_inputs=api_inputs, payload_type='Sensors')
-
-    if upsert_invalid_rows_df.shape[0]>0:
-        logger.error(f"There were {upsert_invalid_rows_df.shape[0]} devices that were not successfully upserted: /n "
-                     f"{upsert_invalid_rows_df}")
-
-    logger.info("Ingestion Complete. ")
-    return upsert_response_status_list, upsert_response_df
-
-@_with_func_attrs(df_required_columns=['DriverClassName', 'DriverDeviceType', 'PropertyName', 'DeviceCode', 'DeviceName', 'SensorName', 'SensorTemplate',
-                        'SensorUnitOfMeasure', 'EquipmentClass', 'EquipmentLabel'])
-def upsert_device_sensors_ext(df: pandas.DataFrame, api_inputs: ApiInputs, tag_columns: list = None,
-                          metadata_columns: list = None, save_additional_columns_as_slices: bool = False):
-    """Upsert device(s) and sensor(s)
-
-    Required fields are:
-
-    - InstallationCode or InstallationId
-    - DriverClassName
-    - DriverDeviceType
-    - DeviceCode
-    - DeviceName
-    - PropertyName
-    - SensorName
-    - SensorTemplate
-    - SensorUnitOfMeasure
-    - EquipmentClass
-    - EquipmentLabel
-
-    Parameters
-    ----------
-    df: pandas.DataFrame
-        The asset register created by the driver including the minimum required set of columns.
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    tag_columns : list, default = None
-        Columns of dataframe that contain tags (Default value = None).
-    metadata_columns : list, default = None
-        Column(s) of dataframe that contain device-level metadata (Default value = None).
-    save_additional_columns_as_slices : bool, default = False
-        Whether additional columns should be saved as slices (Default value = False).
-
-    Returns
-    -------
-    tuple[list, pandas.DataFrame]
-        (response_status_list, upsert_response_df) - Returns the list of response statuses and the dataframe containing
-        the parsed response text.
-
-    """
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    data_frame = df.copy()
-
-    required_columns = ['DriverClassName', 'DriverDeviceType', 'PropertyName', 'DeviceCode', 'DeviceName', 'SensorName', 'SensorTemplate',
-                        'SensorUnitOfMeasure', 'EquipmentClass', 'EquipmentLabel']
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(data_frame.columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        return 'Integration.upsert_device_sensors(): data_frame must contain the following columns: ' + ', '.join(
-            required_columns)
-   
-    if 'InstallationCode' not in data_frame.columns and 'InstallationId' not in data_frame.columns:
-        logger.exception('Must contain InstallationCode or InstallationId')
-        return 'Integration.upsert_device_sensors(): data_frame must contain either InstallationCode or InstallationId columns'
-   
-    if tag_columns is not None and not set(tag_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected tag column(s): %s', set(tag_columns).difference(proposed_columns))
-        return 'Integration.upsert_device_sensors(): data_frame expected to contain the following tag column(s): ' + \
-               ', '.join(tag_columns)
-    elif tag_columns is None:
-        tag_columns = []
-
-    if metadata_columns is not None and not set(metadata_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected metadata column(s): %s', set(metadata_columns).difference(proposed_columns))
-        return 'Integration.upsert_device_sensors(): data_frame expected to contain the following metadata ' \
-               'column(s): ' + ', '.join(metadata_columns)
-    elif metadata_columns is None:
-        metadata_columns = []
-
-    required_columns.append('InstallationCode')
-    required_columns.append('InstallationId')
-    slice_columns = set(proposed_columns).difference(set(required_columns)) - set(tag_columns) - set(metadata_columns)
-    slice_columns = list(slice_columns)
-    slices_data_frame = pandas.DataFrame()
-
-    if len(slice_columns) > 0 or len(tag_columns) > 0 or len(metadata_columns) > 0:
-        def update_values(row, mode):
-            if mode == 'A':
-                j_row = row[slice_columns].to_json()
-                if j_row == '{}':
-                    j_row = ''
-                return str(j_row)
-            elif mode == 'B':
-                j_row = row[tag_columns].to_json()
-                if j_row == '{}':
-                    j_row = ''
-                return str(j_row)
-            else:
-                j_row = row[metadata_columns].to_json()
-                if j_row == '{}':
-                    j_row = ''
-                return str(j_row)
-
-        data_frame['Slices'] = data_frame.apply(update_values, args="A", axis=1)
-       
-        if tag_columns is not None:
-            data_frame['TagsJson'] = data_frame.apply(update_values, args="B", axis=1)
-
-        if metadata_columns is not None:
-            data_frame['MetadataJson'] = data_frame.apply(update_values, args="C", axis=1)
-
-        data_frame = data_frame.drop(columns=slice_columns)
-        slices_data_frame = data_frame[['DeviceCode', 'Slices']]
-
-    headers = api_inputs.api_headers.integration
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion"
-   
-    def group_data_frame(df):
-        if 'InstallationCode' in data_frame.columns:
-            return df.groupby(['InstallationCode', 'DeviceCode'])
-        else:
-            return df.groupby(['InstallationId', 'DeviceCode'])
-   
-    data_frame_grpd = group_data_frame(data_frame)
-
-
-    chunk_list = []
-    for name, group in data_frame_grpd:
-        logger.info("Sending request: POST %s", url)
-        logger.info('Upserting data for InstallationCode = %s and DeviceCode = %s', str(name[0]), str(name[1]))
-        # logger.info('Sensor count to upsert: %s', str(group.shape[0]))
-        print(group.to_json(orient='records'))
-        response = requests.post(url, data=group.to_json(orient='records'), headers=headers)
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        logger.info("Response status: %s", response_status)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1, 'SensorCountToUpsert': str(group.shape[0]),
-                            'response_status': response_status, 'response_df': pandas.DataFrame(),
-                            'invalid_rows': pandas.DataFrame()}]
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1, 'SensorCountToUpsert': str(group.shape[0]),
-                            'response_status': response_status, 'response_df': pandas.DataFrame(),
-                            'invalid_rows': pandas.DataFrame()}]
-        elif response.status_code == 200 and len(response.text) > 0:
-            response_data_frame = pandas.read_json(response.text)
-            logger.info('Dataframe response row count = %s', str(response_data_frame.shape[0]))
-            if response_data_frame.shape[1] > 0:
-                response_data_frame = response_data_frame.assign(InstallationCode=str(name[0]),
-                                                                 SensorCount=group.shape[0])
-                invalid_rows = response_data_frame[response_data_frame['status'] != 'Ok']
-                if invalid_rows.shape[0] > 0:
-                    logger.error("The following rows contain invalid data: %s", invalid_rows)
-                    chunk_list += [
-                        {'Chunk': name, 'DeviceCountToUpsert': 1,
-                         'SensorCountToUpsert': str(group.shape[0]),
-                         'response_status': response_status,
-                         'response_df': response_data_frame[response_data_frame['status'] == 'Ok'],
-                         'invalid_rows': invalid_rows}]
-                else:
-                    chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1,
-                                    'SensorCountToUpsert': str(group.shape[0]),
-                                    'response_status': response_status, 'response_df': response_data_frame,
-                                    'invalid_rows': invalid_rows}]
-
-    upsert_response_df = pandas.DataFrame()
-    upsert_invalid_rows_df = pandas.DataFrame()
-    upsert_response_status_list = []
-    for i in range(len(chunk_list)):
-        upsert_response_df =pandas.concat([upsert_response_df, chunk_list[i]['response_df']], axis=0, ignore_index=True)
-        upsert_invalid_rows_df = pandas.concat([upsert_invalid_rows_df, chunk_list[i]['invalid_rows']], axis=0, ignore_index=True)
-        upsert_response_status_list += [chunk_list[i]['response_status']]
-
-    if save_additional_columns_as_slices and slices_data_frame.shape[0] > 0:
-        slices_merged = pandas.merge(left=upsert_response_df, right=slices_data_frame, left_on='deviceCode',
-                                     right_on='DeviceCode')
-        slices_data_frame = slices_merged[['deviceId', 'Slices']]
-        slices_data_frame = slices_data_frame.rename(columns={'deviceId': 'DeviceId'})
-        upsert_data(slices_data_frame, api_inputs=api_inputs, key_columns=['DeviceId'], table_name='DeviceSensorSlices',
-                    is_slices_table=True)
-
-    upsert_response_df.columns = _column_name_cap(columns=upsert_response_df.columns)
-    _upsert_entities_affected_count(api_inputs=api_inputs,
-                                    entities_affected_count=upsert_response_df['SensorCount'].sum())
-    _adx_support(api_inputs=api_inputs, payload_type='Sensors')
-
-    logger.info("Ingestion Complete. ")
-    return upsert_response_status_list, upsert_response_df
-
-
-@_with_func_attrs(df_required_columns=['InstallationName', 'InstallationCode', 'Address', 'Country', 'Suburb', 'State',
-                                       'StateName', 'FloorAreaM2', 'ZipPostCode'],
-                  df_optional_columns=['Latitude', 'Longitude', 'Timezone', 'InstallationId'])
-def upsert_sites(df: pandas.DataFrame, api_inputs: ApiInputs, tag_columns: list = None,
-                 save_additional_columns_as_slices: bool = False):
-    """Upsert site(s).
-
-    The `df` input must contain the following columns:
-        - InstallationName
-        - InstallationCode
-        - Address
-        - Suburb
-        - State
-        - StateName
-        - Country
-        - FloorAreaM2
-        - ZipPostCode
-
-    The following additional columns are optional:
-        - Latitude
-        - Longitude
-        - Timezone
-        - InstallationId
-            - The UUID of the existing site within the Switch Automation Platform.
-
-    Parameters
-    ----------
-    df: pandas.DataFrame :
-        The dataframe containing the sites to be created/updated in the Switch platform. All required columns must be
-        present with no null values.
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    tag_columns : list, default=[]
-        The columns containing site-level tags. The column header will be the tag group name. (Default value = True)
-    save_additional_columns_as_slices : bool, default = False
-        Whether any additional columns should be saved as slices. (Default value = False)
-
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response, response_data_frame) - Returns the response status and the dataframe containing the parsed response
-        text.
-
-    """
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    data_frame = df.copy()
-
-    required_columns = ['InstallationName', 'InstallationCode', 'Address', 'Country', 'Suburb', 'State', 'StateName',
-                        'FloorAreaM2', 'ZipPostCode']
-    optional_columns = ['Latitude', 'Longitude', 'Timezone', 'InstallationId']
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(proposed_columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        return 'Integration.upsert_sites() - data_frame must contain the following columns: ' + ', '.join(
-            required_columns) + '. Optional Columns include: ' + ', '.join(optional_columns)
-
-    if tag_columns is not None and not set(tag_columns).issubset(data_frame.columns):
-        logger.exception('Missing expected tag column(s): %s', set(tag_columns).difference(proposed_columns))
-        return 'Integration.upsert_sites(): data_frame must contain the following tag columns: ' + ', '.join(
-            tag_columns)
-    elif tag_columns is None:
-        tag_columns = []
-
-    slice_columns = set(proposed_columns).difference(set(required_columns + optional_columns)) - set(tag_columns)
-    slice_columns = list(slice_columns)
-    slices_data_frame = pandas.DataFrame()
-
-    if len(slice_columns) > 0 or len(tag_columns) > 0:
-        def update_values(row, mode):
-            if mode == 'A':
-                j_row = row[slice_columns].to_json()
-                if j_row == '{}':
-                    j_row = ''
-                return str(j_row)
-            else:
-                j_row = row[tag_columns].to_json()
-                if j_row == '{}':
-                    j_row = ''
-                return str(j_row)
-
-        data_frame['Slices'] = data_frame.apply(update_values, args=('A',), axis=1)
-        data_frame['TagsJson'] = data_frame.apply(update_values, args=('B',), axis=1)
-
-        data_frame = data_frame.drop(columns=tag_columns)
-        data_frame = data_frame.drop(columns=slice_columns)
-        slices_data_frame = data_frame[['InstallationCode', 'Slices']]
-
-    headers = api_inputs.api_headers.integration
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/installations/upsert-ingestion"
-    logger.info("Sending request: POST %s", url)
-
-    response = requests.post(url, data=data_frame.to_json(orient='records'), headers=headers)
-
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error(f"API Call was not successful. Response Status: {response.status_code}. Reason: {response.reason}. "
-                     f"Message: {response.text}")
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-
-    response_data_frame = pandas.read_json(response.text)
-
-    if save_additional_columns_as_slices and slices_data_frame.shape[0] > 0:
-        slices_merged = pandas.merge(left=response_data_frame, right=slices_data_frame, left_on='installationCode',
-                                     right_on='InstallationCode')
-        slices_data_frame = slices_merged[['installationId', 'Slices']]
-        slices_data_frame = slices_data_frame.rename(columns={'installationId': 'InstallationId'})
-        upsert_data(slices_data_frame, api_inputs=api_inputs, key_columns=['InstallationId'],
-                    table_name='InstallationSlices', is_slices_table=True)
-
-    response_data_frame.columns = _column_name_cap(columns=response_data_frame.columns)
-
-    count_entities = response_data_frame.apply(
-        lambda row: 'Created' if (row['IsInserted'] == True and row['IsUpdated'] == False) else (
-            'Updated' if row['IsInserted'] == False and row['IsUpdated'] == True else 'Failed'), axis=1).isin(
-        ['Created', 'Updated']).sum()
-
-    _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=count_entities)
-    _adx_support(api_inputs=api_inputs, payload_type='Sites')
-
-    logger.info("Ingestion complete. ")
-
-    return response_status, response_data_frame
-
-
-@_with_func_attrs(df_required_columns=['WorkOrderId', 'InstallationId', 'WorkOrderSiteIdentifier', 'Status',
-                                       'RawStatus', 'Priority', 'RawPriority', 'WorkOrderCategory',
-                                       'RawWorkOrderCategory', 'Type', 'Description', 'CreatedDate',
-                                       'LastModifiedDate', 'WorkStartedDate', 'WorkCompletedDate', 'ClosedDate'],
-                  df_optional_columns=['SubType', 'Vendor', 'VendorId', 'EquipmentClass', 'RawEquipmentClass',
-                                       'EquipmentLabel', 'RawEquipmentId', 'TenantId', 'TenantName', 'NotToExceedCost',
-                                       'TotalCost', 'BillableCost', 'NonBillableCost', 'Location', 'RawLocation',
-                                       'ScheduledStartDate', 'ScheduledCompletionDate'])
-def upsert_workorders(df: pandas.DataFrame, api_inputs: ApiInputs, save_additional_columns_as_slices: bool = False):
-    """Upsert data to the Workorder table.
-
-    The following columns are required to be present in the df:
-
-    - ``WorkOrderId``: unique identifier for the work order instance
-    - ``InstallationId``: the InstallationId (guid) used to uniquely identify a given site within the Switch platform
-    - ``WorkOrderSiteIdentifier``: the work order provider's raw/native site identifier field
-    - ``Status``: the status mapped to the Switch standard values defined by literal: `WORK_ORDER_STATUS`
-    - ``RawStatus``: the work order provider's raw/native status
-    - ``Priority``: the priority mapped to the Switch standard values defined by literal: `WORK_ORDER_PRIORITY`
-    - ``RawPriority``: the work order provider's raw/native priority
-    - ``WorkOrderCategory``: the category mapped to the Switch standard values defined by literal: `WORK_ORDER_CATEGORY`
-    - ``RawWorkOrderCategory``: the work order provider's raw/native category
-    - ``Type`` - work order type (as defined by provider) - e.g. HVAC - Too Hot, etc.
-    - ``Description``: description of the work order.
-    - ``CreatedDate``: the date the work order was created (Submitted status)
-    - ``LastModifiedDate``: datetime the workorder was last modified
-    - ``WorkStartedDate``: datetime work started on the work order (In Progress status)
-    - ``WorkCompletedDate``: datetime work was completed for the work order (Resolved status)
-    - ``ClosedDate``: datetime the workorder was closed (Closed status)
-
-    The following columns are optional:
-
-    - ``SubType``: the sub-type of the work order
-    - ``Vendor``: the name of the vendor
-    - ``VendorId``: the vendor id
-    - ``EquipmentClass``: the Switch defined Equipment Class mapped from the work order provider's definition
-    - ``RawEquipmentClass``: the work order provider's raw/native equipment class
-    - ``EquipmentLabel``: the EquipmentLabel as defined within the Switch platform
-    - ``RawEquipmentId``: the work order provider's raw/native equipment identifier/label
-    - ``TenantId``: the tenant id
-    - ``TenantName``: the name of the tenant
-    - ``NotToExceedCost``: the cost not to be exceeded for the given work order
-    - ``TotalCost``: total cost of the work order
-    - ``BillableCost``: the billable portion of the work order cost
-    - ``NonBillableCost``: the non-billable portion of the work order cost.
-    - ``Location``: the Location as defined within the Switch platform
-    - ``RawLocation``: the work order provider's raw/native location definition
-    - ``ScheduledStartDate``: datetime work was scheduled to start on the given work order
-    - ``ScheduledCompletionDate``" datetime work was scheduled to be completed for the given work order
-
-
-    Parameters
-    ----------
-    df: pandas.DataFrame
-        Dataframe containing the work order data to be upserted.
-    api_inputs: ApiInputs
-        Object returned by initialize() function.
-    save_additional_columns_as_slices : bool, default = False
-         (Default value = False)
-
-    Returns
-    -------
-
-    """
-
-    data_frame = df.copy()
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    required_columns = ['WorkOrderId', 'InstallationId', 'WorkOrderSiteIdentifier', 'Status',
-                        'RawStatus', 'Priority', 'RawPriority', 'WorkOrderCategory', 'RawWorkOrderCategory', 'Type',
-                        'Description', 'CreatedDate', 'LastModifiedDate', 'WorkStartedDate', 'WorkCompletedDate',
-                        'ClosedDate']
-    optional_columns = ['SubType', 'Vendor', 'VendorId', 'EquipmentClass', 'RawEquipmentClass', 'EquipmentLabel',
-                        'RawEquipmentId', 'TenantId', 'TenantName', 'NotToExceedCost', 'TotalCost', 'BillableCost',
-                        'NonBillableCost', 'Location', 'RawLocation', 'ScheduledStartDate', 'ScheduledCompletionDate']
-
-    req_cols = ', '.join(required_columns)
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(set(proposed_columns)):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(set(proposed_columns)))
-        return 'Integration.upsert_workorder() - data_frame must contain the following columns: ' + req_cols
-
-    try:
-        _work_order_schema.validate(data_frame, lazy=True)
-    except pandera.errors.SchemaErrors as err:
-        logger.error('Errors present with columns in df provided.')
-        logger.error(err.failure_cases)
-        schema_error = err.failure_cases
-        return schema_error
-
-    slice_columns = set(proposed_columns).difference(set(required_columns + optional_columns))
-    slice_columns = list(slice_columns)
-
-    missing_optional_columns = set(optional_columns) - set(proposed_columns)
-    for missing_column in missing_optional_columns:
-        data_frame[missing_column] = ''
-
-    if len(slice_columns) > 0 and save_additional_columns_as_slices is True:
-        def update_values(row):
-            j_row = row[slice_columns].to_json()
-            return str(j_row)
-
-        data_frame['Meta'] = data_frame.apply(update_values, axis=1)
-        data_frame = data_frame.drop(columns=slice_columns)
-    elif len(slice_columns) > 0 and save_additional_columns_as_slices is not True:
-        data_frame = data_frame.drop(columns=slice_columns)
-        data_frame['Meta'] = ''
-    else:
-        data_frame['Meta'] = ''
-
-    # payload = {}
-    headers = api_inputs.api_headers.integration
-
-    logger.info("Upserting data to Workorders.")
-
-    data_frame = data_frame.loc[:, ['WorkOrderId', 'InstallationId', 'WorkOrderSiteIdentifier', 'Status', 'Priority',
-                                    'WorkOrderCategory', 'Type', 'Description', 'CreatedDate', 'LastModifiedDate',
-                                    'WorkStartedDate', 'WorkCompletedDate', 'ClosedDate', 'RawPriority',
-                                    'RawWorkOrderCategory', 'RawStatus', 'SubType', 'Vendor', 'VendorId',
-                                    'EquipmentClass', 'RawEquipmentClass', 'EquipmentLabel', 'RawEquipmentId',
-                                    'TenantId', 'TenantName', 'NotToExceedCost', 'TotalCost', 'BillableCost',
-                                    'NonBillableCost', 'Location', 'RawLocation', 'ScheduledStartDate',
-                                    'ScheduledCompletionDate', 'Meta']]
-
-    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name='WorkOrder')
-    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "append",
-                    "tableDef": _get_structure(data_frame),
-                    "keyColumns": ["WorkOrderId"]
-                    }
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/work-order-operation"
-    logger.info("Sending request: POST %s", url)
-
-    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-    elif response.status_code == 200:
-        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
-
-    response_df = pandas.read_json(response.text, typ='series').to_frame().T
-    response_df.columns = _column_name_cap(columns=response_df.columns)
-
-    logger.info("Response status: %s", response_status)
-    logger.info("Ingestion complete. ")
-
-    return response_status, response_df
-
-
-@_with_func_attrs(df_required_columns=['ReservationId', 'InstallationId', 'ReservationSiteIdentifier', 'Status',
-                                       'RawStatus',  'ReservationStart', 'ReservationEnd', 'CreatedDate',
-                                       'LastModifiedDate', 'ObjectPropertyId', 'ResourceType', 'RawResourceType',
-                                       'ReservationSystem'],
-                  df_optional_columns=['ReservationName', 'Description', 'ReservedById', 'ReservedByEmail', 'LocationId',
-                                       'RawLocationId', 'Location', 'RawLocation', 'Source', 'AttendeeCount'])
-def upsert_reservations(df: pandas.DataFrame, api_inputs: ApiInputs, local_date_time_cols: Union[None, List],
-                        utc_date_time_cols: Union[None, List], save_additional_columns_as_slices: bool = False):
-    """Upserts data to the ReservationHistory table.
-
-    The following datetime fields are required and must use the ``local_date_time_cols`` and ``utc_date_time_cols``
-    parameters to define whether their values are in site-local timezone or UTC timezone:
-    - ``CreatedDate``
-    - ``LastModifiedDate``
-    - ``ReservationStart``
-    - ``ReservationEnd``
-
-    The following columns are required to be present in the df:
-    - ``ReservationId``: unique identifier for the reservation instance
-    - ``InstallationId``: the InstallationId (guid) used to uniquely identify a given site within the Switch platform
-    - ``ReservationSiteIdentifier``: the reservation system's raw/native site identifier field
-    - ``Status``: the status mapped to the Switch standard values defined by literal: `RESERVATION_STATUS`
-    - ``RawStatus``: the reservation provider's raw/native status
-    - ``CreatedDate``: the datetime (UTC) the reservation was created (Booked status)
-    - ``LastModifiedDate``: the datetime (UTC) the reservation was last modified
-    - ``ReservationStart``: the datetime (site local) the reservation is booked to start
-    - ``ReservationEnd``: the datetime (site local) the reservation is booked to end
-    - ``ObjectPropertyId``: the ObjectPropertyId (guid) used to uniquely identify the status sensor within the Switch
-    platform that records whether the given location is booked or not booked.
-    - ``ResourceType``: the type of resource booked mapped to the Switch standard values defined by literal: `RESOURCE_TYPE`
-    - ``RawResourceType``: the reservation system's raw/native resource type
-    - ``ReservationSystem``: the reservation system name
-
-    The following columns are optional:
-    - ``ReservationName``: name of the reservation
-    - ``Description``: description of the reservation
-    - ``ReservedById``: the identifier for who created the reservation
-    - ``ReservedByEmail``: the email address for who created the reservation
-    - ``LocationId``: the Location identifier as defined within the Switch platform
-    - ``RawLocationId``: the reservation provider's raw/native location id definition
-    - ``Location``: the Location name as defined within the Switch platform
-    - ``RawLocation``: the reservation provider's raw/native location id definition
-    - ``Source``: the source of the reservation
-    - ``AttendeeCount``: the count of expected attendees, if meeting room.
-
-    Parameters
-    ----------
-    df: pandas.DataFrame
-        Dataframe containing the work order data to be upserted.
-    api_inputs: ApiInputs
-        Object returned by initialize() function.
-    local_date_time_cols: Union[None, List]
-        A list of the datetime columns where values are in site local timezone
-    utc_date_time_cols: Union[None, List]
-        A list of the datetime columns where values are in UTC timezone
-    save_additional_columns_as_slices : bool, default = False
-         (Default value = False)
-
-
-    Returns
-    -------
-    local_date_time_cols: Union[None, List],
-                        utc_date_time_cols: Union[None, List]
-
-    """
-    data_frame = df.copy()
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if not set(['DataFeedFileStatusId']).issubset(set(list(data_frame.columns))):
-        data_frame = data_frame.assign(DataFeedFileStatusId=api_inputs.data_feed_file_status_id)
-
-    required_columns = ['ReservationId', 'InstallationId', 'ReservationSiteIdentifier', 'Status', 'RawStatus',
-                        'ReservationStart', 'ReservationEnd', 'CreatedDate', 'LastModifiedDate',  'ObjectPropertyId',
-                        'ResourceType', 'RawResourceType', 'ReservationSystem', 'DataFeedFileStatusId']
-    optional_columns = ['ReservationName', 'Description', 'ReservedById', 'ReservedByEmail', 'LocationId',
-                        'RawLocationId', 'Location', 'RawLocation', 'Source', 'AttendeeCount']
-
-    req_cols = ', '.join(required_columns)
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(set(proposed_columns)):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(set(proposed_columns)))
-        return 'Integration.upsert_reservation() - data_frame must contain the following columns: ' + req_cols
-
-    datetime_cols = ['CreatedDate', 'LastModifiedDate', 'ReservationStart', 'ReservationEnd']
-
-    if local_date_time_cols is None and utc_date_time_cols is None:
-        error_msg = ('The required datetime columns must be aligned to whether they are in UTC or site-local '
-                         'timezone using the local_date_time_cols and utc_date_time_cols parameters. These '
-                         '2 parameters cannot both be set to None. ')
-        logger.exception(error_msg)
-        return error_msg
-    elif local_date_time_cols is None and not set(datetime_cols).issubset(set(utc_date_time_cols)):
-        error_msg = (f'Missing alignment of datetime column(s) to timezone type: '
-                     f'{set(datetime_cols).difference(set(utc_date_time_cols))}')
-        logger.exception(error_msg)
-        return error_msg
-    # elif type(local_date_time_cols)==list and not set(datetime_cols).issubset(set(local_date_time_cols)) and utc_date_time_cols is None:
-    elif utc_date_time_cols is None and not set(datetime_cols).issubset(set(local_date_time_cols)):
-        error_msg = (f'Missing alignment of datetime column(s) to timezone type: '
-                     f'{set(datetime_cols).difference(set(local_date_time_cols))}')
-        logger.exception(error_msg)
-        return error_msg
-    elif type(local_date_time_cols)==list and type(utc_date_time_cols)==list:
-        nonlocal_dt_cols_expected = set(datetime_cols).difference(set(local_date_time_cols))
-        missed_cols = set(nonlocal_dt_cols_expected).difference(set(utc_date_time_cols))
-        if len(missed_cols) != 0:
-            error_msg = f'Missing alignment of datetime column(s) to timezone type: {missed_cols}'
-            logger.exception(error_msg)
-            return error_msg
-
-    try:
-        _reservation_schema.validate(data_frame, lazy=True)
-    except pandera.errors.SchemaErrors as err:
-        logger.error('Errors present with columns in df provided.')
-        logger.error(err.failure_cases)
-        schema_error = err.failure_cases
-        return schema_error
-
-    slice_columns = set(proposed_columns).difference(set(required_columns + optional_columns))
-    slice_columns = list(slice_columns)
-
-    missing_optional_columns = set(optional_columns) - set(proposed_columns)
-    for missing_column in missing_optional_columns:
-        data_frame[missing_column] = ''
-
-    if len(slice_columns) > 0 and save_additional_columns_as_slices is True:
-        def update_values(row):
-            j_row = row[slice_columns].to_json()
-            return str(j_row)
-
-        data_frame['Meta'] = data_frame.apply(update_values, axis=1)
-        data_frame = data_frame.drop(columns=slice_columns)
-    elif len(slice_columns) > 0 and save_additional_columns_as_slices is not True:
-        data_frame = data_frame.drop(columns=slice_columns)
-        data_frame['Meta'] = ''
-    else:
-        data_frame['Meta'] = ''
-
-    has_local_datetimes = None
-    has_utc_datetimes = None
-    if type(local_date_time_cols) != list and type(utc_date_time_cols) == list:
-        has_local_datetimes = False
-        has_utc_datetimes = True
-    elif type(local_date_time_cols) == list and type(utc_date_time_cols) != list:
-        has_local_datetimes = True
-        has_utc_datetimes = False
-    else:
-        has_local_datetimes = True
-        has_utc_datetimes = True
-
-    if type(datetime_cols) == list:
-        start_date_lst = []
-        end_date_lst = []
-
-        for i in datetime_cols:
-            start_date_lst.append(data_frame[i].min(axis=0, skipna=True))
-            end_date_lst.append(data_frame[i].max(axis=0, skipna=True))
-
-        start_date = min(start_date_lst)
-        end_date = max(end_date_lst)
-
-    def convert_dst_interval_dates(row):
-        row['start'] = pd.to_datetime(row['start'])
-        row['end'] = pd.to_datetime(row['end'])
-        row['offsetToUtcMinutes'] = row['standardOffsetUtc'] + row['dstOffsetUtc']
-        return row
-
-    site_list = data_frame['InstallationId'].unique().tolist()
-    timezones_df = _timezone_dst_offsets(
-        api_inputs=api_inputs, date_from=start_date.date(), date_to=end_date.date(), installation_id_list=site_list)
-
-    if timezones_df.empty:
-        sw.pipeline.logger.exception('Timezone DST offsets failed to retrieve.')
-        return 'Timezone DST offsets failed to retrieve.'
-
-    timezones_df = timezones_df.apply(convert_dst_interval_dates, axis=1)
-    timezones_df_utc = timezones_df.copy(deep=True)
-
-    def convert_dst_interval_range_to_utc(row):
-        dst_start = row['start']
-        dst_end = row['end']
-
-        if row['standardOffsetUtc'] >= 0:
-            dst_start = dst_start - (datetime.timedelta(minutes=row['standardOffsetUtc'])) - datetime.timedelta(
-                minutes=row['dstOffsetUtc'])
-            dst_end = dst_end - (datetime.timedelta(minutes=row['standardOffsetUtc'])) - datetime.timedelta(
-                minutes=row['dstOffsetUtc'])
-        else:
-            dst_start = dst_start + (datetime.timedelta(minutes=row['standardOffsetUtc'])) + datetime.timedelta(
-                minutes=row['dstOffsetUtc'])
-            dst_end = dst_end + (datetime.timedelta(minutes=row['standardOffsetUtc'])) + datetime.timedelta(
-                minutes=row['dstOffsetUtc'])
-        row['start'] = dst_start
-        row['end'] = dst_end
-
-        return row
-
-    timezones_df_utc = timezones_df_utc.apply(convert_dst_interval_range_to_utc, axis=1)
-    timezones_df = timezones_df.drop(columns=['timezoneId', 'standardOffsetUtc', 'dstOffsetUtc'])
-    timezones_df_utc = timezones_df_utc.drop(columns=['timezoneId', 'standardOffsetUtc', 'dstOffsetUtc'])
-
-    timezone_ref_cols = ['installationId', 'offsetToUtcMinutes']
-    timestamp_format = "%Y-%m-%dT%H:%M:%SZ"
-
-    if has_local_datetimes:
-        logger.info(f"Performing timezone conversions for the following datetime columns that are in site-local "
-                    f"timezone: {local_date_time_cols}")
-
-        for col in local_date_time_cols:
-            local_col = col + 'Local'
-            utc_col = col+ 'Utc'
-
-            logger.info(f'Merging data frame input with timezone installation dst offsets for {col}.')
-            data_frame = data_frame.merge(timezones_df, left_on='InstallationId', right_on='installationId', how='inner')
-
-            data_frame = data_frame[(data_frame[col] >= data_frame.start) & (data_frame[col] < data_frame.end) & (
-                    data_frame[col].apply(lambda x: x.year) == data_frame.year)]
-            data_frame = data_frame.drop(columns=['start', 'end', 'year'])
-
-            def to_utc(row):
-                if row['offsetToUtcMinutes'] >= 0:
-                    j_row = row[local_col] - datetime.timedelta(minutes=row['offsetToUtcMinutes'])
-                else:
-                    j_row = row[local_col] + datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
-                return j_row
-
-            data_frame = data_frame.assign(Utc=data_frame[col]).rename(columns={col: local_col, 'Utc': utc_col})
-            data_frame[utc_col] = data_frame.apply(to_utc, axis=1)
-            data_frame = data_frame.drop(columns=timezone_ref_cols)
-
-            if local_col=='CreatedDateLocal':
-                data_frame = data_frame.drop(columns=[local_col])
-            elif local_col=='LastModifiedDateLocal':
-                data_frame = data_frame.drop(columns=[local_col])
-
-        del timezones_df, col, local_col, utc_col
-        gc.collect()
-
-    if has_utc_datetimes:
-        logger.info(f"Performing timezone conversions for the following datetime columns that are in UTC: {utc_date_time_cols}")
-
-        if set(['CreatedDate', 'LastModifiedDate']).issubset(set(utc_date_time_cols)):
-            data_frame = data_frame.rename(
-                columns={'CreatedDate': 'CreatedDateUtc', 'LastModifiedDate': 'LastModifiedDateUtc'})
-            utc_date_time_cols = list(set(utc_date_time_cols).difference(set(['CreatedDate', 'LastModifiedDate'])))
-            if len(utc_date_time_cols) == 0:
-
-                has_utc_datetimes == False
-
-        for col in utc_date_time_cols:
-            local_col = col + 'Local'
-            utc_col = col + 'Utc'
-
-            logger.info(f'Merging data frame input with timezone installation dst offsets for {col}.')
-            data_frame = data_frame.merge(timezones_df_utc, left_on='InstallationId', right_on='installationId',
-                                          how='inner')
-
-            data_frame = data_frame[(data_frame[col] >= data_frame.start) & (data_frame[col] < data_frame.end) & (
-                    data_frame[col].apply(lambda x: x.year) == data_frame.year)]
-            data_frame = data_frame.drop(columns=['start', 'end', 'year'])
-
-            def from_utc(row):
-                if row['offsetToUtcMinutes'] >= 0:
-                    j_row = row[utc_col] + datetime.timedelta(minutes=row['offsetToUtcMinutes'])
-                else:
-                    j_row = row[utc_col] - datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
-                return j_row
-
-            data_frame = data_frame.assign(Local=data_frame[col]).rename(columns={col: utc_col, 'Local': local_col})
-            data_frame[local_col] = data_frame.apply(from_utc, axis=1)
-            data_frame = data_frame.drop(columns=timezone_ref_cols)
-
-        del timezones_df_utc
-        gc.collect()
-
-    # return data_frame
-
-    logger.info('Date time conversion and formatting completed.')
-
-    # timestamp_format = "%Y-%m-%dT%H:%M:%SZ"
-    # timestamp_normalized_format = "%Y-%m-%dT%H:%M:00Z"
-    # data_frame['Timestamp'] = data_frame['Timestamp'].dt.strftime(timestamp_format)
-    # data_frame['TimestampLocal'] = data_frame['TimestampLocal'].dt.strftime(timestamp_format)
-
-    headers = api_inputs.api_headers.integration
-
-    table_def = {
-        'ReservationId': 'string',
-        'InstallationId': 'string',
-        'ReservationSiteIdentifier': 'string',
-        'Status': 'string',
-        'RawStatus': 'string',
-        'ReservationStartLocal': 'datetime',
-        'ReservationStartUtc': 'datetime',
-        'ReservationEndLocal': 'datetime',
-        'ReservationEndUtc': 'datetime',
-        'CreatedDateUtc': 'datetime',
-        'LastModifiedDateUtc': 'datetime',
-        'ObjectPropertyId': 'string',
-        'ResourceType': 'string',
-        'RawResourceType': 'string',
-        'ReservationSystem': 'string',
-        'DataFeedFileStatusId': 'string',
-        'ReservationName': 'string',
-        'Description': 'string',
-        'ReservedById': 'string',
-        'ReservedByEmail': 'string',
-        'LocationId': 'string',
-        'RawLocationId': 'string',
-        'Location': 'string',
-        'RawLocation': 'string',
-        'Source': 'string',
-        'AttendeeCount': 'integer',
-        'Meta': 'dynamic'
-    }
-
-    data_frame = data_frame.loc[:,list(table_def.keys())]
-
-    logger.info("Upserting data to ReservationsHistory.")
-    table_name = 'ReservationHistory'
-
-    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=table_name)
-    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "append",
-                    "tableDef": table_def, 'ingestionMode': "Queue"}
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data-operation?tableName={table_name}"
-    logger.info("Sending request: POST %s", url)
-
-    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
-
-    logger.info("Sending request: POST %s", url)
-
-    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-    elif response.status_code == 200:
-        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
-
-    response_df = pandas.read_json(response.text, typ='series').to_frame().T
-    response_df.columns = _column_name_cap(columns=response_df.columns)
-
-    logger.info("Response status: %s", response_status)
-    logger.info("Ingestion complete. ")
-
-    return response_status, response_df
-
-
-@_with_func_attrs(df_required_columns=['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value'])
-def upsert_timeseries_ds(df: pandas.DataFrame, api_inputs: ApiInputs, is_local_time: bool = True,
-                         save_additional_columns_as_slices: bool = False, data_feed_file_status_id: uuid.UUID = None):
-    """Upserts to Timeseries_Ds table.
-
-    The following columns are required to be present in the data_frame:
-    - InstallationId
-    - ObjectPropertyId
-    - Timestamp
-    - Value
-
-    Parameters
-    ----------
-    df: pandas.DataFrame
-        Dataframe containing the data to be appended to timeseries.
-    api_inputs: ApiInputs
-        Object returned by initialize() function.
-    is_local_time : bool, default = True
-         Whether the datetime values are in local time or UTC. If false, then UTC (Default value = True).
-    save_additional_columns_as_slices : bool, default = False
-         (Default value = False)
-    data_feed_file_status_id : uuid.UUID, default = None
-         Enables developer to identify upserted rows using during development. This data is posted to the
-         DataFeedFileStatusId in the Timeseries_Ds table.
-
-         Once deployed, the DataFeedFileStatusId field will contain a unique Guid which will assist in
-         tracking upload results and logging.
-
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
-        text.
-
-    """
-
-    data_frame = df.copy()
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    required_columns = ['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value']
-    req_cols = ', '.join(required_columns)
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(proposed_columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        return 'integration.upsert_timeseries_ds() - data_frame must contain the following columns: ' + req_cols
-
-    slice_columns = set(proposed_columns).difference(set(required_columns))
-    slice_columns = list(slice_columns)
-
-    if len(slice_columns) > 0 and save_additional_columns_as_slices is True:
-        def update_values(row):
-            j_row = row[slice_columns].to_json()
-            return str(j_row)
-
-        # data_frame['Meta'] = data_frame.apply(update_values, axis=1)
-        data_frame['Meta'] = data_frame[slice_columns].assign(
-            **data_frame[slice_columns].select_dtypes(['datetime', 'object']).astype(str)).apply(update_values, axis=1)
-
-        data_frame = data_frame.drop(columns=slice_columns)
-    elif len(slice_columns) > 0 and save_additional_columns_as_slices is not True:
-        data_frame = data_frame.drop(columns=slice_columns)
-        data_frame['Meta'] = ''
-    else:
-        data_frame['Meta'] = ''
-
-    if api_inputs.data_feed_file_status_id is not None and api_inputs.data_feed_file_status_id != '00000000-0000-0000' \
-                                                                                                  '-0000-000000000000':
-        data_frame['DataFeedFileStatusId'] = api_inputs.data_feed_file_status_id
-    elif data_feed_file_status_id is not None:
-        data_frame['DataFeedFileStatusId'] = data_feed_file_status_id
-    else:
-        data_frame['DataFeedFileStatusId'] = '00000000-0000-0000-0000-000000000000'
-
-    site_list = data_frame['InstallationId'].unique().tolist()
-    start_date = data_frame['Timestamp'].min(axis=0, skipna=True)
-    end_date = data_frame['Timestamp'].max(axis=0, skipna=True)
-
-    timezones = _timezone_offsets(api_inputs=api_inputs, date_from=start_date.date(), date_to=end_date.date(),
-                                  installation_id_list=site_list)
-    timezones['dateFrom'] = timezones['dateFrom'].apply(lambda x: pandas.to_datetime(x))
-    timezones['dateTo'] = timezones['dateTo'].apply(lambda x: pandas.to_datetime(x))
-
-    data_frame = data_frame.merge(timezones, left_on='InstallationId', right_on='installationId', how='inner')
-
-    def in_range(row):
-        j_row = (row['Timestamp'] >= row['dateFrom']) & (
-            row['Timestamp'] < (row['dateTo'] + datetime.timedelta(days=1)))
-        return str(j_row)
-
-    data_frame['InDateRange'] = data_frame.apply(in_range, axis=1)
-    data_frame = data_frame[data_frame['InDateRange'] == 'True']
-
-    if is_local_time:
-        def to_utc(row):
-            if row['offsetToUtcMinutes'] >= 0:
-                j_row = row['TimestampLocal'] - datetime.timedelta(minutes=row['offsetToUtcMinutes'])
-            else:
-                j_row = row['TimestampLocal'] + datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
-            return j_row
-
-        data_frame = data_frame.assign(TimestampLocal=data_frame['Timestamp'])
-        data_frame['Timestamp'] = data_frame.apply(to_utc, axis=1)
-    elif not is_local_time:
-        def from_utc(row):
-            if row['offsetToUtcMinutes'] >= 0:
-                j_row = row['Timestamp'] + datetime.timedelta(minutes=row['offsetToUtcMinutes'])
-            else:
-                j_row = row['Timestamp'] - datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
-            return j_row
-
-        data_frame['TimestampLocal'] = data_frame.apply(from_utc, axis=1)
-
-    def bin_to_15_minute_interval(row, date_col):
-        if row[date_col].minute < 15:
-            j_row = row[date_col].replace(minute=0)
-            return j_row
-        elif row[date_col].minute >= 15 and row[date_col].minute < 30:
-            j_row = row[date_col].replace(minute=15)
-            return j_row
-        elif row[date_col].minute >= 30 and row[date_col].minute < 45:
-            j_row = row[date_col].replace(minute=30)
-            return j_row
-        else:
-            j_row = row[date_col].replace(minute=45)
-            return j_row
-
-    data_frame['TimestampId'] = data_frame.apply(bin_to_15_minute_interval, args=('Timestamp',), axis=1)
-    data_frame['TimestampLocalId'] = data_frame.apply(bin_to_15_minute_interval, args=('TimestampLocal',), axis=1)
-    data_frame = data_frame.drop(columns=['InDateRange', 'dateFrom', 'dateTo', 'installationId', 'offsetToUtcMinutes'])
-
-    # payload = {}
-    headers = api_inputs.api_headers.integration
-
-    logger.info("Upserting data to Timeseries_Ds.")
-
-    data_frame = data_frame.loc[:, ['ObjectPropertyId', 'Timestamp', 'TimestampId', 'TimestampLocal',
-                                    'TimestampLocalId', 'Value', 'DataFeedFileStatusId', 'InstallationId', 'Meta']]
-    name = 'Timeseries_Ds'
-
-    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=name, batch_id=data_feed_file_status_id)
-
-    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "upsert",
-                    "isLocalTime": is_local_time,
-                    "tableDef": _get_structure(data_frame),
-                    "keyColumns": ["ObjectPropertyId", "Timestamp"]
-                    }
-
-    # {'ObjectPropertyId': 'object', 'Timestamp': 'datetime64[ns]', 'Value': 'float64', 'InstallationId': 'object'}
-
-    # ObjectPropertyId: string, Timestamp: datetime, TimestampId: datetime, TimestampLocal: datetime,
-    # TimestampLocalId: datetime, Value: real, DataFeedFileStatusId: string, InstallationId: string, Meta: dynamic
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/time-series-operation?originalName=" \
-          f"{name}"
-    logger.info("Sending request: POST %s", url)
-
-    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-    elif response.status_code == 200:
-        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
-
-    response_df = pandas.read_json(response.text, typ='series').to_frame().T
-    response_df.columns = _column_name_cap(columns=response_df.columns)
-
-    logger.info("Response status: %s", response_status)
-    logger.info("Ingestion complete. ")
-    return response_status, response_df
-
-
-@_with_func_attrs(df_required_columns=['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value'])
-def upsert_timeseries(df: pandas.DataFrame, api_inputs: ApiInputs, is_local_time: bool = True,
-                      save_additional_columns_as_slices: bool = False, data_feed_file_status_id: uuid.UUID = None, 
-                      is_specific_timezone: Union[bool, str] = False, ingestion_mode: IngestionMode = 'Queue',
-                    send_notification: bool = False):
-    """Upserts timeseries to EventHub for processing.
-
-    The following columns are required to be present in the data_frame:
-    - InstallationId
-    - ObjectPropertyId
-    - Timestamp
-    - Value
-
-    Parameters
-    ----------
-    df: pandas.DataFrame
-        Dataframe containing the data to be appended to timeseries.
-    api_inputs: ApiInputs
-        Object returned by initialize() function.
-    is_local_time : bool, default = True
-         Whether the datetime values are in local time or UTC. If, False and is_specific_timezone is False, then UTC (Default value = True).
-         Should be set to False when 'is_specific_timezone' has value.
-    save_additional_columns_as_slices : bool, default = False
-         (Default value = False)
-    data_feed_file_status_id : uuid.UUID, default = None
-         Enables developer to identify upserted rows using during development. This data is posted to the
-         DataFeedFileStatusId in the Timeseries_Ds table.
-
-         Once deployed, the DataFeedFileStatusId field will contain a unique Guid which will assist in
-         tracking upload results and logging.
-    is_specific_timezone : Union[False, str]
-        Accepts a timezone name as the specific timezone used by the source data. Defaults to False.
-        Cannot have value if 'is_local_time' is set to True.
-        Retrieve list of timezones using 'sw.integration.get_timezones()'
-    send_notification : bool
-        This enables Iq Notification messages to be sent when set to `True`
-        Default value = False
-
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
-        text.
-
-    """
-
-    data_frame = df.copy()
-    timezones_df = pd.DataFrame()
-    logger.info(f'Data frame size: {convert_bytes(data_frame.memory_usage(deep=True).sum())}')
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if not isinstance(is_specific_timezone, str) and not isinstance(is_specific_timezone, bool):
-        logger.error("'is_specific_timezone' parameter can only be of type str or bool.")
-        return pandas.DataFrame()
-
-    if is_specific_timezone != False and is_local_time == True:
-        logger.error("Assigning specific timezone is only possible if 'is_local_time' is set to False.")
-        return pandas.DataFrame()
-
-    if is_specific_timezone == True:
-        logger.error("'is_specific_timezone' parameter value, if not False, should be a valid timezone. Retrieve list of timezones using 'sw.integration.get_timezones()")
-        return pandas.DataFrame()
-
-    if is_specific_timezone == '':
-        logger.error("'is_specific_timezone' cannot be an empty string. Retrieve list of timezones using 'sw.integration.get_timezones()")
-        return pandas.DataFrame()
-
-    is_specific_timezone_conversion = False
-    if is_specific_timezone != False and (isinstance(is_specific_timezone, str) and is_specific_timezone != ''):
-        timezone_name_df = get_timezones(api_inputs=api_inputs)
-
-        if timezone_name_df is None or timezone_name_df.__len__() == 0:
-            logger.error("Failed to retrieve the allowed Timezone names.")
-            return pandas.DataFrame()
-        
-        if not is_specific_timezone in timezone_name_df.TimezoneName.values:
-            logger.error(f"'is_specific_timezone' parameter value '{is_specific_timezone}' is not found in Timezone Names list.")
-            logger.info(f"'Timezone name list can be retrieved using 'sw.integration.get_timezones()' method.")
-            return pandas.DataFrame()
-        
-        is_specific_timezone_conversion = True
-
-    required_columns = ['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value']
-    req_cols = ', '.join(required_columns)
-    proposed_columns = data_frame.columns.tolist()
-
-    if not set(required_columns).issubset(proposed_columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        return 'integration.upsert_timeseries() - data_frame must contain the following columns: ' + req_cols
-
-    if not pandas.api.types.is_datetime64_ns_dtype(data_frame['Timestamp']):
-        logger.exception("Timestamp series value is not of type: datetime64[ns] dtype.")
-        return "Timestamp series value is not of type: datetime64[ns] dtype."
-
-    null_count_timestamp = data_frame['Timestamp'].isna().sum()
-    if null_count_timestamp > 0:
-        logger.warning(f"There are {null_count_timestamp} records with a null or empty Timestamp value. " \
-            f"These are being dropped resulting in {data_frame.shape[0] - null_count_timestamp} records will be upserted.")
-        data_frame = data_frame.dropna(subset=['Timestamp'])
-
-    slice_columns = set(proposed_columns).difference(set(required_columns))
-    slice_columns = list(slice_columns)
-
-    if len(slice_columns) > 0 and save_additional_columns_as_slices is True:
-        def update_values(row):
-            j_row = row[slice_columns].to_json()
-            return str(j_row)
-
-        logger.info('Creating Meta Column.')
-        data_frame['Meta'] = data_frame[slice_columns].assign(
-            **data_frame[slice_columns].select_dtypes(['datetime', 'object']).astype(str)).apply(update_values, axis=1)
-
-        data_frame = data_frame.drop(columns=slice_columns)
-    elif len(slice_columns) > 0 and save_additional_columns_as_slices is not True:
-        data_frame = data_frame.drop(columns=slice_columns)
-        data_frame['Meta'] = ''
-    else:
-        data_frame['Meta'] = ''
-
-    if api_inputs.data_feed_file_status_id is not None and \
-            api_inputs.data_feed_file_status_id != '00000000-0000-0000-0000-000000000000' and \
-            api_inputs.data_feed_file_status_id != '':
-        data_frame['DataFeedFileStatusId'] = api_inputs.data_feed_file_status_id
-        data_feed_file_status_id = api_inputs.data_feed_file_status_id
-    elif data_feed_file_status_id is not None and \
-            data_feed_file_status_id != '00000000-0000-0000-0000-000000000000' and \
-            data_feed_file_status_id != '':
-        data_frame['DataFeedFileStatusId'] = data_feed_file_status_id
-    else:
-        data_feed_file_status_id = uuid.uuid4()
-        data_frame['DataFeedFileStatusId'] = data_feed_file_status_id
-
-    
-    start_date = data_frame['Timestamp'].min(axis=0, skipna=True)
-    end_date = data_frame['Timestamp'].max(axis=0, skipna=True)
-
-    specific_timezone_df = pd.DataFrame()
-
-    def convert_dst_interval_dates(row):
-        row['start'] = pd.to_datetime(row['start'])
-        row['end'] = pd.to_datetime(row['end'])
-        row['offsetToUtcMinutes'] = row['standardOffsetUtc'] + row['dstOffsetUtc']
-        return row
-
-    if is_specific_timezone_conversion:
-        def timezone_to_utc(row):
-            if row['offsetToUtcMinutes'] >= 0:
-                j_row = row['Timestamp'] - datetime.timedelta(minutes=row['offsetToUtcMinutes'])
-            else:
-                j_row = row['Timestamp'] + datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
-            return j_row
-
-        specific_timezone_df = _timezone_dst_offsets(api_inputs=api_inputs, date_from=start_date.date(), date_to=end_date.date(), timezone_name=is_specific_timezone)
-
-        if specific_timezone_df.empty:
-            logger.exception('Failed to retrieve the specific timezone offsets.')
-            return 'Failed to retrieve the specific timezone offsets.'
-
-        specific_timezone_df = specific_timezone_df.apply(convert_dst_interval_dates, axis=1)
-        specific_timezone_df = specific_timezone_df.drop(columns=['timezoneId', 'standardOffsetUtc', 'dstOffsetUtc'])
-        
-        data_frame['year'] = pd.DatetimeIndex(data_frame['Timestamp']).year
-        data_frame = data_frame.merge(specific_timezone_df, left_on='year', right_on='year', how='inner')
-
-        del specific_timezone_df
-        gc.collect()
-
-        data_frame = data_frame[(data_frame.Timestamp >= data_frame.start) & (data_frame.Timestamp < data_frame.end) & (data_frame.Timestamp.apply(lambda x: x.year) == data_frame.year)]
-        data_frame = data_frame.drop(columns=['start', 'end', 'year'])
-        
-        data_frame['Timestamp'] = data_frame.apply(timezone_to_utc, axis=1)
-        data_frame = data_frame.drop(columns=['offsetToUtcMinutes'])
-
-        # Set automatically to is_local_time = False 
-        is_local_time = False
-
-    site_list = data_frame['InstallationId'].unique().tolist()
-    timezones_df = _timezone_dst_offsets(api_inputs=api_inputs, date_from=start_date.date(), date_to=end_date.date(),
-                                         installation_id_list=site_list)
-
-    if timezones_df.empty:
-        logger.exception('Timezone DST offsets failed to retrieve.')
-        return 'Timezone DST offsets failed to retrieve.'
-
-    timezones_df = timezones_df.apply(convert_dst_interval_dates, axis=1)
-
-    if not is_local_time:
-        def convert_dst_interval_range_to_utc(row):
-            dst_start = row['start']
-            dst_end = row['end']
-
-            if row['standardOffsetUtc'] >= 0:
-                dst_start = dst_start - (datetime.timedelta(minutes=row['standardOffsetUtc'])) - datetime.timedelta(minutes=row['dstOffsetUtc'])
-                dst_end = dst_end - (datetime.timedelta(minutes=row['standardOffsetUtc'])) - datetime.timedelta(minutes=row['dstOffsetUtc'])
-            else:
-                dst_start = dst_start + (datetime.timedelta(minutes=row['standardOffsetUtc'])) + datetime.timedelta(minutes=row['dstOffsetUtc'])
-                dst_end = dst_end + (datetime.timedelta(minutes=row['standardOffsetUtc'])) + datetime.timedelta(minutes=row['dstOffsetUtc'])
-            row['start'] = dst_start
-            row['end'] = dst_end
-
-            return row
-
-        timezones_df = timezones_df.apply(convert_dst_interval_range_to_utc, axis=1)
-    timezones_df = timezones_df.drop(columns=['timezoneId', 'standardOffsetUtc', 'dstOffsetUtc'])
-
-    logger.info('Merging data frame input with timezone installation dst offsets.')
-    data_frame = data_frame.merge(timezones_df, left_on='InstallationId', right_on='installationId', how='inner')
-    
-    del timezones_df
-    gc.collect()
-
-    data_frame = data_frame[(data_frame.Timestamp >= data_frame.start) & (data_frame.Timestamp < data_frame.end) & (data_frame.Timestamp.apply(lambda x: x.year) == data_frame.year)]
-    data_frame = data_frame.drop(columns=['start', 'end', 'year'])
-
-    if is_local_time:
-        def to_utc(row):
-            if row['offsetToUtcMinutes'] >= 0:
-                j_row = row['TimestampLocal'] - datetime.timedelta(minutes=row['offsetToUtcMinutes'])
-            else:
-                j_row = row['TimestampLocal'] + datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
-            return j_row
-
-        data_frame = data_frame.assign(TimestampLocal=data_frame['Timestamp'])
-        data_frame['Timestamp'] = data_frame.apply(to_utc, axis=1)
-
-    elif not is_local_time:
-        def from_utc(row):
-            if row['offsetToUtcMinutes'] >= 0:
-                j_row = row['Timestamp'] + datetime.timedelta(minutes=row['offsetToUtcMinutes'])
-            else:
-                j_row = row['Timestamp'] - datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
-            return j_row
-        data_frame['TimestampLocal'] = data_frame.apply(from_utc, axis=1)
-
-    def bin_to_15_minute_interval(row, date_col):
-        if row[date_col].minute < 15:
-            j_row = row[date_col].replace(minute=0)
-            return j_row
-        elif row[date_col].minute >= 15 and row[date_col].minute < 30:
-            j_row = row[date_col].replace(minute=15)
-            return j_row
-        elif row[date_col].minute >= 30 and row[date_col].minute < 45:
-            j_row = row[date_col].replace(minute=30)
-            return j_row
-        else:
-            j_row = row[date_col].replace(minute=45)
-            return j_row
-
-    logger.info('Date time conversion and formatting.')
-    data_frame['TimestampId'] = data_frame.apply(bin_to_15_minute_interval, args=('Timestamp',), axis=1)
-    data_frame['TimestampLocalId'] = data_frame.apply(bin_to_15_minute_interval, args=('TimestampLocal',), axis=1)
-
-    timestamp_format = "%Y-%m-%dT%H:%M:%SZ"
-    timestamp_normalized_format = "%Y-%m-%dT%H:%M:00Z"
-    data_frame['Timestamp'] = data_frame['Timestamp'].dt.strftime(timestamp_format)
-    data_frame['TimestampLocal'] = data_frame['TimestampLocal'].dt.strftime(timestamp_format)
-    data_frame['TimestampId'] = data_frame['TimestampId'].dt.strftime(timestamp_normalized_format)
-    data_frame['TimestampLocalId'] = data_frame['TimestampLocalId'].dt.strftime(timestamp_normalized_format)
-
-    headers = api_inputs.api_headers.integration
-
-    logger.info("Upserting data to Timeseries.")
-    data_frame = data_frame.loc[:, ['ObjectPropertyId', 'Timestamp', 'TimestampId', 'TimestampLocal',
-                                    'TimestampLocalId', 'Value', 'DataFeedFileStatusId', 'InstallationId', 'Meta']]
-
-    container = 'data-ingestion-timeseries-adx'
-    folder = 'to-adx-stream' if ingestion_mode == 'Stream' else 'to-eventhub'
-    name = 'Timeseries'
-    
-    logger.info(f'Data frame size: {convert_bytes(data_frame.memory_usage(deep=True).sum())}')
-    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, container=container, folder=folder, name=name, batch_id=data_feed_file_status_id, include_header=True)
-
-    sensor_result = None, 0
-
-    if (send_notification == True):
-        # send IQ notification, process sensor list with latest property value
-        def _upload_sensor_latest_value(api_input, data_frame, data_feed_file_status_id, container):
-
-            folder = 'to-iq-notifications'
-            name = 'Sensors'
-
-            try:
-                data_frame['Timestamp'] = pd.to_datetime(data_frame['Timestamp'])  # Convert Timestamp column to datetime
-                latest_indices = data_frame.groupby('ObjectPropertyId')['Timestamp'].idxmax()
-                latest_sensor_value_df = data_frame.loc[latest_indices, ['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value']]
-
-                logger.info(f'Latest sensors Data frame size: {convert_bytes(latest_sensor_value_df.memory_usage(deep=True).sum())}')
-                latest_sensor_upload_result = Blob.upload(api_inputs=api_inputs, 
-                                                        data_frame=latest_sensor_value_df, 
-                                                        container=container, 
-                                                        folder=folder, 
-                                                        name=name, 
-                                                        batch_id=data_feed_file_status_id, 
-                                                        include_header=True
-                                                    )
-                return True, latest_sensor_upload_result
-            except Exception as e:
-                return False, None
-
-        is_success, sensor_result = _upload_sensor_latest_value(api_input=api_inputs, data_frame=data_frame,
-                                                                   data_feed_file_status_id=data_feed_file_status_id,
-                                                                   container=container)
-        
-        if (is_success == False):
-            logger.exception("Uploading of sensors' latest property value for live notifications failed.")
-            sensor_result = None, 0
-
-    json_payload = {
-        "path": upload_result[0], 
-        "fileCount": upload_result[1], 
-        "ingestionMode": ingestion_mode,
-        "sensorPath": sensor_result[0],
-        "sensorFileCount": sensor_result[1],
-        }
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/timeseries"
-    logger.info("Sending request: POST %s", url)
-
-    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
-
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-    elif response.status_code == 200:
-        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
-
-    response_df = pandas.read_json(response.text, typ='series').to_frame().T
-    response_df.columns = _column_name_cap(columns=response_df.columns)
-
-    logger.info("Response status: %s", response_status)
-    logger.info("Ingestion complete. ")
-    return response_status, response_df
-
-
-def upsert_data(data_frame, api_inputs: ApiInputs, table_name: str, key_columns: list,
-                is_slices_table=False, ingestion_mode: IngestionMode = 'Queue', table_def: dict[str, ADX_TABLE_DEF_TYPES] = None):
-    """Upsert data
-
-    Upserts data to the `table_name` provided to the function call and uses the `key_columns` provided to determine the
-    unique records.
-
-    Parameters
-    ----------
-    data_frame : pandas.DataFrame
-        Dataframe containing the data to be upserted.
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    table_name : str
-        The name of the table where data will be upserted.
-    key_columns : list
-        The columns that determine a unique instance of a record. These are used to update records if new data is
-        provided.
-    is_slices_table : bool
-         (Default value = False)
-    inegstion_mode : IngestionMode
-        (Default value = 'Queue') The type of ingestion to use.
-    table_def : dict[str, ADX_TABLE_DEF_TYPES]
-        (Default value = None) An optional table definition which will be merged to the inferred table structure based from data_frame.
-
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
-        text.
-
-    """
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.integration
-
-    if len(key_columns) == 0 or key_columns is None:
-        logger.error(
-            "You must provide key_columns. This allows the Switch Automation Platform to identify the rows to update.")
-        return False
-
-    logger.info("Data is being upserted for %s", table_name)
-
-    table_structure = _get_structure(data_frame)
-    table_structure = {**table_structure, **table_def} if table_def is not None else table_structure 
-    
-    if is_slices_table is True and 'Slices' in table_structure:
-        table_structure['Slices'] = 'dynamic'
-
-    # upload Blobs to folder
-    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=table_name)
-    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "upsert",
-                    "tableDef": table_structure, "keyColumns": key_columns, 'ingestionMode': ingestion_mode}
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data-operation?tableName={table_name}"
-    logger.info("Sending request: POST %s", url)
-
-    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
-
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-    elif response.status_code == 200:
-        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
-
-    response_df = pandas.read_json(response.text, typ='series').to_frame().T
-    response_df.columns = _column_name_cap(columns=response_df.columns)
-
-    logger.info("Response status: %s", response_status)
-    logger.info("Ingestion complete. ")
-
-    return response_status, response_df
-
-
-def replace_data(data_frame, api_inputs: ApiInputs, table_name: str, table_def: dict[str, ADX_TABLE_DEF_TYPES] = None):
-    """Replace data
-
-    Replaces the data in the ``table_name`` provided to the function call.
-
-    Parameters
-    ----------
-    data_frame : pandas.DataFrame
-        Data frame to be used to replace the data in the `table_name` table.
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    table_name :
-        The name of the table where data will be replaced.
-    table_def : dict[str, ADX_TABLE_DEF_TYPES]
-        (Default value = None) An optional table definition which will be merged to the inferred table structure based from data_frame.
-
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
-        text.
-
-    """
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    headers = api_inputs.api_headers.integration
-
-    logger.info("Replacing all data for %s", table_name)
-
-    table_structure = _get_structure(data_frame)
-    table_structure = {**table_structure, **table_def} if table_def is not None else table_structure
-
-    # upload Blobs to folder
-    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=table_name)
-
-    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "replace",
-                    "tableDef": table_structure}
-    logger.info(json_payload)
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data-operation?tableName={table_name}"
-    logger.info("Sending request: POST %s", url)
-
-    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
-
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, pandas.DataFrame()
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status, pandas.DataFrame()
-    elif response.status_code == 200:
-        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
-
-    response_df = pandas.read_json(response.text, typ='series').to_frame().T
-    response_df.columns = _column_name_cap(columns=response_df.columns)
-
-    logger.info("Ingestion complete. ")
-
-    return response_status, response_df
-
-
-def append_data(data_frame, api_inputs: ApiInputs, table_name: str, table_def: dict[str, ADX_TABLE_DEF_TYPES] = None):
-    """Append data.
-
-    Appends data to the ``table_name`` provided to the function call.
-
-    Parameters
-    ----------
-    data_frame : pandas.DataFrame
-        Data to be appended.
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    table_name : str
-        The name of the table where data will be appended.
-    table_def : dict[str, ADX_TABLE_DEF_TYPES]
-        (Default value = None) An optional table definition which will be merged to the inferred table structure based from data_frame.
-        
-    Returns
-    -------
-    tuple[str, pandas.DataFrame]
-        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
-        text.
-
-    """
-    # payload = {}
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    # payload = {}
-    headers = api_inputs.api_headers.integration
-
-    logger.info("Appending data for %s", table_name)
-
-    table_structure = _get_structure(data_frame)
-    table_structure = {**table_structure, **table_def} if table_def is not None else table_structure 
-
-    # upload Blobs to folder   
-    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=table_name)
-    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "append",
-                    "tableDef": table_structure}
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data-operation?tableName={table_name}"
-    logger.info("Sending request: POST %s", url)
-    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
-
-    response = requests.post(url, json=json_payload, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status
-    elif response.status_code == 200:
-        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
-
-    response_df = pandas.read_json(response.text, typ='series').to_frame().T
-    response_df.columns = _column_name_cap(columns=response_df.columns)
-
-    logger.info("API Response: %s", response_status)
-    logger.info("Ingestion complete. ")
-
-    return response_status, response_df
-
-
-def upsert_file_row_count(api_inputs: ApiInputs, row_count: int):
-    """Updates data feed file status row count.
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    row_count : number
-        Number of rows
-
-    Returns
-    -------
-    str
-        Response status as a string.
-    """
-
-    if row_count is None:
-        row_count = 0
-
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    if (api_inputs.data_feed_id == '00000000-0000-0000-0000-000000000000' or
-            api_inputs.data_feed_file_status_id == '00000000-0000-0000-0000-000000000000'):
-        logger.error("upsert_file_row_count() can only be called in Production.")
-        return False
-
-    headers = api_inputs.api_headers.default
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
-          f"{api_inputs.data_feed_id}/file-status/{api_inputs.data_feed_file_status_id}/row-count/{row_count}"
-
-    response = requests.request("PUT", url, timeout=20, headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-        return response_status
-
-    return response_status
-
-
-def upsert_event_work_order_id(api_inputs: ApiInputs, event_task_id: uuid.UUID, integration_id: str,
-                               work_order_status: str):
-    """
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    event_task_id : uuid.UUID
-        The value of the `work_order_input['EventTaskId']`
-    integration_id : str
-        The 3rd Party work order system's unique identifier for the work order
-    work_order_status : str
-        The status of the work order
-
-    Returns
-    -------
-    (str, str)
-        response_status, response.text - The response status of the call and the text from the response body.
-
-    """
-
-    if api_inputs.api_projects_endpoint == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return 'Error', 'You must call initialize() before using the API.'
-
-    header = api_inputs.api_headers.default
-
-    payload = {
-        "IntegrationId": integration_id,
-        "Status": work_order_status
-    }
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/events/{str(event_task_id)}/work-order"
-
-    response = requests.put(url=url, json=payload, headers=header)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                     response.reason)
-        return response_status, response.text
-
-    return response_status, response.text
-
-
-@_with_func_attrs(df_required_columns=['Identifier'])
-def upsert_tags(api_inputs: ApiInputs, df: pandas.DataFrame, tag_level: TAG_LEVEL):
-    """
-    Upsert tags to Site/Device/Sensors as specified by the tag_level argument.
-    
-    Required fields are:
-    - Identifier
-    - Additional columns as TagGroups / Tags
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    df : DataFrame
-        List of Devices along with corresponding TagsJson to upsert
-    tag_level : TAG_LEVEL
-        Level of tagging applied to the list of Identifier input.
-            If tag_level is Site, Identifier should be InstallationIds.
-            If tag_level is Device, Identifier should be DeviceIds.
-            If tag_level is Sensor, Identifier should be ObjectPropertyIds.
-
-    Returns
-    -------
-    List of affected records
-    
-    """
-    data_frame = df.copy()
-
-    # validate inputs
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    # validate df
-    if data_frame.empty:
-        logger.error("Data Frame is empty. Nothing to upsert.")
-        return pandas.DataFrame()
-
-    # validate tag level
-    if not set([tag_level]).issubset(set(TAG_LEVEL.__args__)):
-        logger.error('tag_level parameter must be set to one of the allowed values defined by the '
-                        'TAG_LEVEL literal: %s', TAG_LEVEL.__args__)
-        return pandas.DataFrame()
-
-    required_columns = getattr(upsert_tags, 'df_required_columns')
-    req_cols = ', '.join(required_columns)
-    proposed_columns = data_frame.columns.tolist()
-
-    # check if required columns are present
-    if not set(required_columns).issubset(proposed_columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        return 'integration.upsert_tags() - data_frame must contain the following columns: ' + req_cols
-
-    slice_columns = set(proposed_columns).difference(set(required_columns))
-
-    if len(list(slice_columns)) > 0:
-        def update_values(row):
-            j_row = row[list(slice_columns)].to_json()
-            return str(j_row)
-
-        data_frame['TagsJson'] = data_frame[list(slice_columns)].assign(
-            **data_frame[list(slice_columns)].select_dtypes(['datetime', 'object']).astype(str)).apply(update_values, axis=1)
-        data_frame = data_frame.drop(columns=list(slice_columns))
-    else:
-        logger.error("Additional Columns aside from Identifier is required to set as TagsJson.")
-        return pandas.DataFrame()
-
-    switcher_tag_level_identifier = {
-        'Site': 'InstallationId',
-        'Device': 'DeviceId',
-        'Sensor': 'ObjectPropertyId'
-    }
-
-    tag_level_identifier_id = switcher_tag_level_identifier.get(tag_level)
-
-    data_frame.rename(columns = {'Identifier':tag_level_identifier_id}, inplace = True)
-
-    # action request
-    headers = api_inputs.api_headers.integration
-
-    switcher_tag_level_uri = {
-        'Site': f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/installations/upsert-ingestion-tags",
-        'Device': f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion-tags",
-        'Sensor': f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/sensors/upsert-ingestion-tags",
-    }
-
-    url = switcher_tag_level_uri.get(tag_level)
-
-    logger.info("Sending request: POST %s", url)
-    response = requests.post(url, data=data_frame.to_json(orient='records'), headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-
-    logger.info("Response status: %s", response_status)
-
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                        response.reason)
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-
-    response_df = pandas.read_json(response.text)
-    response_df.columns = _column_name_cap(response_df.columns)
-
-    return response_df
-
-
-@_with_func_attrs(df_required_columns=['DeviceId'])
-def upsert_device_metadata(api_inputs: ApiInputs, df: pandas.DataFrame):
-    """
-    Upsert metadata on created devices.
-
-    Required fields are:
-    - DeviceId
-    - Additional columns as Metadata
-
-    Parameters
-    ----------
-    api_inputs : ApiInputs
-        Object returned by initialize() function.
-    df : DataFrame
-        List of Devices along with corresponding MetadataJson to upsert
-
-    Returns
-    -------
-    List of affected Devices
-    """
-    data_frame = df.copy()
-
-    # validate inputs
-    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-        logger.error("You must call initialize() before using API.")
-        return pandas.DataFrame()
-
-    # validate df
-    if data_frame.empty:
-        logger.error("Data Frame is empty. Nothing to upsert.")
-        return pandas.DataFrame()
-
-    required_columns = getattr(upsert_device_metadata, 'df_required_columns')
-
-    req_cols = ', '.join(required_columns)
-    proposed_columns = data_frame.columns.tolist()
-
-    # check if required columns are present
-    if not set(required_columns).issubset(proposed_columns):
-        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
-        return 'integration.upsert_device_metadata() - data_frame must contain the following columns: ' + req_cols
-
-    slice_columns = set(proposed_columns).difference(set(required_columns))
-
-    if len(list(slice_columns)) > 0:
-        def update_values(row):
-            j_row = row[list(slice_columns)].to_json()
-            return str(j_row)
-
-        data_frame['MetadataJson'] = data_frame[list(slice_columns)].assign(
-            **data_frame[list(slice_columns)].select_dtypes(['datetime', 'object']).astype(str)).apply(update_values, axis=1)
-        data_frame = data_frame.drop(columns=list(slice_columns))
-    else:
-        logger.error("Additional Columns aside from DeviceId is required to set as MetadataJson.")
-        return pandas.DataFrame()
-
-    # action request
-    headers = api_inputs.api_headers.integration
-
-    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion-metadata"
-
-    logger.info("Sending request: POST %s", url)
-    response = requests.post(url, data=data_frame.to_json(orient='records'), headers=headers)
-    response_status = '{} {}'.format(response.status_code, response.reason)
-
-    logger.info("Response status: %s", response_status)
-
-    if response.status_code != 200:
-        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                        response.reason)
-    elif len(response.text) == 0:
-        logger.error('No data returned for this API call. %s', response.request.url)
-
-    response_df = pandas.read_json(response.text)
-    response_df.columns = _column_name_cap(response_df.columns)
-
-    return response_df
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for integrating asset creation, asset updates, data ingestion, etc into the Switch Automation Platform.
+"""
+import gc
+import re
+import sys
+from typing import Union
+import pandas
+import pandas as pd
+from ..integration.helpers import get_timezones
+import pandera
+import json
+import requests
+import datetime
+import logging
+import uuid
+from .._utils._platform import _get_structure, Blob
+from .._utils._utils import (ApiInputs, _with_func_attrs, _column_name_cap, _work_order_schema, convert_bytes, IngestionMode)
+from .._utils._constants import TAG_LEVEL
+from ..integration._utils import _timezone_offsets, _upsert_entities_affected_count, _adx_support, _timezone_dst_offsets
+from .._utils._constants import ADX_TABLE_DEF_TYPES
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(stream=sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+@_with_func_attrs(df_required_columns=['ApiProjectId', 'InstallationId', 'NetworkDeviceId', 'UserId', 'BatchNo',
+                                       'DriverUniqueId', 'Timestamp', 'DiscoveredValue', 'DriverClassName', 'JobId'],
+                  df_optional_columns=['DriverDeviceType', 'ObjectPropertyTemplateName', 'UnitOfMeasureAbbrev',
+                                       'DeviceName', 'DisplayName', 'EquipmentType', 'EquipmentLabel', ])
+def upsert_discovered_records(df: pandas.DataFrame, api_inputs: ApiInputs, discovery_properties_columns: list,
+                              device_tag_columns: list = None, sensor_tag_columns: list = None,
+                              metadata_columns: list = None, override_existing: bool = False):  # , ontology_tag_columns: list = None):
+    """Upsert discovered records to populate Build - Discovery & Selection UI.
+
+    Parameters
+    ----------
+    df : pandas.DataFrame
+        The dataframe containing the discovered records including the minimum required set of columns.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    discovery_properties_columns : list
+        List of the discovery property columns returned by 3rd party API.
+    device_tag_columns : list, default = None
+        List of columns that represent device-level tag group(s) (Default value = None)
+    sensor_tag_columns : list, default = None
+        List of column names in input `df` that represent sensor-level tag group(s) (Default value = None).
+    metadata_columns : list, default = None
+        List of column names in input `df` that represent device-level metadata key(s) (Default value = None).
+    override_existing : bool, default = False
+        Flag if it the values passed to df will override existing integration records. Only valid if running locally,
+        not on a deployed task where it is triggered via UI.
+    # ontology_tag_columns : list, default = None
+    #     List of BRICK schema or Haystack tags that apply to a given point (Default value = None).
+
+    Returns
+    -------
+    tuple[pandas.DataFrame, pandas.DataFrame]
+        (response_df, errors_df) - Returns the response dataframe and the dataframe containing the parsed errors text
+        (if no errors, then empty dataframe).
+
+    """
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    data_frame = df.copy()
+
+    required_columns = ['ApiProjectId', 'InstallationId', 'NetworkDeviceId', 'UserId', 'BatchNo', 'DriverUniqueId',
+                        'Timestamp', 'DiscoveredValue', 'DriverClassName', 'JobId']
+    optional_columns = ['DriverDeviceType', 'ObjectPropertyTemplateName', 'UnitOfMeasureAbbrev', 'DeviceName',
+                        'DisplayName', 'EquipmentType', 'EquipmentLabel', ]
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(proposed_columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        error= 'integration.upsert_discovered_records() - df must contain the following columns: ' + ', '.join(
+            required_columns) + '. Optional Columns include: ' + ', '.join(optional_columns)
+        return error
+
+    missing_optional_columns = set(optional_columns) - set(proposed_columns)
+    for missing_column in missing_optional_columns:
+        data_frame[missing_column] = None
+
+    if discovery_properties_columns is not None and not set(discovery_properties_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected discovery property column(s): %s',
+                         set(discovery_properties_columns).difference(proposed_columns))
+        error = 'Integration.upsert_discovered_records(): df must contain the following discovery property column(s): ' + \
+               ', '.join(discovery_properties_columns)
+        return error
+    elif discovery_properties_columns is None:
+        logger.exception('Missing expected discovery property column(s): %s',
+                         set(discovery_properties_columns).difference(proposed_columns))
+        error= 'Integration.upsert_discovered_records(): df must contain the following discovery property' \
+               ' columns: ' + ', '.join(discovery_properties_columns)
+        return error
+
+    if device_tag_columns is not None and not set(device_tag_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected device tag column(s): %s',
+                         set(device_tag_columns).difference(proposed_columns))
+        error = 'Integration.upsert_discovered_records(): df expected to contain the following device tag ' \
+               'column(s): ' + ', '.join(device_tag_columns)
+        return error
+    elif device_tag_columns is None:
+        device_tag_columns = []
+
+    if sensor_tag_columns is not None and not set(sensor_tag_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected sensor tag column(s): %s',
+                         set(sensor_tag_columns).difference(proposed_columns))
+        error = 'Integration.upsert_discovered_records(): df expected to contain the following sensor tag ' \
+               'column(s): ' + ', '.join(sensor_tag_columns)
+        return error
+    elif sensor_tag_columns is None:
+        sensor_tag_columns = []
+
+    if metadata_columns is not None and not set(metadata_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected metadata column(s): %s',
+                         set(metadata_columns).difference(proposed_columns))
+        error = 'Integration.upsert_discovered_records(): df expected to contain the following metadata ' \
+               'column(s): ' + ', '.join(metadata_columns)
+        return error
+    elif metadata_columns is None:
+        metadata_columns = []
+
+    expected_cols = required_columns + optional_columns + discovery_properties_columns + device_tag_columns + sensor_tag_columns + metadata_columns
+    if set(expected_cols).symmetric_difference(data_frame.columns).__len__() != 0:
+        if not set(data_frame.columns).issubset(expected_cols):
+            logger.exception(f'Additional column(s) present in df outside those defined in the '
+                             f'integration.upsert_discovered_records.df_required_columns & '
+                             f'integration.upsert_discovered_records.df_optional_columns and those passed to the '
+                             f'discovery_properties_columns, device_tag_columns, sensor_tag_columns, '
+                             f'metadata_columns input arguments: {set(data_frame.columns).difference(expected_cols)}' )
+            error = (f'Integration.upsert_discovered_records(): df contains additional columns outside those '
+                    f'defined in the integration.upsert_discovered_records.df_required_columns &'
+                    f'integration.upsert_discovered_records.df_optional_columns and those defined in the '
+                    f'discovery_properties_columns, device_tag_columns, sensor_tag_columns,metadata_columns input '
+                    f'arguments. \nPlease remove the unexpected column(s) from the input df provided or update the other '
+                    f'input arguments to include the unexpected column(s). '
+                    f'\nThe unexpected columns are: {", ".join(set(data_frame.columns).difference(expected_cols))}.  ')
+            return error
+
+    # if ontology_tag_columns is not None and not set(ontology_tag_columns).issubset(data_frame.columns):
+    #     logger.exception('Missing expected ontology tag column(s): %s',
+    #                      set(ontology_tag_columns).difference(proposed_columns))
+    #     return 'Integration.upsert_discovered_records(): data_frame expected to contain the following ontology tag ' \
+    #            'column(s): ' + ', '.join(ontology_tag_columns)
+    # elif ontology_tag_columns is None:
+    #     ontology_tag_columns = []
+
+    # convert timestamp format to required format
+    data_frame.Timestamp = data_frame.Timestamp.dt.strftime("%Y-%m-%dT%H:%M:%S.%fZ")
+    data_frame = data_frame.rename(columns={'DiscoveredValue': 'CurrentValue'})
+
+    column_dict = {'DiscoveryProperties': discovery_properties_columns + ['Timestamp'],
+                   'DeviceTags': device_tag_columns,
+                   'SensorTags': sensor_tag_columns,
+                   'Metadata': metadata_columns,
+                   # 'OntologyTags': ontology_tag_columns
+                   }
+
+    def set_properties(raw_df: pandas.DataFrame, column_dict: dict):
+        for key, value in column_dict.items():
+
+            def update_values(row):
+                j_row = row[value].to_dict()
+                return j_row
+
+            if len(value) > 0:
+                raw_df[key] = raw_df.apply(update_values, axis=1)
+            else:
+                raw_df[key] = None
+
+        return raw_df
+
+    data_frame = set_properties(raw_df=data_frame, column_dict=column_dict)
+    data_frame = data_frame.drop(columns=discovery_properties_columns + ['Timestamp'] + device_tag_columns +
+                                 sensor_tag_columns + metadata_columns  # + ontology_tag_columns
+                                 )
+    data_frame = data_frame.assign(OntologyTags=None)
+
+    final_req_cols = ['ApiProjectId', 'InstallationId', 'NetworkDeviceId', 'DriverClassName', 'UserId', 'BatchNo',
+                      'DriverUniqueId', 'CurrentValue', 'DriverDeviceType', 'ObjectPropertyTemplateName',
+                      'UnitOfMeasureAbbrev', 'DeviceName', 'DisplayName', 'EquipmentType', 'EquipmentLabel',
+                      'DeviceTags', 'SensorTags', 'OntologyTags', 'Metadata', 'DiscoveryProperties', 'JobId']
+
+    if set(data_frame.columns.tolist()).issubset(final_req_cols):
+        batch_size = 50
+        chunk_list = []
+        payload_error_list = []
+        grouped_df = data_frame.reset_index(drop=True).groupby(by=lambda x: x // batch_size, axis=0)
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/integrations/driver-discovery"
+        headers = api_inputs.api_headers.default
+
+        logger.info(f"Input has been batched into {grouped_df.ngroups} group(s). ")
+
+        for name, group in grouped_df:
+            discovery_payload = group.groupby(
+                by=['ApiProjectId', 'InstallationId', 'NetworkDeviceId', 'DriverClassName', 'UserId', 'BatchNo', 'JobId']).apply(
+                lambda x: x[['DriverUniqueId', 'CurrentValue', 'DriverDeviceType', 'ObjectPropertyTemplateName',
+                             'UnitOfMeasureAbbrev', 'DeviceName', 'DisplayName', 'EquipmentType', 'EquipmentLabel',
+                             'DeviceTags', 'SensorTags', 'OntologyTags', 'Metadata', 'DiscoveryProperties']].to_dict(
+                    orient='records')).reset_index().rename(columns={0: 'Sensors'}).to_json(orient='records')
+
+            # remove outer [] from json
+            discovery_payload = re.sub(r"^[\[]", '', re.sub(r"[\]]$", '', discovery_payload))
+            # The code snippet is attempting to load a JSON string `discovery_payload` into a Python
+            # dictionary using the `json.loads()` function.
+            discovery_payload = json.loads(discovery_payload)
+            if api_inputs.data_feed_file_status_id != '00000000-0000-0000-0000-000000000000':
+                discovery_payload['IsOverride'] = False
+            else:
+                discovery_payload['IsOverride'] = override_existing
+            discovery_payload = json.dumps(discovery_payload)
+
+            logger.info(f"Upserting discovery record group {name} of {grouped_df.ngroups - 1}. ")
+            response = requests.post(url=url, headers=headers, data=discovery_payload)
+
+            response_status = '{} {}'.format(response.status_code, response.reason)
+            if response.status_code != 200 and len(response.text) > 0:
+                logger.error(f"API Call was not successful. Response Status: {response.status_code}. "
+                             f"Reason: {response.reason}. Error Text: {response.text}")
+                payload_error_list += [{'Chunk': name, 'Payload': discovery_payload}]
+                if response.text.startswith('{'):
+                    response_content = response.json()
+                    chunk_list += [{'Chunk': name, 'response_status': response.status_code,
+                                    'response_reason': response.reason, 'error_code': response_content['ErrorCode'],
+                                    'errors': response_content['Errors']}]
+                elif response.text.startswith('"'):
+                    chunk_list += [{'Chunk': name, 'response_status': response.status_code,
+                                    'response_reason': response.reason, 'errors': response.json()}]
+                else:
+                    chunk_list += [{'Chunk': name, 'response_status': response.status_code,
+                                    'response_reason': response.reason, 'errors': response.text}]
+            elif response.status_code != 200 and len(response.text) == 0:
+                logger.error(f"API Call was not successful. Response Status: {response.status_code}. "
+                             f"Reason: {response.reason}. ")
+                payload_error_list += [{'Chunk': name, 'Payload': discovery_payload}]
+                chunk_list += [{'Chunk': name, 'response_status': response.status_code,
+                                'response_reason': response.reason}]
+            elif response.status_code == 200:
+                logger.info(f"API Call was successful. ")
+                chunk_list += [{'Chunk': name, 'response_status': response.status_code,
+                                'response_reason': response.reason}]
+
+        upsert_response_df = pandas.DataFrame(chunk_list)
+
+        if 0 < len(payload_error_list) <= grouped_df.ngroups:
+            payload_error_df = pandas.DataFrame(payload_error_list)
+            logger.error(f"Errors on upsert of discovered records. ")
+            return upsert_response_df, payload_error_df
+
+    return upsert_response_df, pandas.DataFrame()
+
+
+@_with_func_attrs(df_required_columns=['InstallationCode', 'DeviceCode', 'DeviceName', 'SensorName', 'SensorTemplate',
+                                       'SensorUnitOfMeasure', 'EquipmentClass', 'EquipmentLabel'])
+def upsert_device_sensors(df: pandas.DataFrame, api_inputs: ApiInputs, tag_columns: list = None,
+                          metadata_columns: list = None, save_additional_columns_as_slices: bool = False):
+    """Upsert device(s) and sensor(s)
+
+    Required fields are:
+
+    - InstallationCode
+    - DeviceCode
+    - DeviceName
+    - SensorName
+    - SensorTemplate
+    - SensorUnitOfMeasure
+    - EquipmentClass
+    - EquipmentLabel
+
+    Parameters
+    ----------
+    df: pandas.DataFrame
+        The asset register created by the driver including the minimum required set of columns.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    tag_columns : list, default = None
+        Columns of dataframe that contain tags (Default value = None).
+    metadata_columns : list, default = None
+        Column(s) of dataframe that contain device-level metadata (Default value = None).
+    save_additional_columns_as_slices : bool, default = False
+        Whether additional columns should be saved as slices (Default value = False).
+
+    Returns
+    -------
+    tuple[list, pandas.DataFrame]
+        (response_status_list, upsert_response_df) - Returns the list of response statuses and the dataframe containing
+        the parsed response text.
+
+    """
+    pd.set_option('display.max_columns', 10)
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    data_frame = df.copy()
+
+    required_columns = ['InstallationCode', 'DeviceCode', 'DeviceName', 'SensorName', 'SensorTemplate',
+                        'SensorUnitOfMeasure', 'EquipmentClass', 'EquipmentLabel']
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(data_frame.columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        return 'Integration.upsert_device_sensors(): data_frame must contain the following columns: ' + ', '.join(
+            required_columns)
+
+    if tag_columns is not None and not set(tag_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected tag column(s): %s', set(tag_columns).difference(proposed_columns))
+        return 'Integration.upsert_device_sensors(): data_frame expected to contain the following tag column(s): ' + \
+               ', '.join(tag_columns)
+    elif tag_columns is None:
+        tag_columns = []
+
+    if metadata_columns is not None and not set(metadata_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected metadata column(s): %s', set(metadata_columns).difference(proposed_columns))
+        return 'Integration.upsert_device_sensors(): data_frame expected to contain the following metadata ' \
+               'column(s): ' + ', '.join(metadata_columns)
+    elif metadata_columns is None:
+        metadata_columns = []
+
+    slice_columns = set(proposed_columns).difference(set(required_columns)) - set(tag_columns) - set(metadata_columns)
+    slice_columns = list(slice_columns)
+    slices_data_frame = pandas.DataFrame()
+
+    if len(slice_columns) > 0 or len(tag_columns) > 0 or len(metadata_columns) > 0:
+        def update_values(row, mode):
+            if mode == 'A':
+                j_row = row[slice_columns].to_json()
+                if j_row == '{}':
+                    j_row = ''
+                return str(j_row)
+            elif mode == 'B':
+                j_row = row[tag_columns].to_json()
+                if j_row == '{}':
+                    j_row = ''
+                return str(j_row)
+            else:
+                j_row = row[metadata_columns].to_json()
+                if j_row == '{}':
+                    j_row = ''
+                return str(j_row)
+
+        data_frame['Slices'] = data_frame.apply(update_values, args="A", axis=1)
+        data_frame['TagsJson'] = data_frame.apply(update_values, args="B", axis=1)
+        data_frame['MetadataJson'] = data_frame.apply(update_values, args="C", axis=1)
+        data_frame = data_frame.drop(columns=slice_columns)
+        slices_data_frame = data_frame[['DeviceCode', 'Slices']]
+
+    headers = api_inputs.api_headers.integration
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion"
+
+    data_frame_grpd = data_frame.groupby(['InstallationCode', 'DeviceCode'])
+    chunk_list = []
+    for name, group in data_frame_grpd:
+        logger.info("Sending request: POST %s", url)
+        logger.info('Upserting data for InstallationCode = %s and DeviceCode = %s', str(name[0]), str(name[1]))
+        # logger.info('Sensor count to upsert: %s', str(group.shape[0]))
+
+        response = requests.post(url, data=group.to_json(orient='records'), headers=headers)
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        logger.info("Response status: %s", response_status)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1, 'SensorCountToUpsert': str(group.shape[0]),
+                            'response_status': response_status, 'response_df': pandas.DataFrame(),
+                            'invalid_rows': pandas.DataFrame()}]
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1, 'SensorCountToUpsert': str(group.shape[0]),
+                            'response_status': response_status, 'response_df': pandas.DataFrame(),
+                            'invalid_rows': pandas.DataFrame()}]
+        elif response.status_code == 200 and len(response.text) > 0:
+            response_data_frame = pandas.read_json(response.text)
+            logger.info('Dataframe response row count = %s', str(response_data_frame.shape[0]))
+            if response_data_frame.shape[1] > 0:
+                response_data_frame = response_data_frame.assign(InstallationCode=str(name[0]),
+                                                                 SensorCount=group.shape[0])
+                invalid_rows = response_data_frame[response_data_frame['status'] != 'Ok']
+                if invalid_rows.shape[0] > 0:
+                    logger.error("The following rows contain invalid data: \n %s", invalid_rows)
+                    chunk_list += [
+                        {'Chunk': name, 'DeviceCountToUpsert': 1,
+                         'SensorCountToUpsert': str(group.shape[0]),
+                         'response_status': response_status,
+                         'response_df': response_data_frame[response_data_frame['status'] == 'Ok'],
+                         'invalid_rows': invalid_rows}]
+                else:
+                    chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1,
+                                    'SensorCountToUpsert': str(group.shape[0]),
+                                    'response_status': response_status, 'response_df': response_data_frame,
+                                    'invalid_rows': invalid_rows}]
+
+    upsert_response_df = pandas.DataFrame()
+    upsert_invalid_rows_df = pandas.DataFrame()
+    upsert_response_status_list = []
+    for i in range(len(chunk_list)):
+        upsert_response_df = pandas.concat([upsert_response_df, chunk_list[i]['response_df']], axis=0, ignore_index=True)
+        upsert_invalid_rows_df = pandas.concat([upsert_invalid_rows_df, chunk_list[i]['invalid_rows']], axis=0, ignore_index=True)
+        upsert_response_status_list += [chunk_list[i]['response_status']]
+
+    if save_additional_columns_as_slices and slices_data_frame.shape[0] > 0:
+        slices_merged = pandas.merge(left=upsert_response_df, right=slices_data_frame, left_on='deviceCode',
+                                     right_on='DeviceCode')
+        slices_data_frame = slices_merged[['deviceId', 'Slices']]
+        slices_data_frame = slices_data_frame.rename(columns={'deviceId': 'DeviceId'})
+        upsert_data(slices_data_frame, api_inputs=api_inputs, key_columns=['DeviceId'], table_name='DeviceSensorSlices',
+                    is_slices_table=True)
+
+    upsert_response_df.columns = _column_name_cap(columns=upsert_response_df.columns)
+    _upsert_entities_affected_count(api_inputs=api_inputs,
+                                    entities_affected_count=upsert_response_df['SensorCount'].sum())
+    _adx_support(api_inputs=api_inputs, payload_type='Sensors')
+
+    if upsert_invalid_rows_df.shape[0]>0:
+        logger.error(f"There were {upsert_invalid_rows_df.shape[0]} devices that were not successfully upserted: /n "
+                     f"{upsert_invalid_rows_df}")
+
+    logger.info("Ingestion Complete. ")
+    return upsert_response_status_list, upsert_response_df
+
+
+@_with_func_attrs(df_required_columns=['InstallationId', 'DriverUniqueId', 'DiscoveredValue', 'DriverClassName', 'DriverDeviceType',
+                                       'ObjectPropertyTemplateName', 'PropertyName', 'PointName', 'PointClassName', 'UnitOfMeasureAbbrev',
+                                       'DeviceName', 'DisplayName', 'RelatedEntityId', 'Tags'])
+def upsert_device_sensors_iq(api_inputs: ApiInputs, df: pandas.DataFrame):
+    """Upsert device(s) and sensor(s) for Switch IQ
+
+    Required fields are:
+
+    - InstallationId
+    - DriverUniqueId
+    - DiscoveredValue
+    - DriverClassName
+    - DriverDeviceType
+    - ObjectPropertyTemplateName
+    - PropertyName
+    - PointName
+    - PointClassName
+    - UnitOfMeasureAbbrev
+    - DeviceName
+    - DisplayName
+    - RelatedEntityId
+    - Tags
+
+    Parameters
+    ----------
+    df: pandas.DataFrame
+        The asset register created by the driver including the minimum required set of columns.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+
+    Returns
+    -------
+    tuple[list, pandas.DataFrame]
+        (response_status_list, upsert_response_df) - Returns the list of response statuses and the dataframe containing
+        the parsed response text.
+
+    """
+    pd.set_option('display.max_columns', 15)
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    data_frame = df.copy()
+
+    required_columns = ['InstallationId', 'DriverUniqueId', 'DiscoveredValue', 'DriverClassName', 'DriverDeviceType',
+                        'ObjectPropertyTemplateName', 'PropertyName', 'PointName', 'PointClassName', 'UnitOfMeasureAbbrev',
+                        'DeviceName', 'DisplayName', 'RelatedEntityId', 'Tags']
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(data_frame.columns):
+        logger.exception('Missing required column(s): %s', set(
+            required_columns).difference(proposed_columns))
+        return 'Integration.upsert_device_sensors_iq(): data_frame must contain the following columns: ' + ', '.join(
+            required_columns)
+
+    headers = api_inputs.api_headers.integration
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion-v2"
+
+    print(data_frame.to_json(orient='records'))
+    response = requests.post(url, data=data_frame.to_json(
+        orient='records'), headers=headers)
+
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    logger.info("Response status: %s", response_status)
+
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s. Error Message: %s", response.status_code,
+                     response.reason, response.text)
+        response_data_frame = pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s',
+                     response.request.url)
+        response_data_frame = pandas.DataFrame()
+    elif response.status_code == 200 and len(response.text) > 0:
+        response_data_frame = pandas.read_json(response.text)
+        logger.info('Dataframe response row count = %s',
+                    str(response_data_frame.shape[0]))
+
+    return response_data_frame
+
+@_with_func_attrs(df_required_columns=['DriverClassName', 'DriverDeviceType', 'PropertyName', 'DeviceCode', 'DeviceName', 'SensorName', 'SensorTemplate',
+                        'SensorUnitOfMeasure', 'EquipmentClass', 'EquipmentLabel'])
+def upsert_device_sensors_ext(df: pandas.DataFrame, api_inputs: ApiInputs, tag_columns: list = None,
+                          metadata_columns: list = None, save_additional_columns_as_slices: bool = False):
+    """Upsert device(s) and sensor(s)
+
+    Required fields are:
+
+    - InstallationCode or InstallationId
+    - DriverClassName
+    - DriverDeviceType
+    - DeviceCode
+    - DeviceName
+    - PropertyName
+    - SensorName
+    - SensorTemplate
+    - SensorUnitOfMeasure
+    - EquipmentClass
+    - EquipmentLabel
+
+    Parameters
+    ----------
+    df: pandas.DataFrame
+        The asset register created by the driver including the minimum required set of columns.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    tag_columns : list, default = None
+        Columns of dataframe that contain tags (Default value = None).
+    metadata_columns : list, default = None
+        Column(s) of dataframe that contain device-level metadata (Default value = None).
+    save_additional_columns_as_slices : bool, default = False
+        Whether additional columns should be saved as slices (Default value = False).
+
+    Returns
+    -------
+    tuple[list, pandas.DataFrame]
+        (response_status_list, upsert_response_df) - Returns the list of response statuses and the dataframe containing
+        the parsed response text.
+
+    """
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    data_frame = df.copy()
+
+    required_columns = ['DriverClassName', 'DriverDeviceType', 'PropertyName', 'DeviceCode', 'DeviceName', 'SensorName', 'SensorTemplate',
+                        'SensorUnitOfMeasure', 'EquipmentClass', 'EquipmentLabel']
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(data_frame.columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        return 'Integration.upsert_device_sensors(): data_frame must contain the following columns: ' + ', '.join(
+            required_columns)
+   
+    if 'InstallationCode' not in data_frame.columns and 'InstallationId' not in data_frame.columns:
+        logger.exception('Must contain InstallationCode or InstallationId')
+        return 'Integration.upsert_device_sensors(): data_frame must contain either InstallationCode or InstallationId columns'
+   
+    if tag_columns is not None and not set(tag_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected tag column(s): %s', set(tag_columns).difference(proposed_columns))
+        return 'Integration.upsert_device_sensors(): data_frame expected to contain the following tag column(s): ' + \
+               ', '.join(tag_columns)
+    elif tag_columns is None:
+        tag_columns = []
+
+    if metadata_columns is not None and not set(metadata_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected metadata column(s): %s', set(metadata_columns).difference(proposed_columns))
+        return 'Integration.upsert_device_sensors(): data_frame expected to contain the following metadata ' \
+               'column(s): ' + ', '.join(metadata_columns)
+    elif metadata_columns is None:
+        metadata_columns = []
+
+    required_columns.append('InstallationCode')
+    required_columns.append('InstallationId')
+    slice_columns = set(proposed_columns).difference(set(required_columns)) - set(tag_columns) - set(metadata_columns)
+    slice_columns = list(slice_columns)
+    slices_data_frame = pandas.DataFrame()
+
+    if len(slice_columns) > 0 or len(tag_columns) > 0 or len(metadata_columns) > 0:
+        def update_values(row, mode):
+            if mode == 'A':
+                j_row = row[slice_columns].to_json()
+                if j_row == '{}':
+                    j_row = ''
+                return str(j_row)
+            elif mode == 'B':
+                j_row = row[tag_columns].to_json()
+                if j_row == '{}':
+                    j_row = ''
+                return str(j_row)
+            else:
+                j_row = row[metadata_columns].to_json()
+                if j_row == '{}':
+                    j_row = ''
+                return str(j_row)
+
+        data_frame['Slices'] = data_frame.apply(update_values, args="A", axis=1)
+       
+        if tag_columns is not None:
+            data_frame['TagsJson'] = data_frame.apply(update_values, args="B", axis=1)
+
+        if metadata_columns is not None:
+            data_frame['MetadataJson'] = data_frame.apply(update_values, args="C", axis=1)
+
+        data_frame = data_frame.drop(columns=slice_columns)
+        slices_data_frame = data_frame[['DeviceCode', 'Slices']]
+
+    headers = api_inputs.api_headers.integration
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion"
+   
+    def group_data_frame(df):
+        if 'InstallationCode' in data_frame.columns:
+            return df.groupby(['InstallationCode', 'DeviceCode'])
+        else:
+            return df.groupby(['InstallationId', 'DeviceCode'])
+   
+    data_frame_grpd = group_data_frame(data_frame)
+
+
+    chunk_list = []
+    for name, group in data_frame_grpd:
+        logger.info("Sending request: POST %s", url)
+        logger.info('Upserting data for InstallationCode = %s and DeviceCode = %s', str(name[0]), str(name[1]))
+        # logger.info('Sensor count to upsert: %s', str(group.shape[0]))
+        print(group.to_json(orient='records'))
+        response = requests.post(url, data=group.to_json(orient='records'), headers=headers)
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        logger.info("Response status: %s", response_status)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1, 'SensorCountToUpsert': str(group.shape[0]),
+                            'response_status': response_status, 'response_df': pandas.DataFrame(),
+                            'invalid_rows': pandas.DataFrame()}]
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1, 'SensorCountToUpsert': str(group.shape[0]),
+                            'response_status': response_status, 'response_df': pandas.DataFrame(),
+                            'invalid_rows': pandas.DataFrame()}]
+        elif response.status_code == 200 and len(response.text) > 0:
+            response_data_frame = pandas.read_json(response.text)
+            logger.info('Dataframe response row count = %s', str(response_data_frame.shape[0]))
+            if response_data_frame.shape[1] > 0:
+                response_data_frame = response_data_frame.assign(InstallationCode=str(name[0]),
+                                                                 SensorCount=group.shape[0])
+                invalid_rows = response_data_frame[response_data_frame['status'] != 'Ok']
+                if invalid_rows.shape[0] > 0:
+                    logger.error("The following rows contain invalid data: %s", invalid_rows)
+                    chunk_list += [
+                        {'Chunk': name, 'DeviceCountToUpsert': 1,
+                         'SensorCountToUpsert': str(group.shape[0]),
+                         'response_status': response_status,
+                         'response_df': response_data_frame[response_data_frame['status'] == 'Ok'],
+                         'invalid_rows': invalid_rows}]
+                else:
+                    chunk_list += [{'Chunk': name, 'DeviceCountToUpsert': 1,
+                                    'SensorCountToUpsert': str(group.shape[0]),
+                                    'response_status': response_status, 'response_df': response_data_frame,
+                                    'invalid_rows': invalid_rows}]
+
+    upsert_response_df = pandas.DataFrame()
+    upsert_invalid_rows_df = pandas.DataFrame()
+    upsert_response_status_list = []
+    for i in range(len(chunk_list)):
+        upsert_response_df =pandas.concat([upsert_response_df, chunk_list[i]['response_df']], axis=0, ignore_index=True)
+        upsert_invalid_rows_df = pandas.concat([upsert_invalid_rows_df, chunk_list[i]['invalid_rows']], axis=0, ignore_index=True)
+        upsert_response_status_list += [chunk_list[i]['response_status']]
+
+    if save_additional_columns_as_slices and slices_data_frame.shape[0] > 0:
+        slices_merged = pandas.merge(left=upsert_response_df, right=slices_data_frame, left_on='deviceCode',
+                                     right_on='DeviceCode')
+        slices_data_frame = slices_merged[['deviceId', 'Slices']]
+        slices_data_frame = slices_data_frame.rename(columns={'deviceId': 'DeviceId'})
+        upsert_data(slices_data_frame, api_inputs=api_inputs, key_columns=['DeviceId'], table_name='DeviceSensorSlices',
+                    is_slices_table=True)
+
+    upsert_response_df.columns = _column_name_cap(columns=upsert_response_df.columns)
+    _upsert_entities_affected_count(api_inputs=api_inputs,
+                                    entities_affected_count=upsert_response_df['SensorCount'].sum())
+    _adx_support(api_inputs=api_inputs, payload_type='Sensors')
+
+    logger.info("Ingestion Complete. ")
+    return upsert_response_status_list, upsert_response_df
+
+
+@_with_func_attrs(df_required_columns=['InstallationName', 'InstallationCode', 'Address', 'Country', 'Suburb', 'State',
+                                       'StateName', 'FloorAreaM2', 'ZipPostCode'],
+                  df_optional_columns=['Latitude', 'Longitude', 'Timezone', 'InstallationId'])
+def upsert_sites(df: pandas.DataFrame, api_inputs: ApiInputs, tag_columns: list = None,
+                 save_additional_columns_as_slices: bool = False):
+    """Upsert site(s).
+
+    The `df` input must contain the following columns:
+        - InstallationName
+        - InstallationCode
+        - Address
+        - Suburb
+        - State
+        - StateName
+        - Country
+        - FloorAreaM2
+        - ZipPostCode
+
+    The following additional columns are optional:
+        - Latitude
+        - Longitude
+        - Timezone
+        - InstallationId
+            - The UUID of the existing site within the Switch Automation Platform.
+
+    Parameters
+    ----------
+    df: pandas.DataFrame :
+        The dataframe containing the sites to be created/updated in the Switch platform. All required columns must be
+        present with no null values.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    tag_columns : list, default=[]
+        The columns containing site-level tags. The column header will be the tag group name. (Default value = True)
+    save_additional_columns_as_slices : bool, default = False
+        Whether any additional columns should be saved as slices. (Default value = False)
+
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response, response_data_frame) - Returns the response status and the dataframe containing the parsed response
+        text.
+
+    """
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    data_frame = df.copy()
+
+    required_columns = ['InstallationName', 'InstallationCode', 'Address', 'Country', 'Suburb', 'State', 'StateName',
+                        'FloorAreaM2', 'ZipPostCode']
+    optional_columns = ['Latitude', 'Longitude', 'Timezone', 'InstallationId']
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(proposed_columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        return 'Integration.upsert_sites() - data_frame must contain the following columns: ' + ', '.join(
+            required_columns) + '. Optional Columns include: ' + ', '.join(optional_columns)
+
+    if tag_columns is not None and not set(tag_columns).issubset(data_frame.columns):
+        logger.exception('Missing expected tag column(s): %s', set(tag_columns).difference(proposed_columns))
+        return 'Integration.upsert_sites(): data_frame must contain the following tag columns: ' + ', '.join(
+            tag_columns)
+    elif tag_columns is None:
+        tag_columns = []
+
+    slice_columns = set(proposed_columns).difference(set(required_columns + optional_columns)) - set(tag_columns)
+    slice_columns = list(slice_columns)
+    slices_data_frame = pandas.DataFrame()
+
+    if len(slice_columns) > 0 or len(tag_columns) > 0:
+        def update_values(row, mode):
+            if mode == 'A':
+                j_row = row[slice_columns].to_json()
+                if j_row == '{}':
+                    j_row = ''
+                return str(j_row)
+            else:
+                j_row = row[tag_columns].to_json()
+                if j_row == '{}':
+                    j_row = ''
+                return str(j_row)
+
+        data_frame['Slices'] = data_frame.apply(update_values, args=('A',), axis=1)
+        data_frame['TagsJson'] = data_frame.apply(update_values, args=('B',), axis=1)
+
+        data_frame = data_frame.drop(columns=tag_columns)
+        data_frame = data_frame.drop(columns=slice_columns)
+        slices_data_frame = data_frame[['InstallationCode', 'Slices']]
+
+    headers = api_inputs.api_headers.integration
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/installations/upsert-ingestion"
+    logger.info("Sending request: POST %s", url)
+
+    response = requests.post(url, data=data_frame.to_json(orient='records'), headers=headers)
+
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error(f"API Call was not successful. Response Status: {response.status_code}. Reason: {response.reason}. "
+                     f"Message: {response.text}")
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+
+    response_data_frame = pandas.read_json(response.text)
+
+    if save_additional_columns_as_slices and slices_data_frame.shape[0] > 0:
+        slices_merged = pandas.merge(left=response_data_frame, right=slices_data_frame, left_on='installationCode',
+                                     right_on='InstallationCode')
+        slices_data_frame = slices_merged[['installationId', 'Slices']]
+        slices_data_frame = slices_data_frame.rename(columns={'installationId': 'InstallationId'})
+        upsert_data(slices_data_frame, api_inputs=api_inputs, key_columns=['InstallationId'],
+                    table_name='InstallationSlices', is_slices_table=True)
+
+    response_data_frame.columns = _column_name_cap(columns=response_data_frame.columns)
+
+    count_entities = response_data_frame.apply(
+        lambda row: 'Created' if (row['IsInserted'] == True and row['IsUpdated'] == False) else (
+            'Updated' if row['IsInserted'] == False and row['IsUpdated'] == True else 'Failed'), axis=1).isin(
+        ['Created', 'Updated']).sum()
+
+    _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=count_entities)
+    _adx_support(api_inputs=api_inputs, payload_type='Sites')
+
+    logger.info("Ingestion complete. ")
+
+    return response_status, response_data_frame
+
+
+@_with_func_attrs(df_required_columns=['WorkOrderId', 'InstallationId', 'WorkOrderSiteIdentifier', 'Status',
+                                       'RawStatus', 'Priority', 'RawPriority', 'WorkOrderCategory',
+                                       'RawWorkOrderCategory', 'Type', 'Description', 'CreatedDate',
+                                       'LastModifiedDate', 'WorkStartedDate', 'WorkCompletedDate', 'ClosedDate'],
+                  df_optional_columns=['SubType', 'Vendor', 'VendorId', 'EquipmentClass', 'RawEquipmentClass',
+                                       'EquipmentLabel', 'RawEquipmentId', 'TenantId', 'TenantName', 'NotToExceedCost',
+                                       'TotalCost', 'BillableCost', 'NonBillableCost', 'Location', 'RawLocation',
+                                       'ScheduledStartDate', 'ScheduledCompletionDate'])
+def upsert_workorders(df: pandas.DataFrame, api_inputs: ApiInputs, save_additional_columns_as_slices: bool = False):
+    """Upsert data to the Workorder table.
+
+    The following columns are required to be present in the df:
+
+    - ``WorkOrderId``: unique identifier for the work order instance
+    - ``InstallationId``: the InstallationId (guid) used to uniquely identify a given site within the Switch platform
+    - ``WorkOrderSiteIdentifier``: the work order provider's raw/native site identifier field
+    - ``Status``: the status mapped to the Switch standard values defined by literal: `WORK_ORDER_STATUS`
+    - ``RawStatus``: the work order provider's raw/native status
+    - ``Priority``: the priority mapped to the Switch standard values defined by literal: `WORK_ORDER_PRIORITY`
+    - ``RawPriority``: the work order provider's raw/native priority
+    - ``WorkOrderCategory``: the category mapped to the Switch standard values defined by literal: `WORK_ORDER_CATEGORY`
+    - ``RawWorkOrderCategory``: the work order provider's raw/native category
+    - ``Type`` - work order type (as defined by provider) - e.g. HVAC - Too Hot, etc.
+    - ``Description``: description of the work order.
+    - ``CreatedDate``: the date the work order was created (Submitted status)
+    - ``LastModifiedDate``: datetime the workorder was last modified
+    - ``WorkStartedDate``: datetime work started on the work order (In Progress status)
+    - ``WorkCompletedDate``: datetime work was completed for the work order (Resolved status)
+    - ``ClosedDate``: datetime the workorder was closed (Closed status)
+
+    The following columns are optional:
+
+    - ``SubType``: the sub-type of the work order
+    - ``Vendor``: the name of the vendor
+    - ``VendorId``: the vendor id
+    - ``EquipmentClass``: the Switch defined Equipment Class mapped from the work order provider's definition
+    - ``RawEquipmentClass``: the work order provider's raw/native equipment class
+    - ``EquipmentLabel``: the EquipmentLabel as defined within the Switch platform
+    - ``RawEquipmentId``: the work order provider's raw/native equipment identifier/label
+    - ``TenantId``: the tenant id
+    - ``TenantName``: the name of the tenant
+    - ``NotToExceedCost``: the cost not to be exceeded for the given work order
+    - ``TotalCost``: total cost of the work order
+    - ``BillableCost``: the billable portion of the work order cost
+    - ``NonBillableCost``: the non-billable portion of the work order cost.
+    - ``Location``: the Location as defined within the Switch platform
+    - ``RawLocation``: the work order provider's raw/native location definition
+    - ``ScheduledStartDate``: datetime work was scheduled to start on the given work order
+    - ``ScheduledCompletionDate``" datetime work was scheduled to be completed for the given work order
+
+
+    Parameters
+    ----------
+    df: pandas.DataFrame
+        Dataframe containing the work order data to be upserted.
+    api_inputs: ApiInputs
+        Object returned by initialize() function.
+    save_additional_columns_as_slices : bool, default = False
+         (Default value = False)
+
+    Returns
+    -------
+
+    """
+
+    data_frame = df.copy()
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    required_columns = ['WorkOrderId', 'InstallationId', 'WorkOrderSiteIdentifier', 'Status',
+                        'RawStatus', 'Priority', 'RawPriority', 'WorkOrderCategory', 'RawWorkOrderCategory', 'Type',
+                        'Description', 'CreatedDate', 'LastModifiedDate', 'WorkStartedDate', 'WorkCompletedDate',
+                        'ClosedDate']
+    optional_columns = ['SubType', 'Vendor', 'VendorId', 'EquipmentClass', 'RawEquipmentClass', 'EquipmentLabel',
+                        'RawEquipmentId', 'TenantId', 'TenantName', 'NotToExceedCost', 'TotalCost', 'BillableCost',
+                        'NonBillableCost', 'Location', 'RawLocation', 'ScheduledStartDate', 'ScheduledCompletionDate']
+
+    req_cols = ', '.join(required_columns)
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(proposed_columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        return 'Integration.upsert_workorder() - data_frame must contain the following columns: ' + req_cols
+
+    try:
+        _work_order_schema.validate(data_frame, lazy=True)
+    except pandera.errors.SchemaErrors as err:
+        logger.error('Errors present with columns in df provided.')
+        logger.error(err.failure_cases)
+        schema_error = err.failure_cases
+        return schema_error
+
+    slice_columns = set(proposed_columns).difference(set(required_columns + optional_columns))
+    slice_columns = list(slice_columns)
+
+    missing_optional_columns = set(optional_columns) - set(proposed_columns)
+    for missing_column in missing_optional_columns:
+        data_frame[missing_column] = ''
+
+    if len(slice_columns) > 0 and save_additional_columns_as_slices is True:
+        def update_values(row):
+            j_row = row[slice_columns].to_json()
+            return str(j_row)
+
+        data_frame['Meta'] = data_frame.apply(update_values, axis=1)
+        data_frame = data_frame.drop(columns=slice_columns)
+    elif len(slice_columns) > 0 and save_additional_columns_as_slices is not True:
+        data_frame = data_frame.drop(columns=slice_columns)
+        data_frame['Meta'] = ''
+    else:
+        data_frame['Meta'] = ''
+
+    # payload = {}
+    headers = api_inputs.api_headers.integration
+
+    logger.info("Upserting data to Workorders.")
+
+    data_frame = data_frame.loc[:, ['WorkOrderId', 'InstallationId', 'WorkOrderSiteIdentifier', 'Status', 'Priority',
+                                    'WorkOrderCategory', 'Type', 'Description', 'CreatedDate', 'LastModifiedDate',
+                                    'WorkStartedDate', 'WorkCompletedDate', 'ClosedDate', 'RawPriority',
+                                    'RawWorkOrderCategory', 'RawStatus', 'SubType', 'Vendor', 'VendorId',
+                                    'EquipmentClass', 'RawEquipmentClass', 'EquipmentLabel', 'RawEquipmentId',
+                                    'TenantId', 'TenantName', 'NotToExceedCost', 'TotalCost', 'BillableCost',
+                                    'NonBillableCost', 'Location', 'RawLocation', 'ScheduledStartDate',
+                                    'ScheduledCompletionDate', 'Meta']]
+
+    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name='WorkOrder')
+    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "append",
+                    "tableDef": _get_structure(data_frame),
+                    "keyColumns": ["WorkOrderId"]
+                    }
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/work-order-operation"
+    logger.info("Sending request: POST %s", url)
+
+    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+    elif response.status_code == 200:
+        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
+
+    response_df = pandas.read_json(response.text, typ='series').to_frame().T
+    response_df.columns = _column_name_cap(columns=response_df.columns)
+
+    logger.info("Response status: %s", response_status)
+    logger.info("Ingestion complete. ")
+
+    return response_status, response_df
+
+
+@_with_func_attrs(df_required_columns=['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value'])
+def upsert_timeseries_ds(df: pandas.DataFrame, api_inputs: ApiInputs, is_local_time: bool = True,
+                         save_additional_columns_as_slices: bool = False, data_feed_file_status_id: uuid.UUID = None):
+    """Upserts to Timeseries_Ds table.
+
+    The following columns are required to be present in the data_frame:
+    - InstallationId
+    - ObjectPropertyId
+    - Timestamp
+    - Value
+
+    Parameters
+    ----------
+    df: pandas.DataFrame
+        Dataframe containing the data to be appended to timeseries.
+    api_inputs: ApiInputs
+        Object returned by initialize() function.
+    is_local_time : bool, default = True
+         Whether the datetime values are in local time or UTC. If false, then UTC (Default value = True).
+    save_additional_columns_as_slices : bool, default = False
+         (Default value = False)
+    data_feed_file_status_id : uuid.UUID, default = None
+         Enables developer to identify upserted rows using during development. This data is posted to the
+         DataFeedFileStatusId in the Timeseries_Ds table.
+
+         Once deployed, the DataFeedFileStatusId field will contain a unique Guid which will assist in
+         tracking upload results and logging.
+
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
+        text.
+
+    """
+
+    data_frame = df.copy()
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    required_columns = ['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value']
+    req_cols = ', '.join(required_columns)
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(proposed_columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        return 'integration.upsert_timeseries_ds() - data_frame must contain the following columns: ' + req_cols
+
+    slice_columns = set(proposed_columns).difference(set(required_columns))
+    slice_columns = list(slice_columns)
+
+    if len(slice_columns) > 0 and save_additional_columns_as_slices is True:
+        def update_values(row):
+            j_row = row[slice_columns].to_json()
+            return str(j_row)
+
+        # data_frame['Meta'] = data_frame.apply(update_values, axis=1)
+        data_frame['Meta'] = data_frame[slice_columns].assign(
+            **data_frame[slice_columns].select_dtypes(['datetime', 'object']).astype(str)).apply(update_values, axis=1)
+
+        data_frame = data_frame.drop(columns=slice_columns)
+    elif len(slice_columns) > 0 and save_additional_columns_as_slices is not True:
+        data_frame = data_frame.drop(columns=slice_columns)
+        data_frame['Meta'] = ''
+    else:
+        data_frame['Meta'] = ''
+
+    if api_inputs.data_feed_file_status_id is not None and api_inputs.data_feed_file_status_id != '00000000-0000-0000' \
+                                                                                                  '-0000-000000000000':
+        data_frame['DataFeedFileStatusId'] = api_inputs.data_feed_file_status_id
+    elif data_feed_file_status_id is not None:
+        data_frame['DataFeedFileStatusId'] = data_feed_file_status_id
+    else:
+        data_frame['DataFeedFileStatusId'] = '00000000-0000-0000-0000-000000000000'
+
+    site_list = data_frame['InstallationId'].unique().tolist()
+    start_date = data_frame['Timestamp'].min(axis=0, skipna=True)
+    end_date = data_frame['Timestamp'].max(axis=0, skipna=True)
+
+    timezones = _timezone_offsets(api_inputs=api_inputs, date_from=start_date.date(), date_to=end_date.date(),
+                                  installation_id_list=site_list)
+    timezones['dateFrom'] = timezones['dateFrom'].apply(lambda x: pandas.to_datetime(x))
+    timezones['dateTo'] = timezones['dateTo'].apply(lambda x: pandas.to_datetime(x))
+
+    data_frame = data_frame.merge(timezones, left_on='InstallationId', right_on='installationId', how='inner')
+
+    def in_range(row):
+        j_row = (row['Timestamp'] >= row['dateFrom']) & (
+            row['Timestamp'] < (row['dateTo'] + datetime.timedelta(days=1)))
+        return str(j_row)
+
+    data_frame['InDateRange'] = data_frame.apply(in_range, axis=1)
+    data_frame = data_frame[data_frame['InDateRange'] == 'True']
+
+    if is_local_time:
+        def to_utc(row):
+            if row['offsetToUtcMinutes'] >= 0:
+                j_row = row['TimestampLocal'] - datetime.timedelta(minutes=row['offsetToUtcMinutes'])
+            else:
+                j_row = row['TimestampLocal'] + datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
+            return j_row
+
+        data_frame = data_frame.assign(TimestampLocal=data_frame['Timestamp'])
+        data_frame['Timestamp'] = data_frame.apply(to_utc, axis=1)
+    elif not is_local_time:
+        def from_utc(row):
+            if row['offsetToUtcMinutes'] >= 0:
+                j_row = row['Timestamp'] + datetime.timedelta(minutes=row['offsetToUtcMinutes'])
+            else:
+                j_row = row['Timestamp'] - datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
+            return j_row
+
+        data_frame['TimestampLocal'] = data_frame.apply(from_utc, axis=1)
+
+    def bin_to_15_minute_interval(row, date_col):
+        if row[date_col].minute < 15:
+            j_row = row[date_col].replace(minute=0)
+            return j_row
+        elif row[date_col].minute >= 15 and row[date_col].minute < 30:
+            j_row = row[date_col].replace(minute=15)
+            return j_row
+        elif row[date_col].minute >= 30 and row[date_col].minute < 45:
+            j_row = row[date_col].replace(minute=30)
+            return j_row
+        else:
+            j_row = row[date_col].replace(minute=45)
+            return j_row
+
+    data_frame['TimestampId'] = data_frame.apply(bin_to_15_minute_interval, args=('Timestamp',), axis=1)
+    data_frame['TimestampLocalId'] = data_frame.apply(bin_to_15_minute_interval, args=('TimestampLocal',), axis=1)
+    data_frame = data_frame.drop(columns=['InDateRange', 'dateFrom', 'dateTo', 'installationId', 'offsetToUtcMinutes'])
+
+    # payload = {}
+    headers = api_inputs.api_headers.integration
+
+    logger.info("Upserting data to Timeseries_Ds.")
+
+    data_frame = data_frame.loc[:, ['ObjectPropertyId', 'Timestamp', 'TimestampId', 'TimestampLocal',
+                                    'TimestampLocalId', 'Value', 'DataFeedFileStatusId', 'InstallationId', 'Meta']]
+    name = 'Timeseries_Ds'
+
+    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=name, batch_id=data_feed_file_status_id)
+
+    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "upsert",
+                    "isLocalTime": is_local_time,
+                    "tableDef": _get_structure(data_frame),
+                    "keyColumns": ["ObjectPropertyId", "Timestamp"]
+                    }
+
+    # {'ObjectPropertyId': 'object', 'Timestamp': 'datetime64[ns]', 'Value': 'float64', 'InstallationId': 'object'}
+
+    # ObjectPropertyId: string, Timestamp: datetime, TimestampId: datetime, TimestampLocal: datetime,
+    # TimestampLocalId: datetime, Value: real, DataFeedFileStatusId: string, InstallationId: string, Meta: dynamic
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/time-series-operation?originalName=" \
+          f"{name}"
+    logger.info("Sending request: POST %s", url)
+
+    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+    elif response.status_code == 200:
+        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
+
+    response_df = pandas.read_json(response.text, typ='series').to_frame().T
+    response_df.columns = _column_name_cap(columns=response_df.columns)
+
+    logger.info("Response status: %s", response_status)
+    logger.info("Ingestion complete. ")
+    return response_status, response_df
+
+
+@_with_func_attrs(df_required_columns=['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value'])
+def upsert_timeseries(df: pandas.DataFrame, api_inputs: ApiInputs, is_local_time: bool = True,
+                      save_additional_columns_as_slices: bool = False, data_feed_file_status_id: uuid.UUID = None, 
+                      is_specific_timezone: Union[bool, str] = False, ingestion_mode: IngestionMode = 'Queue',
+                    send_notification: bool = False):
+    """Upserts timeseries to EventHub for processing.
+
+    The following columns are required to be present in the data_frame:
+    - InstallationId
+    - ObjectPropertyId
+    - Timestamp
+    - Value
+
+    Parameters
+    ----------
+    df: pandas.DataFrame
+        Dataframe containing the data to be appended to timeseries.
+    api_inputs: ApiInputs
+        Object returned by initialize() function.
+    is_local_time : bool, default = True
+         Whether the datetime values are in local time or UTC. If, False and is_specific_timezone is False, then UTC (Default value = True).
+         Should be set to False when 'is_specific_timezone' has value.
+    save_additional_columns_as_slices : bool, default = False
+         (Default value = False)
+    data_feed_file_status_id : uuid.UUID, default = None
+         Enables developer to identify upserted rows using during development. This data is posted to the
+         DataFeedFileStatusId in the Timeseries_Ds table.
+
+         Once deployed, the DataFeedFileStatusId field will contain a unique Guid which will assist in
+         tracking upload results and logging.
+    is_specific_timezone : Union[False, str]
+        Accepts a timezone name as the specific timezone used by the source data. Defaults to False.
+        Cannot have value if 'is_local_time' is set to True.
+        Retrieve list of timezones using 'sw.integration.get_timezones()'
+    send_notification : bool
+        This enables Iq Notification messages to be sent when set to `True`
+        Default value = False
+
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
+        text.
+
+    """
+
+    data_frame = df.copy()
+    timezones_df = pd.DataFrame()
+    logger.info(f'Data frame size: {convert_bytes(data_frame.memory_usage(deep=True).sum())}')
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if not isinstance(is_specific_timezone, str) and not isinstance(is_specific_timezone, bool):
+        logger.error("'is_specific_timezone' parameter can only be of type str or bool.")
+        return pandas.DataFrame()
+
+    if is_specific_timezone != False and is_local_time == True:
+        logger.error("Assigning specific timezone is only possible if 'is_local_time' is set to False.")
+        return pandas.DataFrame()
+
+    if is_specific_timezone == True:
+        logger.error("'is_specific_timezone' parameter value, if not False, should be a valid timezone. Retrieve list of timezones using 'sw.integration.get_timezones()")
+        return pandas.DataFrame()
+
+    if is_specific_timezone == '':
+        logger.error("'is_specific_timezone' cannot be an empty string. Retrieve list of timezones using 'sw.integration.get_timezones()")
+        return pandas.DataFrame()
+
+    is_specific_timezone_conversion = False
+    if is_specific_timezone != False and (isinstance(is_specific_timezone, str) and is_specific_timezone != ''):
+        timezone_name_df = get_timezones(api_inputs=api_inputs)
+
+        if timezone_name_df is None or timezone_name_df.__len__() == 0:
+            logger.error("Failed to retrieve the allowed Timezone names.")
+            return pandas.DataFrame()
+        
+        if not is_specific_timezone in timezone_name_df.TimezoneName.values:
+            logger.error(f"'is_specific_timezone' parameter value '{is_specific_timezone}' is not found in Timezone Names list.")
+            logger.info(f"'Timezone name list can be retrieved using 'sw.integration.get_timezones()' method.")
+            return pandas.DataFrame()
+        
+        is_specific_timezone_conversion = True
+
+    required_columns = ['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value']
+    req_cols = ', '.join(required_columns)
+    proposed_columns = data_frame.columns.tolist()
+
+    if not set(required_columns).issubset(proposed_columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        return 'integration.upsert_timeseries() - data_frame must contain the following columns: ' + req_cols
+
+    if not pandas.api.types.is_datetime64_ns_dtype(data_frame['Timestamp']):
+        logger.exception("Timestamp series value is not of type: datetime64[ns] dtype.")
+        return "Timestamp series value is not of type: datetime64[ns] dtype."
+
+    null_count_timestamp = data_frame['Timestamp'].isna().sum()
+    if null_count_timestamp > 0:
+        logger.warning(f"There are {null_count_timestamp} records with a null or empty Timestamp value. " \
+            f"These are being dropped resulting in {data_frame.shape[0] - null_count_timestamp} records will be upserted.")
+        data_frame = data_frame.dropna(subset=['Timestamp'])
+
+    slice_columns = set(proposed_columns).difference(set(required_columns))
+    slice_columns = list(slice_columns)
+
+    if len(slice_columns) > 0 and save_additional_columns_as_slices is True:
+        def update_values(row):
+            j_row = row[slice_columns].to_json()
+            return str(j_row)
+
+        logger.info('Creating Meta Column.')
+        data_frame['Meta'] = data_frame[slice_columns].assign(
+            **data_frame[slice_columns].select_dtypes(['datetime', 'object']).astype(str)).apply(update_values, axis=1)
+
+        data_frame = data_frame.drop(columns=slice_columns)
+    elif len(slice_columns) > 0 and save_additional_columns_as_slices is not True:
+        data_frame = data_frame.drop(columns=slice_columns)
+        data_frame['Meta'] = ''
+    else:
+        data_frame['Meta'] = ''
+
+    if api_inputs.data_feed_file_status_id is not None and \
+            api_inputs.data_feed_file_status_id != '00000000-0000-0000-0000-000000000000' and \
+            api_inputs.data_feed_file_status_id != '':
+        data_frame['DataFeedFileStatusId'] = api_inputs.data_feed_file_status_id
+        data_feed_file_status_id = api_inputs.data_feed_file_status_id
+    elif data_feed_file_status_id is not None and \
+            data_feed_file_status_id != '00000000-0000-0000-0000-000000000000' and \
+            data_feed_file_status_id != '':
+        data_frame['DataFeedFileStatusId'] = data_feed_file_status_id
+    else:
+        data_feed_file_status_id = uuid.uuid4()
+        data_frame['DataFeedFileStatusId'] = data_feed_file_status_id
+
+    
+    start_date = data_frame['Timestamp'].min(axis=0, skipna=True)
+    end_date = data_frame['Timestamp'].max(axis=0, skipna=True)
+
+    specific_timezone_df = pd.DataFrame()
+
+    def convert_dst_interval_dates(row):
+        row['start'] = pd.to_datetime(row['start'])
+        row['end'] = pd.to_datetime(row['end'])
+        row['offsetToUtcMinutes'] = row['standardOffsetUtc'] + row['dstOffsetUtc']
+        return row
+
+    if is_specific_timezone_conversion:
+        def timezone_to_utc(row):
+            if row['offsetToUtcMinutes'] >= 0:
+                j_row = row['Timestamp'] - datetime.timedelta(minutes=row['offsetToUtcMinutes'])
+            else:
+                j_row = row['Timestamp'] + datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
+            return j_row
+
+        specific_timezone_df = _timezone_dst_offsets(api_inputs=api_inputs, date_from=start_date.date(), date_to=end_date.date(), timezone_name=is_specific_timezone)
+
+        if specific_timezone_df.empty:
+            logger.exception('Failed to retrieve the specific timezone offsets.')
+            return 'Failed to retrieve the specific timezone offsets.'
+
+        specific_timezone_df = specific_timezone_df.apply(convert_dst_interval_dates, axis=1)
+        specific_timezone_df = specific_timezone_df.drop(columns=['timezoneId', 'standardOffsetUtc', 'dstOffsetUtc'])
+        
+        data_frame['year'] = pd.DatetimeIndex(data_frame['Timestamp']).year
+        data_frame = data_frame.merge(specific_timezone_df, left_on='year', right_on='year', how='inner')
+
+        del specific_timezone_df
+        gc.collect()
+
+        data_frame = data_frame[(data_frame.Timestamp >= data_frame.start) & (data_frame.Timestamp < data_frame.end) & (data_frame.Timestamp.apply(lambda x: x.year) == data_frame.year)]
+        data_frame = data_frame.drop(columns=['start', 'end', 'year'])
+        
+        data_frame['Timestamp'] = data_frame.apply(timezone_to_utc, axis=1)
+        data_frame = data_frame.drop(columns=['offsetToUtcMinutes'])
+
+        # Set automatically to is_local_time = False 
+        is_local_time = False
+
+    site_list = data_frame['InstallationId'].unique().tolist()
+    timezones_df = _timezone_dst_offsets(api_inputs=api_inputs, date_from=start_date.date(), date_to=end_date.date(),
+                                         installation_id_list=site_list)
+
+    if timezones_df.empty:
+        logger.exception('Timezone DST offsets failed to retrieve.')
+        return 'Timezone DST offsets failed to retrieve.'
+
+    timezones_df = timezones_df.apply(convert_dst_interval_dates, axis=1)
+
+    if not is_local_time:
+        def convert_dst_interval_range_to_utc(row):
+            dst_start = row['start']
+            dst_end = row['end']
+
+            if row['standardOffsetUtc'] >= 0:
+                dst_start = dst_start - (datetime.timedelta(minutes=row['standardOffsetUtc'])) - datetime.timedelta(minutes=row['dstOffsetUtc'])
+                dst_end = dst_end - (datetime.timedelta(minutes=row['standardOffsetUtc'])) - datetime.timedelta(minutes=row['dstOffsetUtc'])
+            else:
+                dst_start = dst_start + (datetime.timedelta(minutes=row['standardOffsetUtc'])) + datetime.timedelta(minutes=row['dstOffsetUtc'])
+                dst_end = dst_end + (datetime.timedelta(minutes=row['standardOffsetUtc'])) + datetime.timedelta(minutes=row['dstOffsetUtc'])
+            row['start'] = dst_start
+            row['end'] = dst_end
+
+            return row
+
+        timezones_df = timezones_df.apply(convert_dst_interval_range_to_utc, axis=1)
+    timezones_df = timezones_df.drop(columns=['timezoneId', 'standardOffsetUtc', 'dstOffsetUtc'])
+
+    logger.info('Merging data frame input with timezone installation dst offsets.')
+    data_frame = data_frame.merge(timezones_df, left_on='InstallationId', right_on='installationId', how='inner')
+    
+    del timezones_df
+    gc.collect()
+
+    data_frame = data_frame[(data_frame.Timestamp >= data_frame.start) & (data_frame.Timestamp < data_frame.end) & (data_frame.Timestamp.apply(lambda x: x.year) == data_frame.year)]
+    data_frame = data_frame.drop(columns=['start', 'end', 'year'])
+
+    if is_local_time:
+        def to_utc(row):
+            if row['offsetToUtcMinutes'] >= 0:
+                j_row = row['TimestampLocal'] - datetime.timedelta(minutes=row['offsetToUtcMinutes'])
+            else:
+                j_row = row['TimestampLocal'] + datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
+            return j_row
+
+        data_frame = data_frame.assign(TimestampLocal=data_frame['Timestamp'])
+        data_frame['Timestamp'] = data_frame.apply(to_utc, axis=1)
+
+    elif not is_local_time:
+        def from_utc(row):
+            if row['offsetToUtcMinutes'] >= 0:
+                j_row = row['Timestamp'] + datetime.timedelta(minutes=row['offsetToUtcMinutes'])
+            else:
+                j_row = row['Timestamp'] - datetime.timedelta(minutes=abs(row['offsetToUtcMinutes']))
+            return j_row
+        data_frame['TimestampLocal'] = data_frame.apply(from_utc, axis=1)
+
+    def bin_to_15_minute_interval(row, date_col):
+        if row[date_col].minute < 15:
+            j_row = row[date_col].replace(minute=0)
+            return j_row
+        elif row[date_col].minute >= 15 and row[date_col].minute < 30:
+            j_row = row[date_col].replace(minute=15)
+            return j_row
+        elif row[date_col].minute >= 30 and row[date_col].minute < 45:
+            j_row = row[date_col].replace(minute=30)
+            return j_row
+        else:
+            j_row = row[date_col].replace(minute=45)
+            return j_row
+
+    logger.info('Date time conversion and formatting.')
+    data_frame['TimestampId'] = data_frame.apply(bin_to_15_minute_interval, args=('Timestamp',), axis=1)
+    data_frame['TimestampLocalId'] = data_frame.apply(bin_to_15_minute_interval, args=('TimestampLocal',), axis=1)
+
+    timestamp_format = "%Y-%m-%dT%H:%M:%SZ"
+    timestamp_normalized_format = "%Y-%m-%dT%H:%M:00Z"
+    data_frame['Timestamp'] = data_frame['Timestamp'].dt.strftime(timestamp_format)
+    data_frame['TimestampLocal'] = data_frame['TimestampLocal'].dt.strftime(timestamp_format)
+    data_frame['TimestampId'] = data_frame['TimestampId'].dt.strftime(timestamp_normalized_format)
+    data_frame['TimestampLocalId'] = data_frame['TimestampLocalId'].dt.strftime(timestamp_normalized_format)
+
+    headers = api_inputs.api_headers.integration
+
+    logger.info("Upserting data to Timeseries.")
+    data_frame = data_frame.loc[:, ['ObjectPropertyId', 'Timestamp', 'TimestampId', 'TimestampLocal',
+                                    'TimestampLocalId', 'Value', 'DataFeedFileStatusId', 'InstallationId', 'Meta']]
+
+    container = 'data-ingestion-timeseries-adx'
+    folder = 'to-adx-stream' if ingestion_mode == 'Stream' else 'to-eventhub'
+    name = 'Timeseries'
+    
+    logger.info(f'Data frame size: {convert_bytes(data_frame.memory_usage(deep=True).sum())}')
+    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, container=container, folder=folder, name=name, batch_id=data_feed_file_status_id, include_header=True)
+
+    sensor_result = None, 0
+
+    if (send_notification == True):
+        # send IQ notification, process sensor list with latest property value
+        def _upload_sensor_latest_value(api_input, data_frame, data_feed_file_status_id, container):
+
+            folder = 'to-iq-notifications'
+            name = 'Sensors'
+
+            try:
+                data_frame['Timestamp'] = pd.to_datetime(data_frame['Timestamp'])  # Convert Timestamp column to datetime
+                latest_indices = data_frame.groupby('ObjectPropertyId')['Timestamp'].idxmax()
+                latest_sensor_value_df = data_frame.loc[latest_indices, ['ObjectPropertyId', 'InstallationId', 'Timestamp', 'Value']]
+
+                logger.info(f'Latest sensors Data frame size: {convert_bytes(latest_sensor_value_df.memory_usage(deep=True).sum())}')
+                latest_sensor_upload_result = Blob.upload(api_inputs=api_inputs, 
+                                                        data_frame=latest_sensor_value_df, 
+                                                        container=container, 
+                                                        folder=folder, 
+                                                        name=name, 
+                                                        batch_id=data_feed_file_status_id, 
+                                                        include_header=True
+                                                    )
+                return True, latest_sensor_upload_result
+            except Exception as e:
+                return False, None
+
+        is_success, sensor_result = _upload_sensor_latest_value(api_input=api_inputs, data_frame=data_frame,
+                                                                   data_feed_file_status_id=data_feed_file_status_id,
+                                                                   container=container)
+        
+        if (is_success == False):
+            logger.exception("Uploading of sensors' latest property value for live notifications failed.")
+            sensor_result = None, 0
+
+    json_payload = {
+        "path": upload_result[0], 
+        "fileCount": upload_result[1], 
+        "ingestionMode": ingestion_mode,
+        "sensorPath": sensor_result[0],
+        "sensorFileCount": sensor_result[1],
+        }
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/timeseries"
+    logger.info("Sending request: POST %s", url)
+
+    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
+
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+    elif response.status_code == 200:
+        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
+
+    response_df = pandas.read_json(response.text, typ='series').to_frame().T
+    response_df.columns = _column_name_cap(columns=response_df.columns)
+
+    logger.info("Response status: %s", response_status)
+    logger.info("Ingestion complete. ")
+    return response_status, response_df
+
+
+def upsert_data(data_frame, api_inputs: ApiInputs, table_name: str, key_columns: list,
+                is_slices_table=False, ingestion_mode: IngestionMode = 'Queue', table_def: dict[str, ADX_TABLE_DEF_TYPES] = None):
+    """Upsert data
+
+    Upserts data to the `table_name` provided to the function call and uses the `key_columns` provided to determine the
+    unique records.
+
+    Parameters
+    ----------
+    data_frame : pandas.DataFrame
+        Dataframe containing the data to be upserted.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    table_name : str
+        The name of the table where data will be upserted.
+    key_columns : list
+        The columns that determine a unique instance of a record. These are used to update records if new data is
+        provided.
+    is_slices_table : bool
+         (Default value = False)
+    inegstion_mode : IngestionMode
+        (Default value = 'Queue') The type of ingestion to use.
+    table_def : dict[str, ADX_TABLE_DEF_TYPES]
+        (Default value = None) An optional table definition which will be merged to the inferred table structure based from data_frame.
+
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
+        text.
+
+    """
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.integration
+
+    if len(key_columns) == 0 or key_columns is None:
+        logger.error(
+            "You must provide key_columns. This allows the Switch Automation Platform to identify the rows to update.")
+        return False
+
+    logger.info("Data is being upserted for %s", table_name)
+
+    table_structure = _get_structure(data_frame)
+    table_structure = {**table_structure, **table_def} if table_def is not None else table_structure 
+    
+    if is_slices_table is True and 'Slices' in table_structure:
+        table_structure['Slices'] = 'dynamic'
+
+    # upload Blobs to folder
+    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=table_name)
+    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "upsert",
+                    "tableDef": table_structure, "keyColumns": key_columns, 'ingestionMode': ingestion_mode}
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data-operation?tableName={table_name}"
+    logger.info("Sending request: POST %s", url)
+
+    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
+
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+    elif response.status_code == 200:
+        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
+
+    response_df = pandas.read_json(response.text, typ='series').to_frame().T
+    response_df.columns = _column_name_cap(columns=response_df.columns)
+
+    logger.info("Response status: %s", response_status)
+    logger.info("Ingestion complete. ")
+
+    return response_status, response_df
+
+
+def replace_data(data_frame, api_inputs: ApiInputs, table_name: str, table_def: dict[str, ADX_TABLE_DEF_TYPES] = None):
+    """Replace data
+
+    Replaces the data in the ``table_name`` provided to the function call.
+
+    Parameters
+    ----------
+    data_frame : pandas.DataFrame
+        Data frame to be used to replace the data in the `table_name` table.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    table_name :
+        The name of the table where data will be replaced.
+    table_def : dict[str, ADX_TABLE_DEF_TYPES]
+        (Default value = None) An optional table definition which will be merged to the inferred table structure based from data_frame.
+
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
+        text.
+
+    """
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    headers = api_inputs.api_headers.integration
+
+    logger.info("Replacing all data for %s", table_name)
+
+    table_structure = _get_structure(data_frame)
+    table_structure = {**table_structure, **table_def} if table_def is not None else table_structure
+
+    # upload Blobs to folder
+    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=table_name)
+
+    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "replace",
+                    "tableDef": table_structure}
+    logger.info(json_payload)
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data-operation?tableName={table_name}"
+    logger.info("Sending request: POST %s", url)
+
+    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
+
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, pandas.DataFrame()
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status, pandas.DataFrame()
+    elif response.status_code == 200:
+        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
+
+    response_df = pandas.read_json(response.text, typ='series').to_frame().T
+    response_df.columns = _column_name_cap(columns=response_df.columns)
+
+    logger.info("Ingestion complete. ")
+
+    return response_status, response_df
+
+
+def append_data(data_frame, api_inputs: ApiInputs, table_name: str, table_def: dict[str, ADX_TABLE_DEF_TYPES] = None):
+    """Append data.
+
+    Appends data to the ``table_name`` provided to the function call.
+
+    Parameters
+    ----------
+    data_frame : pandas.DataFrame
+        Data to be appended.
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    table_name : str
+        The name of the table where data will be appended.
+    table_def : dict[str, ADX_TABLE_DEF_TYPES]
+        (Default value = None) An optional table definition which will be merged to the inferred table structure based from data_frame.
+        
+    Returns
+    -------
+    tuple[str, pandas.DataFrame]
+        (response_status, response_df) - Returns the response status and the dataframe containing the parsed response
+        text.
+
+    """
+    # payload = {}
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    # payload = {}
+    headers = api_inputs.api_headers.integration
+
+    logger.info("Appending data for %s", table_name)
+
+    table_structure = _get_structure(data_frame)
+    table_structure = {**table_structure, **table_def} if table_def is not None else table_structure 
+
+    # upload Blobs to folder   
+    upload_result = Blob.upload(api_inputs=api_inputs, data_frame=data_frame, name=table_name)
+    json_payload = {"path": upload_result[0], "fileCount": upload_result[1], "operation": "append",
+                    "tableDef": table_structure}
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/adx/data-operation?tableName={table_name}"
+    logger.info("Sending request: POST %s", url)
+    logger.info("Sending request to ingest %s files from %s", str(upload_result[1]), upload_result[0])
+
+    response = requests.post(url, json=json_payload, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status
+    elif response.status_code == 200:
+        _upsert_entities_affected_count(api_inputs=api_inputs, entities_affected_count=data_frame.shape[0])
+
+    response_df = pandas.read_json(response.text, typ='series').to_frame().T
+    response_df.columns = _column_name_cap(columns=response_df.columns)
+
+    logger.info("API Response: %s", response_status)
+    logger.info("Ingestion complete. ")
+
+    return response_status, response_df
+
+
+def upsert_file_row_count(api_inputs: ApiInputs, row_count: int):
+    """Updates data feed file status row count.
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    row_count : number
+        Number of rows
+
+    Returns
+    -------
+    str
+        Response status as a string.
+    """
+
+    if row_count is None:
+        row_count = 0
+
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    if (api_inputs.data_feed_id == '00000000-0000-0000-0000-000000000000' or
+            api_inputs.data_feed_file_status_id == '00000000-0000-0000-0000-000000000000'):
+        logger.error("upsert_file_row_count() can only be called in Production.")
+        return False
+
+    headers = api_inputs.api_headers.default
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
+          f"{api_inputs.data_feed_id}/file-status/{api_inputs.data_feed_file_status_id}/row-count/{row_count}"
+
+    response = requests.request("PUT", url, timeout=20, headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+        return response_status
+
+    return response_status
+
+
+def upsert_event_work_order_id(api_inputs: ApiInputs, event_task_id: uuid.UUID, integration_id: str,
+                               work_order_status: str):
+    """
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    event_task_id : uuid.UUID
+        The value of the `work_order_input['EventTaskId']`
+    integration_id : str
+        The 3rd Party work order system's unique identifier for the work order
+    work_order_status : str
+        The status of the work order
+
+    Returns
+    -------
+    (str, str)
+        response_status, response.text - The response status of the call and the text from the response body.
+
+    """
+
+    if api_inputs.api_projects_endpoint == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return 'Error', 'You must call initialize() before using the API.'
+
+    header = api_inputs.api_headers.default
+
+    payload = {
+        "IntegrationId": integration_id,
+        "Status": work_order_status
+    }
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/events/{str(event_task_id)}/work-order"
+
+    response = requests.put(url=url, json=payload, headers=header)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                     response.reason)
+        return response_status, response.text
+
+    return response_status, response.text
+
+
+@_with_func_attrs(df_required_columns=['Identifier'])
+def upsert_tags(api_inputs: ApiInputs, df: pandas.DataFrame, tag_level: TAG_LEVEL):
+    """
+    Upsert tags to Site/Device/Sensors as specified by the tag_level argument.
+    
+    Required fields are:
+    - Identifier
+    - Additional columns as TagGroups / Tags
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    df : DataFrame
+        List of Devices along with corresponding TagsJson to upsert
+    tag_level : TAG_LEVEL
+        Level of tagging applied to the list of Identifier input.
+            If tag_level is Site, Identifier should be InstallationIds.
+            If tag_level is Device, Identifier should be DeviceIds.
+            If tag_level is Sensor, Identifier should be ObjectPropertyIds.
+
+    Returns
+    -------
+    List of affected records
+    
+    """
+    data_frame = df.copy()
+
+    # validate inputs
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    # validate df
+    if data_frame.empty:
+        logger.error("Data Frame is empty. Nothing to upsert.")
+        return pandas.DataFrame()
+
+    # validate tag level
+    if not set([tag_level]).issubset(set(TAG_LEVEL.__args__)):
+        logger.error('tag_level parameter must be set to one of the allowed values defined by the '
+                        'TAG_LEVEL literal: %s', TAG_LEVEL.__args__)
+        return pandas.DataFrame()
+
+    required_columns = getattr(upsert_tags, 'df_required_columns')
+    req_cols = ', '.join(required_columns)
+    proposed_columns = data_frame.columns.tolist()
+
+    # check if required columns are present
+    if not set(required_columns).issubset(proposed_columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        return 'integration.upsert_tags() - data_frame must contain the following columns: ' + req_cols
+
+    slice_columns = set(proposed_columns).difference(set(required_columns))
+
+    if len(list(slice_columns)) > 0:
+        def update_values(row):
+            j_row = row[list(slice_columns)].to_json()
+            return str(j_row)
+
+        data_frame['TagsJson'] = data_frame[list(slice_columns)].assign(
+            **data_frame[list(slice_columns)].select_dtypes(['datetime', 'object']).astype(str)).apply(update_values, axis=1)
+        data_frame = data_frame.drop(columns=list(slice_columns))
+    else:
+        logger.error("Additional Columns aside from Identifier is required to set as TagsJson.")
+        return pandas.DataFrame()
+
+    switcher_tag_level_identifier = {
+        'Site': 'InstallationId',
+        'Device': 'DeviceId',
+        'Sensor': 'ObjectPropertyId'
+    }
+
+    tag_level_identifier_id = switcher_tag_level_identifier.get(tag_level)
+
+    data_frame.rename(columns = {'Identifier':tag_level_identifier_id}, inplace = True)
+
+    # action request
+    headers = api_inputs.api_headers.integration
+
+    switcher_tag_level_uri = {
+        'Site': f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/installations/upsert-ingestion-tags",
+        'Device': f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion-tags",
+        'Sensor': f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/sensors/upsert-ingestion-tags",
+    }
+
+    url = switcher_tag_level_uri.get(tag_level)
+
+    logger.info("Sending request: POST %s", url)
+    response = requests.post(url, data=data_frame.to_json(orient='records'), headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+
+    logger.info("Response status: %s", response_status)
+
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                        response.reason)
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+
+    response_df = pandas.read_json(response.text)
+    response_df.columns = _column_name_cap(response_df.columns)
+
+    return response_df
+
+
+@_with_func_attrs(df_required_columns=['DeviceId'])
+def upsert_device_metadata(api_inputs: ApiInputs, df: pandas.DataFrame):
+    """
+    Upsert metadata on created devices.
+
+    Required fields are:
+    - DeviceId
+    - Additional columns as Metadata
+
+    Parameters
+    ----------
+    api_inputs : ApiInputs
+        Object returned by initialize() function.
+    df : DataFrame
+        List of Devices along with corresponding MetadataJson to upsert
+
+    Returns
+    -------
+    List of affected Devices
+    """
+    data_frame = df.copy()
+
+    # validate inputs
+    if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+        logger.error("You must call initialize() before using API.")
+        return pandas.DataFrame()
+
+    # validate df
+    if data_frame.empty:
+        logger.error("Data Frame is empty. Nothing to upsert.")
+        return pandas.DataFrame()
+
+    required_columns = getattr(upsert_device_metadata, 'df_required_columns')
+
+    req_cols = ', '.join(required_columns)
+    proposed_columns = data_frame.columns.tolist()
+
+    # check if required columns are present
+    if not set(required_columns).issubset(proposed_columns):
+        logger.exception('Missing required column(s): %s', set(required_columns).difference(proposed_columns))
+        return 'integration.upsert_device_metadata() - data_frame must contain the following columns: ' + req_cols
+
+    slice_columns = set(proposed_columns).difference(set(required_columns))
+
+    if len(list(slice_columns)) > 0:
+        def update_values(row):
+            j_row = row[list(slice_columns)].to_json()
+            return str(j_row)
+
+        data_frame['MetadataJson'] = data_frame[list(slice_columns)].assign(
+            **data_frame[list(slice_columns)].select_dtypes(['datetime', 'object']).astype(str)).apply(update_values, axis=1)
+        data_frame = data_frame.drop(columns=list(slice_columns))
+    else:
+        logger.error("Additional Columns aside from DeviceId is required to set as MetadataJson.")
+        return pandas.DataFrame()
+
+    # action request
+    headers = api_inputs.api_headers.integration
+
+    url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/devices/upsert-ingestion-metadata"
+
+    logger.info("Sending request: POST %s", url)
+    response = requests.post(url, data=data_frame.to_json(orient='records'), headers=headers)
+    response_status = '{} {}'.format(response.status_code, response.reason)
+
+    logger.info("Response status: %s", response_status)
+
+    if response.status_code != 200:
+        logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                        response.reason)
+    elif len(response.text) == 0:
+        logger.error('No data returned for this API call. %s', response.request.url)
+
+    response_df = pandas.read_json(response.text)
+    response_df.columns = _column_name_cap(response_df.columns)
+
+    return response_df
```

### Comparing `switch_api-0.5.4b2/switch_api/pipeline/__init__.py` & `switch_api-0.5.4b3/switch_api/pipeline/__init__.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,87 +1,87 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""Module defining the Task abstract base class which is inherited by the following specific task types:
-
-- IntegrationTask
-- DiscoverableIntegrationTask
-- AnalyticsTask
-- LogicModuleTask
-- QueueTask
-- EventWorkOrderTask
-
-Also includes the Automation class which contains helper functions.
-
----------------
-IntegrationTask
----------------
-Base class used to create integrations between the Switch Automation Platform and other platforms, low-level services,
-or hardware.
-
-Examples include:
-    - Pulling readings or other types of data from REST APIs
-    - Protocol Translators which ingest data sent to the platform via email, ftp or direct upload within platform.
-
----------------------------
-DiscoverableIntegrationTask
----------------------------
-Base class used to create integrations between the Switch Automation Platform and 3rd party APIs.
-
-Similar to the IntegrationTask, but includes a secondary method `run_discovery()` which triggers discovery of available
-points on the 3rd party API and upserts these records to the Switch Platform backend so that the records are available
-in Build - Discovery & Selection UI.
-
-Examples include:
-    - Pulling readings or other types of data from REST APIs
-
-
--------------
-AnalyticsTask
--------------
-Base class used to create specific analytics functionality which may leverage existing data from the platform. Each
-task may add value to, or supplement, this data and write it back.
-
-Examples include:
-    - Anomaly Detection
-    - Leaky Pipes
-    - Peer Tracking
-
----------------
-LogicModuleTask
----------------
-Base class that handles the running, reprocessing and scheduling of the legacy logic modules in a way which enables
-integration with other platform functionality.
-
----------
-QueueTask
----------
-Base class used to create data pipelines that are fed via a queue.
-
----------
-Blob Task
----------
-
-Base class used to create integrations that post data to the Switch Automation Platform using a blob container & Event
-Hub Queue as the source.
-
-----------
-Automation
-----------
-This class contains the helper methods used to register, deploy, and test the created tasks. Additional helper
-functions for retrieving details of existing tasks on the Switch Automation Platform are also included in this module.
-
-"""
-
-from .pipeline import (IntegrationTask, QueueTask, LogicModuleTask, AnalyticsTask, DiscoverableIntegrationTask,
-                       EventWorkOrderTask, BlobTask, Guide, logger)
-from .automation import Automation
-from .definitions import (IntegrationDeviceConfigPropertyDefinition, IntegrationDeviceDefinition,
-                          AnalyticsSettings, EventWorkOrderFieldDefinition, IntegrationSettings)
-
-
-__all__ = ['IntegrationTask', 'QueueTask', 'LogicModuleTask', 'AnalyticsTask', 'DiscoverableIntegrationTask',
-           'EventWorkOrderTask', 'BlobTask', 'Guide', 'Automation', 'logger', 'IntegrationDeviceConfigPropertyDefinition',
-           'IntegrationDeviceDefinition', 'EventWorkOrderFieldDefinition',
-           'AnalyticsSettings', 'IntegrationSettings']
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""Module defining the Task abstract base class which is inherited by the following specific task types:
+
+- IntegrationTask
+- DiscoverableIntegrationTask
+- AnalyticsTask
+- LogicModuleTask
+- QueueTask
+- EventWorkOrderTask
+
+Also includes the Automation class which contains helper functions.
+
+---------------
+IntegrationTask
+---------------
+Base class used to create integrations between the Switch Automation Platform and other platforms, low-level services,
+or hardware.
+
+Examples include:
+    - Pulling readings or other types of data from REST APIs
+    - Protocol Translators which ingest data sent to the platform via email, ftp or direct upload within platform.
+
+---------------------------
+DiscoverableIntegrationTask
+---------------------------
+Base class used to create integrations between the Switch Automation Platform and 3rd party APIs.
+
+Similar to the IntegrationTask, but includes a secondary method `run_discovery()` which triggers discovery of available
+points on the 3rd party API and upserts these records to the Switch Platform backend so that the records are available
+in Build - Discovery & Selection UI.
+
+Examples include:
+    - Pulling readings or other types of data from REST APIs
+
+
+-------------
+AnalyticsTask
+-------------
+Base class used to create specific analytics functionality which may leverage existing data from the platform. Each
+task may add value to, or supplement, this data and write it back.
+
+Examples include:
+    - Anomaly Detection
+    - Leaky Pipes
+    - Peer Tracking
+
+---------------
+LogicModuleTask
+---------------
+Base class that handles the running, reprocessing and scheduling of the legacy logic modules in a way which enables
+integration with other platform functionality.
+
+---------
+QueueTask
+---------
+Base class used to create data pipelines that are fed via a queue.
+
+---------
+Blob Task
+---------
+
+Base class used to create integrations that post data to the Switch Automation Platform using a blob container & Event
+Hub Queue as the source.
+
+----------
+Automation
+----------
+This class contains the helper methods used to register, deploy, and test the created tasks. Additional helper
+functions for retrieving details of existing tasks on the Switch Automation Platform are also included in this module.
+
+"""
+
+from .pipeline import (IntegrationTask, QueueTask, LogicModuleTask, AnalyticsTask, DiscoverableIntegrationTask,
+                       EventWorkOrderTask, BlobTask, Guide, logger)
+from .automation import Automation
+from .definitions import (IntegrationDeviceConfigPropertyDefinition, IntegrationDeviceDefinition,
+                          AnalyticsSettings, EventWorkOrderFieldDefinition, IntegrationSettings)
+
+
+__all__ = ['IntegrationTask', 'QueueTask', 'LogicModuleTask', 'AnalyticsTask', 'DiscoverableIntegrationTask',
+           'EventWorkOrderTask', 'BlobTask', 'Guide', 'Automation', 'logger', 'IntegrationDeviceConfigPropertyDefinition',
+           'IntegrationDeviceDefinition', 'EventWorkOrderFieldDefinition',
+           'AnalyticsSettings', 'IntegrationSettings']
```

### Comparing `switch_api-0.5.4b2/switch_api/pipeline/automation.py` & `switch_api-0.5.4b3/switch_api/pipeline/automation.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,1805 +1,1805 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""Module defining the Automation class which contains register, deployment and helper methods.
-
-----------
-Automation
-----------
-
-This class contains the helper methods used to register, deploy, and test the created tasks. Additional helper functions
- for retrieving details of existing tasks on the Switch Automation Platform are also included in this module.
-
-"""
-from typing import List, Union
-import pandas
-import requests
-import inspect
-import json
-import logging
-import sys
-import uuid
-from io import StringIO
-from azure.servicebus import ServiceBusClient, ServiceBusReceiveMode
-from .._utils._utils import _is_valid_regex, ApiInputs, _column_name_cap, DataFeedFileProcessOutput
-from .._utils._marketplace import add_marketplace_item
-from .._utils._constants import (argus_prefix, EXPECTED_DELIVERY, MAPPING_ENTITIES,
-                                 QUEUE_NAME, ERROR_TYPE, SCHEDULE_TIMEZONE, DEPLOY_TYPE, TASK_PRIORITY, TASK_FRAMEWORK, GUIDES_EXTERNAL_TYPES, GUIDES_SCOPE)
-from .._utils._platform import _get_ingestion_service_bus_connection_string, Blob
-from .pipeline import (Task, QueueTask, IntegrationTask, LogicModuleTask, AnalyticsTask, EventWorkOrderTask,
-                       DiscoverableIntegrationTask, BlobTask, Guide)
-from ..extensions import ExtensionTask, replace_extensions_imports, has_extensions_support
-from .definitions import (IntegrationDeviceDefinition, EventWorkOrderFieldDefinition, AnalyticsSettings,
-                          IntegrationSettings, IntegrationDeviceConfigPropertyDefinition)
-
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-class Automation:
-    """Automation class defines the methods used to register and deploy tasks. """
-
-    @staticmethod
-    def run_queue_task(task: QueueTask, api_inputs: ApiInputs, consume_all_messages: bool = False):
-        """Runs a Queue Task when in Development Mode
-
-        The Queue Name should ideally be a testing Queue as messages will be consumed
-
-        Parameters
-        ----------
-        task : QueueTask
-            The custom QueueTask instance created by the user.
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        consume_all_messages : bool, default=False
-            Consume all messages as they are read.
-
-        Returns
-        -------
-        bool
-            indicating whether the call was successful
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        if not issubclass(type(task), QueueTask):
-            logger.error("Driver must be an implementation of the QueueTask (Task).")
-            return False
-
-        logger.info("Running queue for  %s (%s) using queue name: %s", str(type(task).__name__), str(task.id),
-                    str(task.queue_name))
-
-        task.start(api_inputs)
-
-        message_count = 0
-
-        with ServiceBusClient.from_connection_string(
-                _get_ingestion_service_bus_connection_string(api_inputs, task.queue_type)) as client:
-            logger.info(f"Preparing Receiver for Queue: {task.queue_name}")
-            with client.get_queue_receiver(task.queue_name,
-                                           receive_mode=ServiceBusReceiveMode.RECEIVE_AND_DELETE) as receiver:
-                while True:
-                    messages = []
-                    received_message_array = receiver.receive_messages(
-                        max_message_count=task.maximum_message_count_per_call, max_wait_time=2)
-                    for message in received_message_array:
-                        messages.append(str(message))
-                        message_count += 1
-
-                    if len(received_message_array) == 0:
-                        break
-
-                    task.process_queue(api_inputs, messages)
-
-                    if consume_all_messages == False:
-                        break
-        logger.info('Total messages consumed: %s', str(message_count))
-
-    @staticmethod
-    def reserve_instance(task: Task, api_inputs: ApiInputs, data_feed_id: uuid.UUID, minutes_to_reserve: int = 10):
-        """Reserve a testing instance.
-
-        Reserves a testing instance for the `driver`.
-
-        Parameters
-        ----------
-        task : Task
-            The custom Driver class created by the user.
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        data_feed_id : uuid.UUID
-            The unique identifier of the data feed being tested.
-        minutes_to_reserve : int, default = 10
-                The duration in minutes that the testing instance will be reserved for (Default value = 10).
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the details of the reserved testing instance.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        logger.info("Reserving a testing instance for %s (%s) on Data Feed Id (%s). ", str(type(task).__name__),
-                    str(task.id), str(data_feed_id))
-
-        url = argus_prefix + "ReserveInstance/" + str(data_feed_id) + "/" + str(minutes_to_reserve)
-        logger.info("Sending request: GET %s", url)
-
-        response = requests.request("GET", url, timeout=20, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text, typ="Series")
-
-        return df
-
-    @staticmethod
-    def register_task(api_inputs: ApiInputs, task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
-                                                         AnalyticsTask, LogicModuleTask, EventWorkOrderTask, ExtensionTask]):
-        """Register the task.
-
-        Registers the task that was defined.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        task : Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask, AnalyticsTask, LogicModuleTask, EventWorkOrderTask]
-            An instance of the custom class created from the Abstract Base Class `Task` or its abstract sub-classes:
-            `IntegrationTask`,`DiscoverableIntegrationTask`, `AnalyticsTask`, `QueueTask`, `EventWorkOrderTask`, 
-            `LogicModuleTask`, or `ExtensionTask`.
-
-        Returns
-        -------
-        pandas.DataFrame
-
-        """
-
-        if not issubclass(type(task), Task) and not issubclass(type(task), ExtensionTask):
-            logger.error("Driver must be an implementation of the Abstract Base Class (Task or ExtensionTask).")
-            return False
-
-        base_class = ''
-        allowed_base_classes = tuple([Task] + Task.__subclasses__() + [ExtensionTask])
-        if issubclass(type(task), allowed_base_classes):
-            if len(task.__class__.__bases__) == 1:
-                base_class = task.__class__.__base__.__qualname__
-            else:
-                base_classes = []
-                for i in range(len(task.__class__.__bases__)):
-                    base_classes.append(task.__class__.__bases__[i].__qualname__)
-                base_class = list(set(base_classes).symmetric_difference([Guide.__qualname__]))
-                base_class = base_class[0]
-
-        if base_class == '':
-            logger.error(f'Task must be an implementation of one of the Task sub-classes: {Task.__subclasses__()}')
-            return pandas.DataFrame()
-        elif not set([base_class]).issubset(
-                {allowed_base_classes[i].__qualname__ for i in range(len(allowed_base_classes))}):
-            logger.error(f"Task must be an implementation of one of the Task sub-classes:  {Task.__subclasses__()}")
-
-        if not issubclass(type(task), ExtensionTask):
-            if type(task.mapping_entities) != list:
-                logger.error('The mapping_entities parameter must have type = list. ')
-                return pandas.DataFrame()
-            elif not set(task.mapping_entities).issubset(set(MAPPING_ENTITIES.__args__)):
-                logger.error('mapping_entities property must be a list containing one of the allowed values defined by the '
-                             'MAPPING_ENTITIES literal: %s', MAPPING_ENTITIES.__args__)
-                return pandas.DataFrame()
-
-        task_settings = dict()
-
-        if base_class == 'DiscoverableIntegrationTask':
-            if not issubclass(type(task.integration_device_type_definition), IntegrationDeviceDefinition):
-                logger.error(f"The integration_device_type_definition property must be a subclass of the "
-                             f"IntegrationDeviceDefinition class. Current integration_device_type_definition property "
-                             f"is a {type(task.integration_device_type_definition)}")
-                return pandas.DataFrame()
-            elif issubclass(type(task.integration_device_type_definition), IntegrationDeviceDefinition):
-                props = task.integration_device_type_definition.config_properties
-
-                if type(task.integration_device_type_definition.config_properties) != list:
-                    logger.error("The task.integration_device_type_definition.config_properties must have type = list")
-                    return pandas.DataFrame()
-                else:
-                    integration_settings_list = list()
-                    if task.integration_device_type_definition.expose_address == True:
-                        integration_settings_list.append({
-                            'property_name': 'Address',
-                            'display_name': task.integration_device_type_definition.address_label,
-                            'editor': 'text_box', 'default_value': None, 'allowed_values': None})
-                    prop_types = {True: [], False: []}
-                    prop_issubclass = set()
-                    for setting in task.integration_device_type_definition.config_properties:
-                        val = issubclass(type(setting), IntegrationDeviceConfigPropertyDefinition)
-                        prop_issubclass.add(val)
-                        prop_types[val] += [setting]
-                        integration_settings_list.append({
-                            'property_name': setting.property_name, 'display_label': setting.display_label,
-                            'editor': setting.editor, 'default_value': setting.default_value,
-                            'allowed_values': setting.allowed_values})
-
-                    if not prop_issubclass.issubset({True}):
-                        logger.error(f"The task.integration_device_type_definition.config_properties parameter must be "
-                                     f"a list containing one or more subclasses of the "
-                                     f"IntegrationDeviceConfigPropertyDefinition class. {len(prop_types[False])} out of "
-                                     f"the {len(task.integration_device_type_definition.config_properties)} items in the"
-                                     f" list do not conform to this requirement. ")
-
-            if len(integration_settings_list) > 0:
-                task_settings['integration_settings'] = integration_settings_list
-
-        if base_class == 'EventWorkOrderTask':
-            # Validate that the work_order_fields_definition property matches required definition format.
-            if type(task.work_order_fields_definition) != list:
-                logger.error(f"The task.work_order_fields_definition property must have type list")
-                return pandas.DataFrame
-            else:
-                prop_types = {True: [], False: []}
-                prop_issubclass = set()
-                work_order_input_list = list()
-                for setting in task.work_order_fields_definition:
-                    val = issubclass(type(setting), EventWorkOrderFieldDefinition)
-                    prop_issubclass.add(val)
-                    prop_types[val] += [setting]
-                    work_order_input_list.append({'property_name': setting.property_name,
-                                                  'display_label': setting.display_label,
-                                                  'editor': setting.editor, 'default_value': setting.default_value,
-                                                  'allowed_values': setting.allowed_values})
-                if not prop_issubclass.issubset({True}):
-                    logger.error(f"The task.work_order_fields_definition property must be a list containing one or "
-                                 f"more subclasses of the EventWorkOrderFieldDefinition class. "
-                                 f"{len(prop_types[False])} out of the {len(task.work_order_fields_definition)} items "
-                                 f"in the list do not conform to this requirement. ")
-
-            # Validate that the integration_settings_definition property matches required definition format.
-            if type(task.integration_settings_definition) != list:
-                logger.error(f"The task.integration_settings_definition property must have type list")
-                return pandas.DataFrame
-            else:
-                prop_types = {True: [], False: []}
-                prop_issubclass = set()
-                integration_settings_list = list()
-                for setting in task.integration_settings_definition:
-                    val = issubclass(type(setting), IntegrationSettings)
-                    prop_issubclass.add(val)
-                    prop_types[val] += [setting]
-                    integration_settings_list.append({'property_name': setting.property_name,
-                                                      'display_label': setting.display_label,
-                                                      'editor': setting.editor, 'default_value': setting.default_value,
-                                                      'allowed_values': setting.allowed_values})
-                if not prop_issubclass.issubset({True}):
-                    logger.error(f"The task.integration_settings_definition property must be a list containing one or "
-                                 f"more subclasses of the IntegrationSettings class. {len(prop_types[False])} "
-                                 f"out of the {len(task.integration_settings_definition)} items in the list do not "
-                                 f"conform to this requirement. ")
-            if len(work_order_input_list) > 0 and len(integration_settings_list) > 0:
-                task_settings['work_order_input'] = work_order_input_list
-                task_settings['integration_settings'] = integration_settings_list
-
-        if base_class == 'AnalyticsTask':
-            # Validate that the analytics_settings_definition matches required definition format.
-            if type(task.analytics_settings_definition) != list:
-                logger.error("The task.analytics_settings_definition property must have type list")
-                return pandas.DataFrame
-            else:
-                prop_types = {True: [], False: []}
-                prop_issubclass = set()
-                analytics_settings_list = list()
-                for setting in task.analytics_settings_definition:
-                    val = issubclass(type(setting), AnalyticsSettings)
-                    prop_issubclass.add(val)
-                    prop_types[val] += [setting]
-                    analytics_settings_list.append({'property_name': setting.property_name,
-                                                    'display_label': setting.display_label,
-                                                    'editor': setting.editor, 'default_value': setting.default_value,
-                                                    'allowed_values': setting.allowed_values})
-                if not prop_issubclass.issubset({True}):
-                    logger.error(f"The task.analytics_settings_definition property must be a list containing one or "
-                                 f"more subclasses of the AnalyticsSettings class. "
-                                 f"{len(prop_types[False])} out of the {len(task.analytics_settings_definition)} items "
-                                 f"in the list do not conform to this requirement. ")
-
-            if len(analytics_settings_list) > 0:
-                task_settings['analytics_settings'] = analytics_settings_list
-
-        logger.info(f'Registering {str(type(task).__name__)} ({task.id}) to the Switch Driver Library:')
-
-        json_payload_verify = {
-            "driverId": str(task.id),
-            "name": type(task).__name__
-        }
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/drivers/verification"
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.post(url, json=json_payload_verify, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif response.text == "Failed":
-            logger.error("This Driver Guid is already associated with a Driver with a Different Name.")
-            return response_status, pandas.DataFrame()
-
-        script_code = replace_extensions_imports(task, 'extensions') if has_extensions_support(
-            task) else Automation._get_task_code(task)
-
-        json_payload = {
-            "driverId": str(task.id),
-            "name": type(task).__name__,
-            "specification": task.description,
-            "mappingEntities": task.mapping_entities if hasattr(task, 'mapping_entities') else [],
-            "scriptCode": script_code,
-            "baseClass": base_class,
-            "settings": task_settings
-            # {"integration_settings":[{"property_name":"Dummy1", "display_name":"", "default_value":"", "allowed_values":[], "editor":""}], "work_order_input":[], "analytics_settings":[], }
-        }
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/script-driver/registration"
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.post(url, json=json_payload, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text)
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def register_guide_task(api_inputs: ApiInputs, task, local_folder_path: str,
-                            external_type: GUIDES_EXTERNAL_TYPES = 'SwitchGuides',
-                            scope: GUIDES_SCOPE = 'Portfolio-wide'):
-        """Register Guide Task
-
-        Registers the task that was defined and registers the guide and associated files to the marketplace.
-
-        1. Registers the driver to Script Drivers Table
-        2. Upload Blob Files from local folder path
-        3. Register driver to Marketplace Items
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        task : Union[DiscoverableIntegrationTask, GuideTask]
-            An instance of the custom class created from the Abstract Base Class `Task` or its abstract sub-classes:
-            `DiscoverableIntegrationTask`, or `GuideTask`.
-        local_folder_path : str
-            Local file path to the `.vue` and `.js` form files to be uploaded to blob
-        external_type : GUIDES_EXTERNAL_TYPES, optional
-            Defaults to 'SwitchGuides'.
-        scope : GUIDES_SCOPE, optional
-            Marketplace item scope. Defaults to 'Portfolio-wide'.
-
-        Returns
-        -------
-        Union[bool, pandas.DataFrame, tuple]
-
-        """
-
-        if not api_inputs or not api_inputs.api_base_url or not api_inputs.bearer_token:
-            logger.error("You must call initialize() before using the API.")
-            return pandas.DataFrame()
-
-        if len(task.__class__.__bases__) == 1:
-            logger.error(f"The task must subclass Task or one of its subclasses as well as the Guide class")
-            return False
-        elif issubclass(type(task), Guide) != True:
-            logger.error(f"The task must subclass the Guide class in addition to one of the Task sub-classes. ")
-            return False
-
-        min_tags = task.minimum_tags_met()
-        if min_tags != True:
-            if min_tags == False:
-                logger.error(f"The 'guide_tags' dictionary must not be empty. ")
-            logger.error("The 'guide_tags' must be a dictionary with a minimum of one key. ")
-            return False
-
-        short_desc_length = task.check_short_desc_length()
-        if short_desc_length != True:
-            if short_desc_length == False:
-                logger.error("The 'guide_short_description' must be a string that does not exceed 250 characters. ")
-                return False
-            logger.error(f"The 'guide_short_description' must be a string that does not exceed 250 characters. Current "
-                         f"length: {short_desc_length}")
-            return False
-
-
-        logger.info("Registering task...")
-        response = Automation.register_task(api_inputs, task)
-
-        # make it so this prints back properly
-        pandas.set_option('display.max_columns', 10)
-
-        _, register = (0, response) if isinstance(response, pandas.DataFrame) else response
-
-
-        if response.shape[0] == 0:
-            logger.error("Task registration failed. The guide forms were not uploaded to blob and the guide was not "
-                         "registered to the marketplace. ")
-            return False
-        else:
-            logger.info(f"Task registered successfully: \n {register}")
-
-        logger.info("Uploading Guide form to Blob storage container...")
-        blob_response =  Blob._guide_blob_upload(api_inputs=api_inputs, local_folder_path=local_folder_path,
-                                                 driver_id=task.id)
-
-        if blob_response == False:
-            logger.error("Failed to upload blob files for guide registration and hence Marketplace "
-                         "Registration was not completed. You will need to reregister before the guide will be shown in "
-                         "the Marketplace.")
-            return register, blob_response
-        else:
-            logger.info(f"Guide form successfully uploaded to blob. ")
-
-        # Request Clone of Assets to DataCenters
-        headers = api_inputs.api_headers.default
-
-        payload = {
-           "folderPath": task.id + '/'
-        }
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/guide/form/assets-clone"
-
-        logger.info("Sending request: POST %s", url)
-
-        blob_sync_response = requests.post(url, json=payload, headers=headers)
-        response_status = '{} {}'.format(blob_sync_response.status_code, blob_sync_response.reason)
-
-        logger.info(f"Sync Guide Form assets for DriverId={task.id} is {response_status}")
-
-        # Register to Marketplace
-        marketplace_response = add_marketplace_item(
-            api_inputs=api_inputs, name=task.marketplace_name, short_description=task.guide_short_description,
-            description=task.guide_description, version=task.guide_version, external_type=external_type,
-            external_id=task.id, tags=task.guide_tags, scope=scope, image_file_name=task.image_file_name,
-            card_image_file_name=task.card_image_file_name)
-
-        __, marketplace_df = (0, response) if isinstance(response, pandas.DataFrame) else marketplace_response
-
-        if __ != 0 and marketplace_df.shape[0] > 0:
-            logger.error(f"Marketplace registration was not successful. Please retry. Error response: \n {marketplace_df}")
-            return register, blob_response, marketplace_df
-        elif __ != 0 and marketplace_df.shape[0] == 0:
-            logger.error(f"Marketplace registration was not successful. Please retry. ")
-            return register, blob_response, __
-        else:
-            logger.info(f"Marketplace registration successful. \n {marketplace_df}")
-
-        return register, blob_response, blob_sync_response, marketplace_df
-
-    @staticmethod
-    def list_tasks(api_inputs: ApiInputs, search_name_pattern: str = '*'):
-        """Get a list of the registered tasks.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        search_name_pattern : str, optional
-            A pattern that should be used as a filter when retrieving the list of deployed drivers
-            (Default value = '*').
-
-        Returns
-        -------
-        pandas.DataFrame
-            Dataframe containing the registered tasks.
-
-        """
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/drivers"
-
-        if search_name_pattern != '*' and search_name_pattern != '':
-            url += f"?name={search_name_pattern}"
-
-        logger.info("Sending request: GET %s", url)
-
-        response = requests.request("GET", url, timeout=20, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text)
-        df = df.drop(columns=['driverId'])
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def deploy_as_on_demand_data_feed(task: EventWorkOrderTask,
-                                      api_inputs: ApiInputs, data_feed_id: uuid.UUID, settings: dict,
-                                      queue_name: QUEUE_NAME = 'task', data_feed_name: str = None):
-        """Deploy the custom driver as an on demand data feed.
-
-        This deployment method is only suitable for the tasks that subclass the EventWorkOrderTask base class.
-
-        Parameters
-        ----------
-        task : EventWorkOrderTask
-            The custom driver class created from the Abstract Base Class `EventWorkOrderTask`
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        data_feed_id : uuid.UUID
-            The DataFeedId to update if existing, else will create a new record with the given value.
-        settings : dict
-            List of settings used to deploy the driver. May containing information needed to access 3rd party api
-             - e.g. URI, Username, Pwd, AccessToken, etc
-        queue_name : QUEUE_NAME, optional
-            The queue name (Default value = 'task').
-        data_feed_name : str, Optional
-            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
-            task name.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the details of the deployed https endpoint data feed.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        if not issubclass(task.__class__, EventWorkOrderTask):
-            logger.error("Only task derived from EventWorkOrderTask can be deployed as OnDemand.")
-            return pandas.DataFrame()
-
-        if not isinstance(settings, dict):
-            logger.error("setting parameter should be of type dict.")
-            return pandas.DataFrame()
-        elif not settings or not bool(settings) or len(settings) == 0:
-            logger.error("setting parameter for On Demand Data Feed cannot be empty.")
-            return pandas.DataFrame()
-
-        if data_feed_id is None and data_feed_name is None:
-            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
-            data_feed_name = task.__class__.__qualname__
-        elif data_feed_id is not None and data_feed_name is None:
-            data_feed_id = data_feed_id
-            data_feed_name = data_feed_name
-
-        headers = api_inputs.api_headers.default
-
-        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
-            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
-                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
-            return pandas.DataFrame()
-
-        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
-                    api_inputs.api_project_id)
-
-        payload = {
-            "dataFeedId": str(data_feed_id),
-            "driverId": str(task.id),
-            "name": data_feed_name,
-            "feedType": ",".join(task.mapping_entities),
-            "expectedDelivery": "NA",
-            "sourceType": "OnDemand",
-            "queueName": queue_name,
-            "settings": settings
-        }
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
-
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.post(url, json=payload, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(StringIO(response.text), typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def deploy_as_email_data_feed(task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
-                                              AnalyticsTask, LogicModuleTask, EventWorkOrderTask],
-                                  api_inputs: ApiInputs, data_feed_id: uuid.UUID,
-                                  expected_delivery: EXPECTED_DELIVERY, email_subject_regex: str,
-                                  email_address_domain: str, queue_name: QUEUE_NAME = 'task',
-                                  data_feed_name: str = None, task_priority: TASK_PRIORITY = 'default',
-                                  task_framework: TASK_FRAMEWORK = 'PythonScriptFramework'):
-        """Deploy task as an email data feed.
-
-        Deploys the created `task` as an email data feed. This allows the driver to ingest data sent via email. The
-        data must be sent to data@switchautomation.com to be processed. If it is sent to another email address, the task
-        will not be run.
-
-        Parameters
-        ----------
-        task : Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask, AnalyticsTask, LogicModuleTask,
-        EventWorkOrderTask]
-            The custom driver class created from the Abstract Base Class `Task`
-        api_inputs : ApiInputs
-            Object returned by initialize() function.
-        data_feed_id : uuid.UUID
-            The DataFeedId to update if existing, else will create a new record with the given value.
-        expected_delivery : EXPECTED_DELIVERY
-            The expected delivery frequency.
-        email_subject_regex : str
-            Regex expression used to parse the email subject line to determine which driver the received file will
-            be processed by.
-        email_address_domain : str
-            The email domain, without the @ symbol, of the sender. For example, if the email address that will send
-            file(s) for this data feed to the Switch Automation Platform is sender@test.com, the string that should be
-            passed to this parameter is "test.com".
-        queue_name : QUEUE_NAME, Optional
-            The name of queue (Default value = 'task').
-        data_feed_name : str, Optional
-            The name of the data feed (to be displayed in Task Insights UI). If not provided, the API will automatically
-            default to using the task.name property of the `task` passed to function.
-        task_priority: TASK_PRIORITY
-            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be alloted
-            to run the task - 'default`, 'standard', or 'advanced'. Defaults to 'default'.
-        task_framework: TASK_FRAMEWORK
-            Determines the framework of the datafeed tasks when processing.
-            'PythonScriptFramework' for the old task runner engine.
-            'TaskInsightsEngine' for the new task running in container apps.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the details of the deployed email data feed.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        if data_feed_id is None and data_feed_name is None:
-            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
-            data_feed_name = task.__class__.__qualname__
-        elif data_feed_id is not None and data_feed_name is None:
-            data_feed_id = data_feed_id
-            data_feed_name = data_feed_name
-
-        headers = api_inputs.api_headers.default
-
-        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
-                    api_inputs.api_project_id)
-
-        if not _is_valid_regex(email_subject_regex):
-            logger.error("%s is not valid regex.", email_subject_regex)
-            return pandas.DataFrame()
-
-        if email_address_domain == "switchautomation.com":
-            logger.info("Emails can only be received from the %s domain.", email_address_domain)
-
-        if "@" in email_address_domain:
-            logger.error("Do not include the @ in the email_address_domain parameter. ")
-            return pandas.DataFrame()
-
-        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
-            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
-                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
-            return pandas.DataFrame()
-
-        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
-            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
-                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
-            return pandas.DataFrame()
-
-        if not set([task_framework]).issubset(set(TASK_FRAMEWORK.__args__)):
-            logger.error('task_framework parameter must be set to one of the allowed values defined by the '
-                         'TASK_FRAMEWORK literal: %s', TASK_FRAMEWORK.__args__)
-            return pandas.DataFrame()
-
-        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__)):
-            logger.error('task_priority parameter must be set to one of the allowed values defined by the '
-                         'TASK_PRIORITY literal: %s', TASK_PRIORITY.__args__)
-            return pandas.DataFrame()
-
-        logger.info('Task Framework is "%s"', str(task_framework))
-        logger.info('Task Priotiy is "%s"', str(task_priority))
-
-        inbox_container = "data-exchange"
-        payload = {
-            "dataFeedId": str(data_feed_id),
-            "driverId": str(task.id),
-            "name": data_feed_name,
-            "feedType": ",".join(task.mapping_entities),
-            "expectedDelivery": expected_delivery,
-            "sourceType": "Email",
-            "queueName": queue_name,
-            "taskFramework": task_framework,
-            "taskPriority": task_priority,
-            "email": {
-                "emailAddressDomain": email_address_domain,
-                "emailSubjectRegex": email_subject_regex,
-                "container": inbox_container
-            }
-        }
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
-
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.post(url, json=payload, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text, typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def deploy_as_ftp_data_feed(task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
-                                            AnalyticsTask, LogicModuleTask, EventWorkOrderTask],
-                                api_inputs: ApiInputs, data_feed_id: uuid.UUID, expected_delivery: EXPECTED_DELIVERY,
-                                ftp_user_name: str, ftp_password: str, queue_name: QUEUE_NAME = 'task',
-                                data_feed_name: str = None, task_priority: TASK_PRIORITY = 'default',
-                                task_framework: TASK_FRAMEWORK = 'PythonScriptFramework'):
-        """Deploy the custom driver as an FTP data feed
-
-        Deploys the custom driver to receive data via an FTP data feed. Sets the `ftp_user_name` & `ftp_password` and
-        the `expected_delivery` of the file.
-
-        Parameters
-        ----------
-        task : Task
-            The custom driver class created from the Abstract Base Class 'Task'
-        api_inputs : ApiInputs
-            Object returned by the initialize() function.
-        data_feed_id : uuid.UUID
-            The DataFeedId to update if existing, else will create a new record with the given value.
-        expected_delivery : EXPECTED_DELIVERY
-            The expected delivery frequency of the data.
-        ftp_user_name : str
-            The user_name to be used by the ftp service to authenticate delivery of the data feed.
-        ftp_password : str
-            The password to be used by the ftp service for the given `ftp_user_name` to authenticate delivery of the
-            data feed.
-        queue_name : QUEUE_NAME, default = 'task'
-            The queue name (Default value = 'task').
-        data_feed_name : str, Optional
-            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
-            task name.
-        task_priority: TASK_PRIORITY
-            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be alloted
-            to run the task - 'default`, 'standard', or 'advanced'. Defaults to 'default'.
-        task_framework: TASK_FRAMEWORK
-            Determines the framework of the datafeed tasks when processing.
-            'PythonScriptFramework' for the old task runner engine.
-            'TaskInsightsEngine' for the new task running in container apps.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the details of the deployed ftp data feed.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        if data_feed_id is None and data_feed_name is None:
-            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
-            data_feed_name = task.__class__.__qualname__
-        elif data_feed_id is not None and data_feed_name is None:
-            data_feed_id = data_feed_id
-            data_feed_name = data_feed_name
-
-        headers = api_inputs.api_headers.default
-
-        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
-            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
-                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
-            return pandas.DataFrame()
-
-        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
-            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
-                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
-            return pandas.DataFrame()
-
-        if not set([task_framework]).issubset(set(TASK_FRAMEWORK.__args__)):
-            logger.error('task_framework parameter must be set to one of the allowed values defined by the '
-                         'TASK_FRAMEWORK literal: %s', TASK_FRAMEWORK.__args__)
-            return pandas.DataFrame()
-
-        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__)):
-            logger.error('task_priority parameter must be set to one of the allowed values defined by the '
-                         'TASK_PRIORITY literal: %s', TASK_PRIORITY.__args__)
-            return pandas.DataFrame()
-
-        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
-                    api_inputs.api_project_id)
-        logger.info('Task Framework is "%s"', str(task_framework))
-        logger.info('Task Priotiy is "%s"', str(task_priority))
-
-        inbox_container = "data-exchange"
-        payload = {
-            "dataFeedId": str(data_feed_id),
-            "driverId": str(task.id),
-            "name": data_feed_name,
-            "feedType": ",".join(task.mapping_entities),
-            "expectedDelivery": expected_delivery,
-            "sourceType": "Ftp",
-            "queueName": queue_name,
-            "taskFramework": task_framework,
-            "taskPriority": task_priority,
-            "ftp": {
-                "ftpUserName": ftp_user_name,
-                "ftpPassword": ftp_password,
-                "container": inbox_container
-            }
-        }
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
-
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.post(url, json=payload, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text, typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def deploy_as_upload_data_feed(task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
-                                               AnalyticsTask, LogicModuleTask, EventWorkOrderTask],
-                                   api_inputs: ApiInputs, data_feed_id: uuid.UUID, expected_delivery: EXPECTED_DELIVERY,
-                                   queue_name: QUEUE_NAME = 'task', data_feed_name: str = None,
-                                   task_priority: TASK_PRIORITY = 'default',
-                                   task_framework: TASK_FRAMEWORK = 'PythonScriptFramework'):
-        """Deploy the custom driver as a REST API end point Datafeed.
-
-        To upload a file to the deployed data feed, use the UploadUrl from the response dataframe (with request type
-        POST) with the following two headers:
-
-        - 'Ocp-Apim-Subscription-Key' - set to the value of ``api_inputs.subscription_key``
-        - 'Authorization' - set to the value 'Bearer ``api_inputs.bearer_token``'
-
-        For example, to upload a file using the ``requests`` package:
-
-        >>> import requests
-        >>> url = df.loc[0,'UploadUrl']
-        >>> payload={}
-        >>> file_path = 'C:/xxyyzz.txt'
-        >>> files={'file': open(file_path, 'rb')}
-        >>> headers = {'Ocp-Apim-Subscription-Key': api_inputs.subscription_key, 'Authorization': f'Bearer {api_inputs.bearer_token}'}
-        >>> response = requests.request("POST", url, headers=headers, data=payload, files=files)
-        >>> print(response.text)
-
-        Parameters
-        ----------
-        task : Task
-            The custom driver class created from the Abstract Base Class 'Task'
-        api_inputs : ApiInputs
-            Object returned by the initialize() function.
-        data_feed_id : uuid.UUID
-            The DataFeedId to update if existing, else will create a new record with the given value.
-        expected_delivery : EXPECTED_DELIVERY
-            The expected delivery frequency of the data.
-        queue_name : QUEUE_NAME, optional
-            The queue name (Default value = 'task').
-        data_feed_name : str, Optional
-            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
-            task name.
-        task_priority: TASK_PRIORITY
-            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be alloted
-            to run the task - 'default`, 'standard', or 'advanced'. Defaults to 'default'.
-        task_framework: TASK_FRAMEWORK
-            Determines the framework of the datafeed tasks when processing.
-            'PythonScriptFramework' for the old task runner engine.
-            'TaskInsightsEngine' for the new task running in container apps.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the details of the deployed https endpoint data feed.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        if data_feed_id is None and data_feed_name is None:
-            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
-            data_feed_name = task.__class__.__qualname__
-        elif data_feed_id is not None and data_feed_name is None:
-            data_feed_id = data_feed_id
-            data_feed_name = data_feed_name
-
-        headers = api_inputs.api_headers.default
-
-        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
-            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
-                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
-            return pandas.DataFrame()
-
-        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
-            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
-                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
-            return pandas.DataFrame()
-
-        if not set([task_framework]).issubset(set(TASK_FRAMEWORK.__args__)):
-            logger.error('task_framework parameter must be set to one of the allowed values defined by the '
-                         'TASK_FRAMEWORK literal: %s', TASK_FRAMEWORK.__args__)
-            return pandas.DataFrame()
-
-        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__)):
-            logger.error('task_priority parameter must be set to one of the allowed values defined by the '
-                         'TASK_PRIORITY literal: %s', TASK_PRIORITY.__args__)
-            return pandas.DataFrame()
-
-        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
-                    api_inputs.api_project_id)
-        logger.info('Task Framework is "%s"', str(task_framework))
-        logger.info('Task Priotiy is "%s"', str(task_priority))
-
-        payload = {
-            "dataFeedId": str(data_feed_id),
-            "driverId": str(task.id),
-            "name": data_feed_name,
-            "feedType": ",".join(task.mapping_entities),
-            "expectedDelivery": expected_delivery,
-            "sourceType": "Upload",
-            "queueName": queue_name,
-            "taskFramework": task_framework,
-            "taskPriority": task_priority,
-            "upload": {
-                "placeholder": ""
-            },
-        }
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
-
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.post(url, json=payload, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(StringIO(response.text), typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def deploy_as_blob_data_feed(task: BlobTask,
-                                 api_inputs: ApiInputs, data_feed_id: uuid.UUID, expected_delivery: EXPECTED_DELIVERY,
-                                 data_feed_name: str = None,
-                                 task_priority: TASK_PRIORITY = 'standard'):
-        """Deploy the BlobTask as a blob Datafeed.
-
-        Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can
-        be registered or deployed
-
-        Parameters
-        ----------
-        task : BlobTask
-            The custom driver class created from the Abstract Base Class 'BlobTask'
-        api_inputs : ApiInputs
-            Object returned by the initialize() function.
-        data_feed_id : uuid.UUID
-            The DataFeedId to update if existing, else will create a new record with the given value.
-        expected_delivery : EXPECTED_DELIVERY
-            The expected delivery frequency of the data.
-        data_feed_name : str, Optional
-            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
-            task name.
-        task_priority: TASK_PRIORITY
-            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be
-            alloted to run the task - 'standard' or 'advanced'. Defaults to 'standard'.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the details of the deployed blob data feed.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        if not issubclass(task.__class__, BlobTask):
-            logger.error("Only task derived from BlobTask can be deployed as Blob Data Feed.")
-            return pandas.DataFrame()
-
-        if data_feed_id is None and data_feed_name is None:
-            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
-            data_feed_name = task.__class__.__qualname__
-        elif data_feed_id is not None and data_feed_name is None:
-            data_feed_id = data_feed_id
-            data_feed_name = data_feed_name
-
-        headers = api_inputs.api_headers.default
-
-        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
-            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
-                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
-            return pandas.DataFrame()
-
-        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__).difference(set(["default"]))):
-            logger.error(f'task_priority parameter must be set to one of the allowed values defined by the '
-                         f'TASK_PRIORITY literal: {set(TASK_PRIORITY.__args__).difference(set(["default"]))}')
-            return pandas.DataFrame()
-
-        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
-                    api_inputs.api_project_id)
-        logger.info('Task Framework is "%s"', 'TaskInsightsEngine')
-        logger.info('Task Priotiy is "%s"', str(task_priority))
-
-        payload = {
-            "dataFeedId": str(data_feed_id),
-            "driverId": str(task.id),
-            "name": data_feed_name,
-            "feedType": ",".join(task.mapping_entities),
-            "expectedDelivery": expected_delivery,
-            "sourceType": "Blob",
-            "queueName": "task",
-            "taskFramework": "TaskInsightsEngine",
-            "taskPriority": task_priority,
-        }
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
-
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.post(url, json=payload, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(StringIO(response.text), typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def deploy_on_timer(task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
-                                    AnalyticsTask, LogicModuleTask, EventWorkOrderTask],
-                        api_inputs: ApiInputs, data_feed_id: uuid.UUID, expected_delivery: EXPECTED_DELIVERY,
-                        cron_schedule: str, queue_name: QUEUE_NAME = "task", settings: dict = None,
-                        schedule_timezone: SCHEDULE_TIMEZONE = 'Local', timezone_offset_minutes: int = None,
-                        data_feed_name: str = None, task_priority: TASK_PRIORITY = 'default',
-                        task_framework: TASK_FRAMEWORK = 'PythonScriptFramework'):
-        """Deploy driver to run on timer.
-
-        Parameters
-        ----------
-        task : Task
-            The custom driver class created from the Abstract Base Class `Task`.
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        data_feed_id : uuid.UUID
-            The DataFeedId to update if existing, else will create a new record with the given value.
-        expected_delivery : EXPECTED_DELIVERY
-            The expected delivery frequency.
-        cron_schedule : str
-            The CRONOS cron object containing the required schedule for the driver to be run. For details on the
-            required format, see: https://crontab.cronhub.io/
-        queue_name : QUEUE_NAME, optional
-            The queue name (Default value = 'task').
-        settings : dict, Optional
-            List of settings used to deploy the driver. For example, may contain the user_name and password required to
-            authenticate calls to a third-party API (Default value = None).
-        schedule_timezone : SCHEDULE_TIMEZONE, optional
-            Whether the ``cron_schedule`` should be applied based on Local or Utc timezone. If set to `Local`, this is
-            taken as the timezone of the western-most site in the given portfolio (Default value = 'Local').
-        timezone_offset_minutes: int, Optional
-            Timezone offset in minutes (from UTC) to be used when applying the ``cron_schedule`` (Default value = None).
-        data_feed_name : str, Optional
-            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
-            task name.
-        task_priority: TASK_PRIORITY
-            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be alloted
-            to run the task - 'default`, 'standard', or 'advanced'. Defaults to 'default'.
-        task_framework: TASK_FRAMEWORK
-            Determines the framework of the datafeed tasks when processing.
-            'PythonScriptFramework' for the old task runner engine.
-            'TaskInsightsEngine' for the new task running in container apps.
-
-        Returns
-        -------
-        pandas.Dataframe
-            A dataframe containing the details of the deployed data feed.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        if data_feed_id is None and data_feed_name is None:
-            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
-            data_feed_name = task.__class__.__qualname__
-        elif data_feed_id is not None and data_feed_name is None:
-            data_feed_id = data_feed_id
-            data_feed_name = data_feed_name
-
-        if timezone_offset_minutes is None:
-            timezone_offset_minutes = 0
-
-        headers = api_inputs.api_headers.default
-
-        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
-            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
-                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
-            return pandas.DataFrame()
-
-        if not set([schedule_timezone]).issubset(set(SCHEDULE_TIMEZONE.__args__)):
-            logger.error('schedule_timezone parameter must be set to one of the allowed values defined by the '
-                         'SCHEDULE_TIMEZONE literal: %s', SCHEDULE_TIMEZONE.__args__)
-            return pandas.DataFrame()
-
-        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
-            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
-                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
-            return pandas.DataFrame()
-
-        if 5 > len(cron_schedule.split(' ')) > 6:
-            logger.error("cron_schedule parameter must be in the format * * * * *")
-            return pandas.DataFrame()
-
-        # if task.__class__.__qualname__ == 'DiscoverableIntegrationTask':
-        #     config_props = task.integration_device_type_definition.config_properties
-        #     property_name_dict = dict
-        #     for i in range(len(config_props)):
-        #         property_name_dict[config_props[i].property_name] = config_props[i].default_value
-
-        # if task.__class__.__qualname__ == 'EventWorkOrderTask':
-        #     config_props = task.integration_settings_definition
-        #     property_name_list = []
-        #     property_defaults_dict = {}
-        #     for i in len(config_props):
-        #         property_name_list.append(config_props[i].property_name)
-        #         property_defaults_dict[config_props[i].property_name] = config_props[i].default_value
-        #
-        #     if settings is not None and not set(settings.keys()).issubset(set(property_name_list)):
-        #         missing_keys = list(set(property_name_list).difference(set(settings.keys())))
-        #         missing_required_keys = []
-        #         missing_optional_keys = []
-        #         for j in range(len(missing_keys)):
-        #             if property_defaults_dict[missing_keys[j]] is None or property_defaults_dict[missing_keys[j]] == '':
-        #                 missing_required_keys.append(missing_keys[j])
-        #                 missing_optional_keys
-        #             else:
-        #                 missing_optional_keys.append(missing_keys[j])
-        #                 missing_required_keys
-        #         if len(missing_required_keys) > 0 and len(missing_optional_keys) == 0:
-        #             loggger.error(f"settings parameter is missing the following required key(s): {missing_required_keys}.")
-
-        if not set([task_framework]).issubset(set(TASK_FRAMEWORK.__args__)):
-            logger.error('task_framework parameter must be set to one of the allowed values defined by the '
-                         'TASK_FRAMEWORK literal: %s', TASK_FRAMEWORK.__args__)
-            return pandas.DataFrame()
-
-        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__)):
-            logger.error('task_priority parameter must be set to one of the allowed values defined by the '
-                         'TASK_PRIORITY literal: %s', TASK_PRIORITY.__args__)
-            return pandas.DataFrame()
-
-        logger.info('Deploy %s (%s) on timer for ApiProjectID: %s and schedule: %s.', type(task).__name__,
-                    str(task.id), api_inputs.api_project_id, cron_schedule)
-        logger.info('Feed Type is %s', str(task.mapping_entities))
-        logger.info('Task Framework is "%s"', str(task_framework))
-        logger.info('Task Priotiy is "%s"', str(task_priority))
-        logger.info('Settings to be passed to the driver on start are: %s', str(settings))
-
-        payload = {
-            "dataFeedId": str(data_feed_id),
-            "driverId": str(task.id),
-            "name": data_feed_name,
-            "feedType": ",".join(task.mapping_entities),
-            "expectedDelivery": expected_delivery,
-            "sourceType": "Timer",
-            "queueName": queue_name,
-            "taskFramework": task_framework,
-            "taskPriority": task_priority,
-            "timer": {
-                "cronSchedule": cron_schedule,
-                "timezoneOffsetMinutes": timezone_offset_minutes,
-                "scheduleTimezone": schedule_timezone
-            },
-            "settings": settings
-        }
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
-
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.post(url, json=payload, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200 and len(response.text) > 0:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            error_df = pandas.read_json(response.text)
-            return response_status, error_df
-        elif response.status_code != 200 and len(response.text) == 0:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text, typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def cancel_deployed_data_feed(api_inputs: ApiInputs, data_feed_id: uuid.UUID, deployment_type: List[DEPLOY_TYPE]):
-        """Cancel deployment for a given `data_feed_id` and `deployment_type`
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        data_feed_id: uuid.UUID
-            Datafeed Id to cancel deployment
-        deployment_type: List[DEPLOY_TYPE]
-
-
-        Returns
-        -------
-        str
-            A string containing the response text.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        if data_feed_id == None or data_feed_id == '':
-            logger.error('Data feed Id cannot be Empty or None')
-            return
-
-        if type(deployment_type) != list:
-            logger.error('deployment_type should be of type  list.')
-            return
-        elif deployment_type == None or len(deployment_type) == 0:
-            logger.error('deployment_type cannot be empty. Please specify deployment_type to cancel deployment.')
-            return
-        elif not set(deployment_type).issubset(set(DEPLOY_TYPE.__args__)):
-            logger.error('deployment_type item parameter must be set to one of the allowed values defined by the '
-                         'DEPLOY_TYPE literal: %s', DEPLOY_TYPE.__args__)
-            return
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/deployment/" \
-              f"{data_feed_id}/cancel"
-
-        logger.info("Cancel deployment type/s with DataFeedID: %s for ApiProjectID: %s", data_feed_id,
-                    api_inputs.api_project_id)
-        logger.info("Source Types: %s", ",".join(deployment_type))
-        logger.info("Sending request: POST %s", url)
-
-        payload = {
-            'sourceTypes': deployment_type
-        }
-
-        response = requests.post(url, headers=headers, json=deployment_type)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        # Check if there's any timer deploy_type being cancelled, delete it as well by calling delete method
-        if any("Timer" in deploy_type for deploy_type in deployment_type):
-            return Automation.delete_deployed_data_feed(api_inputs=api_inputs, data_feed_id=data_feed_id)
-
-        df = pandas.read_json(response.text, typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def delete_deployed_data_feed(api_inputs: ApiInputs, data_feed_id: uuid.UUID):
-        """Delete deployment for a given `data_feed_id`.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        data_feed_id: uuid.UUID
-            Datafeed Id to cancel deployment
-
-        Returns
-        -------
-        str
-            A string containing the response text.
-
-        """
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        if data_feed_id == None or data_feed_id == '':
-            logger.error('Data feed Id cannot be None')
-            return
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/deployment/" \
-              f"{data_feed_id}/delete"
-
-        logger.info("Delete deployment with DataFeedID: %s for ApiProjectID: %s", data_feed_id,
-                    api_inputs.api_project_id)
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.delete(url, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text, typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def list_deployments(api_inputs: ApiInputs, search_name_pattern='*'):
-        """Retrieve list of deployed drivers.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        search_name_pattern : str
-                A pattern that should be used as a filter when retrieving the list of deployed drivers
-                (Default value = '*').
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the drivers deployed for the given ApiProjectID that match the `search_name_pattern`.
-
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/search"
-
-        if search_name_pattern != '' and search_name_pattern != '*':
-            url += f"?name={search_name_pattern}"
-
-        logger.info("Sending request: GET %s", url)
-
-        response = requests.request("GET", url, timeout=20, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        # We need to wrap json results in StringIO when stringified JSON is present in the response.
-        # Symptoms of not doing this are: ValueError: Unexpected character found when decoding object value
-        # and incorrect suggestion in the error on installing a package called 'fsspec'
-        # Installing 'fsspec' package will not solve the issue.
-        df = pandas.read_json(StringIO(response.text))
-        df.columns = _column_name_cap(df.columns)
-        return df
-
-    @staticmethod
-    def list_data_feed_history(api_inputs: ApiInputs, data_feed_id: uuid.UUID, top_count: int = 10):
-        """Retrieve data feed history
-
-        Retrieves the `top_count` records for the given `data_feed_id`.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        data_feed_id : uuid.UUID
-            The unique identifier for the data feed that history should be retrieved for.
-        top_count : int, default = 10
-            The top record count to be retrieved. (Default value = 10).
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the `top_count` history records for the given `data_feed_id`.
-
-        """
-
-        if data_feed_id is None or data_feed_id == '':
-            logger.error('The data_feed_id parameter must be provided. ')
-            return
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
-              f"{data_feed_id}/history?topCount={top_count}"
-
-        logger.info("Sending request: GET %s", url)
-
-        response = requests.request("GET", url, timeout=20, headers=headers)
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text)
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def run_data_feed(api_inputs: ApiInputs, data_feed_id: uuid.UUID):
-        """Trigger an on-demand run of the python job based on data feed id. This will be sent to the queue for
-        processing and will undergo same procedure as the rest of the datafeed.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        data_feed_id : uuid.UUID
-            The unique identifier for the data feed that history should be retrieved for.
-        """
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        if data_feed_id is None or data_feed_id == '':
-            logger.error('The data_feed_id parameter must be provided. ')
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/datafeed/{data_feed_id}/execute"
-
-        logger.info("Sending request: POST %s", url)
-
-        response = requests.request("POST", url, timeout=20, headers=headers)
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        # We need to wrap json results in StringIO when stringified JSON is present in the response.
-        # Symptoms of not doing this are: ValueError: Unexpected character found when decoding object value
-        # and incorrect suggestion in the error on installing a package called 'fsspec'
-        # Installing 'fsspec' package will not solve the issue.
-        df = pandas.read_json(StringIO(response.text),
-                              typ='Series').to_frame().T
-        df.columns = _column_name_cap(df.columns)
-        return df
-
-    @staticmethod
-    def data_feed_history_process_output(api_inputs: ApiInputs, data_feed_id: uuid.UUID = None,
-                                         data_feed_file_status_id: uuid.UUID = None, row_number: int = None):
-        """Retrieve data feed history process output
-
-        Retrieves the `top_count` records for the given `data_feed_id`.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        data_feed_id : uuid.UUID, default = None
-            The unique identifier for the data feed that output should be retrieved for. This parameter works with the
-            `row_number`
-        data_feed_file_status_id : uuid.UUID, default = None
-            The unique identifier for the data feed history should be retrieved for. This UUID can be retrieved from
-            the list_data_feed_history() method.
-        row_number: int, default = 0
-            The row number from the list_data_feed_history method. It can be used in place of the file_status_id. Use
-            row_number=0 to retrieve the most recent process.
-
-        Returns
-        -------
-        log : str
-            Containing the full print and output log for the process
-
-        """
-
-        if data_feed_id is None and data_feed_file_status_id is None:
-            logger.error('Must supply either the data_feed_id + row_number, or the file_status_id')
-            return
-        elif data_feed_id is not None and data_feed_file_status_id is None and row_number is None:
-            logger.error('When supplying the data_feed_id, you must also provide the row_number to retrieve. ')
-            return
-        elif data_feed_id is None and data_feed_file_status_id is not None and row_number is not None:
-            logger.error('Please supply either the data_feed_id + row_number, or the file_status_id. ')
-            return
-        elif data_feed_id is not None and data_feed_file_status_id is not None and row_number is not None:
-            logger.error('Please supply either the data_feed_id + row_number, or the file_status_id. ')
-            return
-        elif data_feed_id is not None and data_feed_file_status_id is not None and row_number is None:
-            logger.error('Please supply either the data_feed_id + row_number or the file_status_id. ')
-            return
-
-        if row_number is None:
-            row_number = -1
-
-        if data_feed_id is None:
-            data_feed_id = '00000000-0000-0000-0000-000000000000'
-
-        if data_feed_file_status_id is None:
-            data_feed_file_status_id = '00000000-0000-0000-0000-000000000000'
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
-              f"{data_feed_id}/file-status/{data_feed_file_status_id}/process-output?rowCount={row_number}"
-        logger.info("Sending request: GET %s", url)
-
-        response = requests.request("GET", url, timeout=20, headers=headers)
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        data_feed_file_process_output = json.loads(response.text, object_hook=lambda d: DataFeedFileProcessOutput(
-            data_feed_id=d['dataFeedId'], data_feed_file_status_id=d['fileStatusId'],
-            client_tracking_id=d['clientTrackingId'],
-            source_type=d['sourceType'], file_name=d['fileName'], file_received=d['fileReceived'],
-            file_process_status=d['fileProcessStatus'], file_process_status_change=d['fileProcessStatusChange'],
-            process_started=d['processStarted'], process_completed=d['processCompleted'],
-            minutes_since_received=d['minutesSinceReceived'], minutes_since_processed=d['minutesSinceProcessed'],
-            file_size=d['fileSize'], log_file_path=d['logFile'], output=d['output'], error=d['error']))
-
-        return data_feed_file_process_output
-
-    @staticmethod
-    def data_feed_history_process_errors(api_inputs: ApiInputs, data_feed_id: uuid.UUID,
-                                         data_feed_file_status_id: uuid.UUID):
-        """Retrieve the unique error types for a given data feed file.
-
-        Retrieves the distinct error types present for the given `data_feed_id` and `data_feed_file_status_id`.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        data_feed_id : uuid.UUID
-            The unique identifier for the data feed that errors should be retrieved for.
-        data_feed_file_status_id : uuid.UUID
-            The unique identifier for the file processed for a given data feed that errors should be retrieved for.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the distinct error types present for the given data_feed_file_status_id.
-
-        """
-
-        if data_feed_id == '' or data_feed_file_status_id == '':
-            logger.error('The data_feed_id and data_feed_file_status_id parameters must be provided. ')
-            return
-
-        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
-            logger.error("You must call initialize() before using API.")
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
-              f"{data_feed_id}/file-status/{data_feed_file_status_id}/process-errors"
-        logger.info("Sending request: GET %s", url)
-
-        response = requests.request("GET", url, timeout=20, headers=headers)
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        df = pandas.read_json(response.text)
-        df.columns = _column_name_cap(df.columns)
-
-        return df
-
-    @staticmethod
-    def data_feed_history_errors_by_type(api_inputs: ApiInputs, data_feed_id: uuid.UUID,
-                                         data_feed_file_status_id: uuid.UUID, error_type: ERROR_TYPE):
-        """Retrieve the encountered errors for a given error type, data feed & file.
-
-        Retrieves the errors identified for a given `error_type` for the `data_feed_id` and `data_feed_file_status_id`.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            Object returned by initialize.initialize() function
-        data_feed_id : uuid.UUID
-            The unique identifier for the data feed that errors should be retrieved for.
-        data_feed_file_status_id
-            The unique identifier for the data feed that errors should be retrieved for.
-        error_type: ERROR_TYPE
-            The error type of the structured logs to be retrieved.
-
-        Returns
-        -------
-        df : pandas.DataFrame
-            Dataframe containing the errors identified for the given `data_feed_file_status_id`.
-
-        """
-
-        if (data_feed_id == '' or data_feed_file_status_id == '' or data_feed_id is None or
-                data_feed_file_status_id is None):
-            logger.error('The data_feed_id and data_feed_file_status_id parameters must be provided. ')
-            return
-
-        if not set([error_type]).issubset(set(ERROR_TYPE.__args__)):
-            logger.error('error_type parameter must be set to one of the allowed values defined by the '
-                         'ERROR_TYPE literal: %s', ERROR_TYPE.__args__)
-            return pandas.DataFrame()
-
-        headers = api_inputs.api_headers.default
-
-        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
-              f"{data_feed_id}/file-status/{data_feed_file_status_id}/process-errors/type/{error_type}"
-
-        logger.info("Sending request: GET %s", url)
-
-        response = requests.request("GET", url, timeout=20, headers=headers)
-
-        response_status = '{} {}'.format(response.status_code, response.reason)
-        if response.status_code != 200:
-            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
-                         response.reason)
-            return response_status, pandas.DataFrame()
-        elif len(response.text) == 0:
-            logger.error('No data returned for this API call. %s', response.request.url)
-            return response_status, pandas.DataFrame()
-
-        # , error_bad_lines=False)
-        df = pandas.read_csv(StringIO(response.text), sep=",")
-        return df
-
-    @staticmethod
-    def _get_task_code(task):
-        """
-
-        Parameters
-        ----------
-        task : Task
-            The custom driver class created from the Abstract Base Class `Task`.
-
-        Returns
-        -------
-        driver_code : str
-            The code used to create the `task`
-
-        """
-        task_type = type(task)
-        task_code = ''
-        parent_module = inspect.getmodule(task_type)
-        for codeLine in inspect.getsource(parent_module).split('\n'):
-            if codeLine.startswith('import ') or codeLine.startswith('from '):
-                task_code += codeLine + '\n'
-
-        task_code += '\n'
-        task_code += inspect.getsource(task_type)
-        task_code += ''
-        task_code += 'task = ' + task_type.__name__ + '()'
-
-        return task_code
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""Module defining the Automation class which contains register, deployment and helper methods.
+
+----------
+Automation
+----------
+
+This class contains the helper methods used to register, deploy, and test the created tasks. Additional helper functions
+ for retrieving details of existing tasks on the Switch Automation Platform are also included in this module.
+
+"""
+from typing import List, Union
+import pandas
+import requests
+import inspect
+import json
+import logging
+import sys
+import uuid
+from io import StringIO
+from azure.servicebus import ServiceBusClient, ServiceBusReceiveMode
+from .._utils._utils import _is_valid_regex, ApiInputs, _column_name_cap, DataFeedFileProcessOutput
+from .._utils._marketplace import add_marketplace_item
+from .._utils._constants import (argus_prefix, EXPECTED_DELIVERY, MAPPING_ENTITIES,
+                                 QUEUE_NAME, ERROR_TYPE, SCHEDULE_TIMEZONE, DEPLOY_TYPE, TASK_PRIORITY, TASK_FRAMEWORK, GUIDES_EXTERNAL_TYPES, GUIDES_SCOPE)
+from .._utils._platform import _get_ingestion_service_bus_connection_string, Blob
+from .pipeline import (Task, QueueTask, IntegrationTask, LogicModuleTask, AnalyticsTask, EventWorkOrderTask,
+                       DiscoverableIntegrationTask, BlobTask, Guide)
+from ..extensions import ExtensionTask, replace_extensions_imports, has_extensions_support
+from .definitions import (IntegrationDeviceDefinition, EventWorkOrderFieldDefinition, AnalyticsSettings,
+                          IntegrationSettings, IntegrationDeviceConfigPropertyDefinition)
+
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+class Automation:
+    """Automation class defines the methods used to register and deploy tasks. """
+
+    @staticmethod
+    def run_queue_task(task: QueueTask, api_inputs: ApiInputs, consume_all_messages: bool = False):
+        """Runs a Queue Task when in Development Mode
+
+        The Queue Name should ideally be a testing Queue as messages will be consumed
+
+        Parameters
+        ----------
+        task : QueueTask
+            The custom QueueTask instance created by the user.
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        consume_all_messages : bool, default=False
+            Consume all messages as they are read.
+
+        Returns
+        -------
+        bool
+            indicating whether the call was successful
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        if not issubclass(type(task), QueueTask):
+            logger.error("Driver must be an implementation of the QueueTask (Task).")
+            return False
+
+        logger.info("Running queue for  %s (%s) using queue name: %s", str(type(task).__name__), str(task.id),
+                    str(task.queue_name))
+
+        task.start(api_inputs)
+
+        message_count = 0
+
+        with ServiceBusClient.from_connection_string(
+                _get_ingestion_service_bus_connection_string(api_inputs, task.queue_type)) as client:
+            logger.info(f"Preparing Receiver for Queue: {task.queue_name}")
+            with client.get_queue_receiver(task.queue_name,
+                                           receive_mode=ServiceBusReceiveMode.RECEIVE_AND_DELETE) as receiver:
+                while True:
+                    messages = []
+                    received_message_array = receiver.receive_messages(
+                        max_message_count=task.maximum_message_count_per_call, max_wait_time=2)
+                    for message in received_message_array:
+                        messages.append(str(message))
+                        message_count += 1
+
+                    if len(received_message_array) == 0:
+                        break
+
+                    task.process_queue(api_inputs, messages)
+
+                    if consume_all_messages == False:
+                        break
+        logger.info('Total messages consumed: %s', str(message_count))
+
+    @staticmethod
+    def reserve_instance(task: Task, api_inputs: ApiInputs, data_feed_id: uuid.UUID, minutes_to_reserve: int = 10):
+        """Reserve a testing instance.
+
+        Reserves a testing instance for the `driver`.
+
+        Parameters
+        ----------
+        task : Task
+            The custom Driver class created by the user.
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        data_feed_id : uuid.UUID
+            The unique identifier of the data feed being tested.
+        minutes_to_reserve : int, default = 10
+                The duration in minutes that the testing instance will be reserved for (Default value = 10).
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the details of the reserved testing instance.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        logger.info("Reserving a testing instance for %s (%s) on Data Feed Id (%s). ", str(type(task).__name__),
+                    str(task.id), str(data_feed_id))
+
+        url = argus_prefix + "ReserveInstance/" + str(data_feed_id) + "/" + str(minutes_to_reserve)
+        logger.info("Sending request: GET %s", url)
+
+        response = requests.request("GET", url, timeout=20, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text, typ="Series")
+
+        return df
+
+    @staticmethod
+    def register_task(api_inputs: ApiInputs, task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
+                                                         AnalyticsTask, LogicModuleTask, EventWorkOrderTask, ExtensionTask]):
+        """Register the task.
+
+        Registers the task that was defined.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        task : Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask, AnalyticsTask, LogicModuleTask, EventWorkOrderTask]
+            An instance of the custom class created from the Abstract Base Class `Task` or its abstract sub-classes:
+            `IntegrationTask`,`DiscoverableIntegrationTask`, `AnalyticsTask`, `QueueTask`, `EventWorkOrderTask`, 
+            `LogicModuleTask`, or `ExtensionTask`.
+
+        Returns
+        -------
+        pandas.DataFrame
+
+        """
+
+        if not issubclass(type(task), Task) and not issubclass(type(task), ExtensionTask):
+            logger.error("Driver must be an implementation of the Abstract Base Class (Task or ExtensionTask).")
+            return False
+
+        base_class = ''
+        allowed_base_classes = tuple([Task] + Task.__subclasses__() + [ExtensionTask])
+        if issubclass(type(task), allowed_base_classes):
+            if len(task.__class__.__bases__) == 1:
+                base_class = task.__class__.__base__.__qualname__
+            else:
+                base_classes = []
+                for i in range(len(task.__class__.__bases__)):
+                    base_classes.append(task.__class__.__bases__[i].__qualname__)
+                base_class = list(set(base_classes).symmetric_difference([Guide.__qualname__]))
+                base_class = base_class[0]
+
+        if base_class == '':
+            logger.error(f'Task must be an implementation of one of the Task sub-classes: {Task.__subclasses__()}')
+            return pandas.DataFrame()
+        elif not set([base_class]).issubset(
+                {allowed_base_classes[i].__qualname__ for i in range(len(allowed_base_classes))}):
+            logger.error(f"Task must be an implementation of one of the Task sub-classes:  {Task.__subclasses__()}")
+
+        if not issubclass(type(task), ExtensionTask):
+            if type(task.mapping_entities) != list:
+                logger.error('The mapping_entities parameter must have type = list. ')
+                return pandas.DataFrame()
+            elif not set(task.mapping_entities).issubset(set(MAPPING_ENTITIES.__args__)):
+                logger.error('mapping_entities property must be a list containing one of the allowed values defined by the '
+                             'MAPPING_ENTITIES literal: %s', MAPPING_ENTITIES.__args__)
+                return pandas.DataFrame()
+
+        task_settings = dict()
+
+        if base_class == 'DiscoverableIntegrationTask':
+            if not issubclass(type(task.integration_device_type_definition), IntegrationDeviceDefinition):
+                logger.error(f"The integration_device_type_definition property must be a subclass of the "
+                             f"IntegrationDeviceDefinition class. Current integration_device_type_definition property "
+                             f"is a {type(task.integration_device_type_definition)}")
+                return pandas.DataFrame()
+            elif issubclass(type(task.integration_device_type_definition), IntegrationDeviceDefinition):
+                props = task.integration_device_type_definition.config_properties
+
+                if type(task.integration_device_type_definition.config_properties) != list:
+                    logger.error("The task.integration_device_type_definition.config_properties must have type = list")
+                    return pandas.DataFrame()
+                else:
+                    integration_settings_list = list()
+                    if task.integration_device_type_definition.expose_address == True:
+                        integration_settings_list.append({
+                            'property_name': 'Address',
+                            'display_name': task.integration_device_type_definition.address_label,
+                            'editor': 'text_box', 'default_value': None, 'allowed_values': None})
+                    prop_types = {True: [], False: []}
+                    prop_issubclass = set()
+                    for setting in task.integration_device_type_definition.config_properties:
+                        val = issubclass(type(setting), IntegrationDeviceConfigPropertyDefinition)
+                        prop_issubclass.add(val)
+                        prop_types[val] += [setting]
+                        integration_settings_list.append({
+                            'property_name': setting.property_name, 'display_label': setting.display_label,
+                            'editor': setting.editor, 'default_value': setting.default_value,
+                            'allowed_values': setting.allowed_values})
+
+                    if not prop_issubclass.issubset({True}):
+                        logger.error(f"The task.integration_device_type_definition.config_properties parameter must be "
+                                     f"a list containing one or more subclasses of the "
+                                     f"IntegrationDeviceConfigPropertyDefinition class. {len(prop_types[False])} out of "
+                                     f"the {len(task.integration_device_type_definition.config_properties)} items in the"
+                                     f" list do not conform to this requirement. ")
+
+            if len(integration_settings_list) > 0:
+                task_settings['integration_settings'] = integration_settings_list
+
+        if base_class == 'EventWorkOrderTask':
+            # Validate that the work_order_fields_definition property matches required definition format.
+            if type(task.work_order_fields_definition) != list:
+                logger.error(f"The task.work_order_fields_definition property must have type list")
+                return pandas.DataFrame
+            else:
+                prop_types = {True: [], False: []}
+                prop_issubclass = set()
+                work_order_input_list = list()
+                for setting in task.work_order_fields_definition:
+                    val = issubclass(type(setting), EventWorkOrderFieldDefinition)
+                    prop_issubclass.add(val)
+                    prop_types[val] += [setting]
+                    work_order_input_list.append({'property_name': setting.property_name,
+                                                  'display_label': setting.display_label,
+                                                  'editor': setting.editor, 'default_value': setting.default_value,
+                                                  'allowed_values': setting.allowed_values})
+                if not prop_issubclass.issubset({True}):
+                    logger.error(f"The task.work_order_fields_definition property must be a list containing one or "
+                                 f"more subclasses of the EventWorkOrderFieldDefinition class. "
+                                 f"{len(prop_types[False])} out of the {len(task.work_order_fields_definition)} items "
+                                 f"in the list do not conform to this requirement. ")
+
+            # Validate that the integration_settings_definition property matches required definition format.
+            if type(task.integration_settings_definition) != list:
+                logger.error(f"The task.integration_settings_definition property must have type list")
+                return pandas.DataFrame
+            else:
+                prop_types = {True: [], False: []}
+                prop_issubclass = set()
+                integration_settings_list = list()
+                for setting in task.integration_settings_definition:
+                    val = issubclass(type(setting), IntegrationSettings)
+                    prop_issubclass.add(val)
+                    prop_types[val] += [setting]
+                    integration_settings_list.append({'property_name': setting.property_name,
+                                                      'display_label': setting.display_label,
+                                                      'editor': setting.editor, 'default_value': setting.default_value,
+                                                      'allowed_values': setting.allowed_values})
+                if not prop_issubclass.issubset({True}):
+                    logger.error(f"The task.integration_settings_definition property must be a list containing one or "
+                                 f"more subclasses of the IntegrationSettings class. {len(prop_types[False])} "
+                                 f"out of the {len(task.integration_settings_definition)} items in the list do not "
+                                 f"conform to this requirement. ")
+            if len(work_order_input_list) > 0 and len(integration_settings_list) > 0:
+                task_settings['work_order_input'] = work_order_input_list
+                task_settings['integration_settings'] = integration_settings_list
+
+        if base_class == 'AnalyticsTask':
+            # Validate that the analytics_settings_definition matches required definition format.
+            if type(task.analytics_settings_definition) != list:
+                logger.error("The task.analytics_settings_definition property must have type list")
+                return pandas.DataFrame
+            else:
+                prop_types = {True: [], False: []}
+                prop_issubclass = set()
+                analytics_settings_list = list()
+                for setting in task.analytics_settings_definition:
+                    val = issubclass(type(setting), AnalyticsSettings)
+                    prop_issubclass.add(val)
+                    prop_types[val] += [setting]
+                    analytics_settings_list.append({'property_name': setting.property_name,
+                                                    'display_label': setting.display_label,
+                                                    'editor': setting.editor, 'default_value': setting.default_value,
+                                                    'allowed_values': setting.allowed_values})
+                if not prop_issubclass.issubset({True}):
+                    logger.error(f"The task.analytics_settings_definition property must be a list containing one or "
+                                 f"more subclasses of the AnalyticsSettings class. "
+                                 f"{len(prop_types[False])} out of the {len(task.analytics_settings_definition)} items "
+                                 f"in the list do not conform to this requirement. ")
+
+            if len(analytics_settings_list) > 0:
+                task_settings['analytics_settings'] = analytics_settings_list
+
+        logger.info(f'Registering {str(type(task).__name__)} ({task.id}) to the Switch Driver Library:')
+
+        json_payload_verify = {
+            "driverId": str(task.id),
+            "name": type(task).__name__
+        }
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/drivers/verification"
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.post(url, json=json_payload_verify, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif response.text == "Failed":
+            logger.error("This Driver Guid is already associated with a Driver with a Different Name.")
+            return response_status, pandas.DataFrame()
+
+        script_code = replace_extensions_imports(task, 'extensions') if has_extensions_support(
+            task) else Automation._get_task_code(task)
+
+        json_payload = {
+            "driverId": str(task.id),
+            "name": type(task).__name__,
+            "specification": task.description,
+            "mappingEntities": task.mapping_entities if hasattr(task, 'mapping_entities') else [],
+            "scriptCode": script_code,
+            "baseClass": base_class,
+            "settings": task_settings
+            # {"integration_settings":[{"property_name":"Dummy1", "display_name":"", "default_value":"", "allowed_values":[], "editor":""}], "work_order_input":[], "analytics_settings":[], }
+        }
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/script-driver/registration"
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.post(url, json=json_payload, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text)
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def register_guide_task(api_inputs: ApiInputs, task, local_folder_path: str,
+                            external_type: GUIDES_EXTERNAL_TYPES = 'SwitchGuides',
+                            scope: GUIDES_SCOPE = 'Portfolio-wide'):
+        """Register Guide Task
+
+        Registers the task that was defined and registers the guide and associated files to the marketplace.
+
+        1. Registers the driver to Script Drivers Table
+        2. Upload Blob Files from local folder path
+        3. Register driver to Marketplace Items
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        task : Union[DiscoverableIntegrationTask, GuideTask]
+            An instance of the custom class created from the Abstract Base Class `Task` or its abstract sub-classes:
+            `DiscoverableIntegrationTask`, or `GuideTask`.
+        local_folder_path : str
+            Local file path to the `.vue` and `.js` form files to be uploaded to blob
+        external_type : GUIDES_EXTERNAL_TYPES, optional
+            Defaults to 'SwitchGuides'.
+        scope : GUIDES_SCOPE, optional
+            Marketplace item scope. Defaults to 'Portfolio-wide'.
+
+        Returns
+        -------
+        Union[bool, pandas.DataFrame, tuple]
+
+        """
+
+        if not api_inputs or not api_inputs.api_base_url or not api_inputs.bearer_token:
+            logger.error("You must call initialize() before using the API.")
+            return pandas.DataFrame()
+
+        if len(task.__class__.__bases__) == 1:
+            logger.error(f"The task must subclass Task or one of its subclasses as well as the Guide class")
+            return False
+        elif issubclass(type(task), Guide) != True:
+            logger.error(f"The task must subclass the Guide class in addition to one of the Task sub-classes. ")
+            return False
+
+        min_tags = task.minimum_tags_met()
+        if min_tags != True:
+            if min_tags == False:
+                logger.error(f"The 'guide_tags' dictionary must not be empty. ")
+            logger.error("The 'guide_tags' must be a dictionary with a minimum of one key. ")
+            return False
+
+        short_desc_length = task.check_short_desc_length()
+        if short_desc_length != True:
+            if short_desc_length == False:
+                logger.error("The 'guide_short_description' must be a string that does not exceed 250 characters. ")
+                return False
+            logger.error(f"The 'guide_short_description' must be a string that does not exceed 250 characters. Current "
+                         f"length: {short_desc_length}")
+            return False
+
+
+        logger.info("Registering task...")
+        response = Automation.register_task(api_inputs, task)
+
+        # make it so this prints back properly
+        pandas.set_option('display.max_columns', 10)
+
+        _, register = (0, response) if isinstance(response, pandas.DataFrame) else response
+
+
+        if response.shape[0] == 0:
+            logger.error("Task registration failed. The guide forms were not uploaded to blob and the guide was not "
+                         "registered to the marketplace. ")
+            return False
+        else:
+            logger.info(f"Task registered successfully: \n {register}")
+
+        logger.info("Uploading Guide form to Blob storage container...")
+        blob_response =  Blob._guide_blob_upload(api_inputs=api_inputs, local_folder_path=local_folder_path,
+                                                 driver_id=task.id)
+
+        if blob_response == False:
+            logger.error("Failed to upload blob files for guide registration and hence Marketplace "
+                         "Registration was not completed. You will need to reregister before the guide will be shown in "
+                         "the Marketplace.")
+            return register, blob_response
+        else:
+            logger.info(f"Guide form successfully uploaded to blob. ")
+
+        # Request Clone of Assets to DataCenters
+        headers = api_inputs.api_headers.default
+
+        payload = {
+           "folderPath": task.id + '/'
+        }
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/guide/form/assets-clone"
+
+        logger.info("Sending request: POST %s", url)
+
+        blob_sync_response = requests.post(url, json=payload, headers=headers)
+        response_status = '{} {}'.format(blob_sync_response.status_code, blob_sync_response.reason)
+
+        logger.info(f"Sync Guide Form assets for DriverId={task.id} is {response_status}")
+
+        # Register to Marketplace
+        marketplace_response = add_marketplace_item(
+            api_inputs=api_inputs, name=task.marketplace_name, short_description=task.guide_short_description,
+            description=task.guide_description, version=task.guide_version, external_type=external_type,
+            external_id=task.id, tags=task.guide_tags, scope=scope, image_file_name=task.image_file_name,
+            card_image_file_name=task.card_image_file_name)
+
+        __, marketplace_df = (0, response) if isinstance(response, pandas.DataFrame) else marketplace_response
+
+        if __ != 0 and marketplace_df.shape[0] > 0:
+            logger.error(f"Marketplace registration was not successful. Please retry. Error response: \n {marketplace_df}")
+            return register, blob_response, marketplace_df
+        elif __ != 0 and marketplace_df.shape[0] == 0:
+            logger.error(f"Marketplace registration was not successful. Please retry. ")
+            return register, blob_response, __
+        else:
+            logger.info(f"Marketplace registration successful. \n {marketplace_df}")
+
+        return register, blob_response, blob_sync_response, marketplace_df
+
+    @staticmethod
+    def list_tasks(api_inputs: ApiInputs, search_name_pattern: str = '*'):
+        """Get a list of the registered tasks.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        search_name_pattern : str, optional
+            A pattern that should be used as a filter when retrieving the list of deployed drivers
+            (Default value = '*').
+
+        Returns
+        -------
+        pandas.DataFrame
+            Dataframe containing the registered tasks.
+
+        """
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/drivers"
+
+        if search_name_pattern != '*' and search_name_pattern != '':
+            url += f"?name={search_name_pattern}"
+
+        logger.info("Sending request: GET %s", url)
+
+        response = requests.request("GET", url, timeout=20, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text)
+        df = df.drop(columns=['driverId'])
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def deploy_as_on_demand_data_feed(task: EventWorkOrderTask,
+                                      api_inputs: ApiInputs, data_feed_id: uuid.UUID, settings: dict,
+                                      queue_name: QUEUE_NAME = 'task', data_feed_name: str = None):
+        """Deploy the custom driver as an on demand data feed.
+
+        This deployment method is only suitable for the tasks that subclass the EventWorkOrderTask base class.
+
+        Parameters
+        ----------
+        task : EventWorkOrderTask
+            The custom driver class created from the Abstract Base Class `EventWorkOrderTask`
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        data_feed_id : uuid.UUID
+            The DataFeedId to update if existing, else will create a new record with the given value.
+        settings : dict
+            List of settings used to deploy the driver. May containing information needed to access 3rd party api
+             - e.g. URI, Username, Pwd, AccessToken, etc
+        queue_name : QUEUE_NAME, optional
+            The queue name (Default value = 'task').
+        data_feed_name : str, Optional
+            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
+            task name.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the details of the deployed https endpoint data feed.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        if not issubclass(task.__class__, EventWorkOrderTask):
+            logger.error("Only task derived from EventWorkOrderTask can be deployed as OnDemand.")
+            return pandas.DataFrame()
+
+        if not isinstance(settings, dict):
+            logger.error("setting parameter should be of type dict.")
+            return pandas.DataFrame()
+        elif not settings or not bool(settings) or len(settings) == 0:
+            logger.error("setting parameter for On Demand Data Feed cannot be empty.")
+            return pandas.DataFrame()
+
+        if data_feed_id is None and data_feed_name is None:
+            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
+            data_feed_name = task.__class__.__qualname__
+        elif data_feed_id is not None and data_feed_name is None:
+            data_feed_id = data_feed_id
+            data_feed_name = data_feed_name
+
+        headers = api_inputs.api_headers.default
+
+        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
+            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
+                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
+            return pandas.DataFrame()
+
+        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
+                    api_inputs.api_project_id)
+
+        payload = {
+            "dataFeedId": str(data_feed_id),
+            "driverId": str(task.id),
+            "name": data_feed_name,
+            "feedType": ",".join(task.mapping_entities),
+            "expectedDelivery": "NA",
+            "sourceType": "OnDemand",
+            "queueName": queue_name,
+            "settings": settings
+        }
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
+
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.post(url, json=payload, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(StringIO(response.text), typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def deploy_as_email_data_feed(task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
+                                              AnalyticsTask, LogicModuleTask, EventWorkOrderTask],
+                                  api_inputs: ApiInputs, data_feed_id: uuid.UUID,
+                                  expected_delivery: EXPECTED_DELIVERY, email_subject_regex: str,
+                                  email_address_domain: str, queue_name: QUEUE_NAME = 'task',
+                                  data_feed_name: str = None, task_priority: TASK_PRIORITY = 'default',
+                                  task_framework: TASK_FRAMEWORK = 'PythonScriptFramework'):
+        """Deploy task as an email data feed.
+
+        Deploys the created `task` as an email data feed. This allows the driver to ingest data sent via email. The
+        data must be sent to data@switchautomation.com to be processed. If it is sent to another email address, the task
+        will not be run.
+
+        Parameters
+        ----------
+        task : Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask, AnalyticsTask, LogicModuleTask,
+        EventWorkOrderTask]
+            The custom driver class created from the Abstract Base Class `Task`
+        api_inputs : ApiInputs
+            Object returned by initialize() function.
+        data_feed_id : uuid.UUID
+            The DataFeedId to update if existing, else will create a new record with the given value.
+        expected_delivery : EXPECTED_DELIVERY
+            The expected delivery frequency.
+        email_subject_regex : str
+            Regex expression used to parse the email subject line to determine which driver the received file will
+            be processed by.
+        email_address_domain : str
+            The email domain, without the @ symbol, of the sender. For example, if the email address that will send
+            file(s) for this data feed to the Switch Automation Platform is sender@test.com, the string that should be
+            passed to this parameter is "test.com".
+        queue_name : QUEUE_NAME, Optional
+            The name of queue (Default value = 'task').
+        data_feed_name : str, Optional
+            The name of the data feed (to be displayed in Task Insights UI). If not provided, the API will automatically
+            default to using the task.name property of the `task` passed to function.
+        task_priority: TASK_PRIORITY
+            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be alloted
+            to run the task - 'default`, 'standard', or 'advanced'. Defaults to 'default'.
+        task_framework: TASK_FRAMEWORK
+            Determines the framework of the datafeed tasks when processing.
+            'PythonScriptFramework' for the old task runner engine.
+            'TaskInsightsEngine' for the new task running in container apps.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the details of the deployed email data feed.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        if data_feed_id is None and data_feed_name is None:
+            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
+            data_feed_name = task.__class__.__qualname__
+        elif data_feed_id is not None and data_feed_name is None:
+            data_feed_id = data_feed_id
+            data_feed_name = data_feed_name
+
+        headers = api_inputs.api_headers.default
+
+        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
+                    api_inputs.api_project_id)
+
+        if not _is_valid_regex(email_subject_regex):
+            logger.error("%s is not valid regex.", email_subject_regex)
+            return pandas.DataFrame()
+
+        if email_address_domain == "switchautomation.com":
+            logger.info("Emails can only be received from the %s domain.", email_address_domain)
+
+        if "@" in email_address_domain:
+            logger.error("Do not include the @ in the email_address_domain parameter. ")
+            return pandas.DataFrame()
+
+        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
+            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
+                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
+            return pandas.DataFrame()
+
+        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
+            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
+                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
+            return pandas.DataFrame()
+
+        if not set([task_framework]).issubset(set(TASK_FRAMEWORK.__args__)):
+            logger.error('task_framework parameter must be set to one of the allowed values defined by the '
+                         'TASK_FRAMEWORK literal: %s', TASK_FRAMEWORK.__args__)
+            return pandas.DataFrame()
+
+        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__)):
+            logger.error('task_priority parameter must be set to one of the allowed values defined by the '
+                         'TASK_PRIORITY literal: %s', TASK_PRIORITY.__args__)
+            return pandas.DataFrame()
+
+        logger.info('Task Framework is "%s"', str(task_framework))
+        logger.info('Task Priotiy is "%s"', str(task_priority))
+
+        inbox_container = "data-exchange"
+        payload = {
+            "dataFeedId": str(data_feed_id),
+            "driverId": str(task.id),
+            "name": data_feed_name,
+            "feedType": ",".join(task.mapping_entities),
+            "expectedDelivery": expected_delivery,
+            "sourceType": "Email",
+            "queueName": queue_name,
+            "taskFramework": task_framework,
+            "taskPriority": task_priority,
+            "email": {
+                "emailAddressDomain": email_address_domain,
+                "emailSubjectRegex": email_subject_regex,
+                "container": inbox_container
+            }
+        }
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
+
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.post(url, json=payload, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text, typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def deploy_as_ftp_data_feed(task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
+                                            AnalyticsTask, LogicModuleTask, EventWorkOrderTask],
+                                api_inputs: ApiInputs, data_feed_id: uuid.UUID, expected_delivery: EXPECTED_DELIVERY,
+                                ftp_user_name: str, ftp_password: str, queue_name: QUEUE_NAME = 'task',
+                                data_feed_name: str = None, task_priority: TASK_PRIORITY = 'default',
+                                task_framework: TASK_FRAMEWORK = 'PythonScriptFramework'):
+        """Deploy the custom driver as an FTP data feed
+
+        Deploys the custom driver to receive data via an FTP data feed. Sets the `ftp_user_name` & `ftp_password` and
+        the `expected_delivery` of the file.
+
+        Parameters
+        ----------
+        task : Task
+            The custom driver class created from the Abstract Base Class 'Task'
+        api_inputs : ApiInputs
+            Object returned by the initialize() function.
+        data_feed_id : uuid.UUID
+            The DataFeedId to update if existing, else will create a new record with the given value.
+        expected_delivery : EXPECTED_DELIVERY
+            The expected delivery frequency of the data.
+        ftp_user_name : str
+            The user_name to be used by the ftp service to authenticate delivery of the data feed.
+        ftp_password : str
+            The password to be used by the ftp service for the given `ftp_user_name` to authenticate delivery of the
+            data feed.
+        queue_name : QUEUE_NAME, default = 'task'
+            The queue name (Default value = 'task').
+        data_feed_name : str, Optional
+            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
+            task name.
+        task_priority: TASK_PRIORITY
+            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be alloted
+            to run the task - 'default`, 'standard', or 'advanced'. Defaults to 'default'.
+        task_framework: TASK_FRAMEWORK
+            Determines the framework of the datafeed tasks when processing.
+            'PythonScriptFramework' for the old task runner engine.
+            'TaskInsightsEngine' for the new task running in container apps.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the details of the deployed ftp data feed.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        if data_feed_id is None and data_feed_name is None:
+            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
+            data_feed_name = task.__class__.__qualname__
+        elif data_feed_id is not None and data_feed_name is None:
+            data_feed_id = data_feed_id
+            data_feed_name = data_feed_name
+
+        headers = api_inputs.api_headers.default
+
+        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
+            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
+                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
+            return pandas.DataFrame()
+
+        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
+            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
+                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
+            return pandas.DataFrame()
+
+        if not set([task_framework]).issubset(set(TASK_FRAMEWORK.__args__)):
+            logger.error('task_framework parameter must be set to one of the allowed values defined by the '
+                         'TASK_FRAMEWORK literal: %s', TASK_FRAMEWORK.__args__)
+            return pandas.DataFrame()
+
+        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__)):
+            logger.error('task_priority parameter must be set to one of the allowed values defined by the '
+                         'TASK_PRIORITY literal: %s', TASK_PRIORITY.__args__)
+            return pandas.DataFrame()
+
+        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
+                    api_inputs.api_project_id)
+        logger.info('Task Framework is "%s"', str(task_framework))
+        logger.info('Task Priotiy is "%s"', str(task_priority))
+
+        inbox_container = "data-exchange"
+        payload = {
+            "dataFeedId": str(data_feed_id),
+            "driverId": str(task.id),
+            "name": data_feed_name,
+            "feedType": ",".join(task.mapping_entities),
+            "expectedDelivery": expected_delivery,
+            "sourceType": "Ftp",
+            "queueName": queue_name,
+            "taskFramework": task_framework,
+            "taskPriority": task_priority,
+            "ftp": {
+                "ftpUserName": ftp_user_name,
+                "ftpPassword": ftp_password,
+                "container": inbox_container
+            }
+        }
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
+
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.post(url, json=payload, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text, typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def deploy_as_upload_data_feed(task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
+                                               AnalyticsTask, LogicModuleTask, EventWorkOrderTask],
+                                   api_inputs: ApiInputs, data_feed_id: uuid.UUID, expected_delivery: EXPECTED_DELIVERY,
+                                   queue_name: QUEUE_NAME = 'task', data_feed_name: str = None,
+                                   task_priority: TASK_PRIORITY = 'default',
+                                   task_framework: TASK_FRAMEWORK = 'PythonScriptFramework'):
+        """Deploy the custom driver as a REST API end point Datafeed.
+
+        To upload a file to the deployed data feed, use the UploadUrl from the response dataframe (with request type
+        POST) with the following two headers:
+
+        - 'Ocp-Apim-Subscription-Key' - set to the value of ``api_inputs.subscription_key``
+        - 'Authorization' - set to the value 'Bearer ``api_inputs.bearer_token``'
+
+        For example, to upload a file using the ``requests`` package:
+
+        >>> import requests
+        >>> url = df.loc[0,'UploadUrl']
+        >>> payload={}
+        >>> file_path = 'C:/xxyyzz.txt'
+        >>> files={'file': open(file_path, 'rb')}
+        >>> headers = {'Ocp-Apim-Subscription-Key': api_inputs.subscription_key, 'Authorization': f'Bearer {api_inputs.bearer_token}'}
+        >>> response = requests.request("POST", url, headers=headers, data=payload, files=files)
+        >>> print(response.text)
+
+        Parameters
+        ----------
+        task : Task
+            The custom driver class created from the Abstract Base Class 'Task'
+        api_inputs : ApiInputs
+            Object returned by the initialize() function.
+        data_feed_id : uuid.UUID
+            The DataFeedId to update if existing, else will create a new record with the given value.
+        expected_delivery : EXPECTED_DELIVERY
+            The expected delivery frequency of the data.
+        queue_name : QUEUE_NAME, optional
+            The queue name (Default value = 'task').
+        data_feed_name : str, Optional
+            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
+            task name.
+        task_priority: TASK_PRIORITY
+            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be alloted
+            to run the task - 'default`, 'standard', or 'advanced'. Defaults to 'default'.
+        task_framework: TASK_FRAMEWORK
+            Determines the framework of the datafeed tasks when processing.
+            'PythonScriptFramework' for the old task runner engine.
+            'TaskInsightsEngine' for the new task running in container apps.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the details of the deployed https endpoint data feed.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        if data_feed_id is None and data_feed_name is None:
+            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
+            data_feed_name = task.__class__.__qualname__
+        elif data_feed_id is not None and data_feed_name is None:
+            data_feed_id = data_feed_id
+            data_feed_name = data_feed_name
+
+        headers = api_inputs.api_headers.default
+
+        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
+            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
+                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
+            return pandas.DataFrame()
+
+        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
+            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
+                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
+            return pandas.DataFrame()
+
+        if not set([task_framework]).issubset(set(TASK_FRAMEWORK.__args__)):
+            logger.error('task_framework parameter must be set to one of the allowed values defined by the '
+                         'TASK_FRAMEWORK literal: %s', TASK_FRAMEWORK.__args__)
+            return pandas.DataFrame()
+
+        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__)):
+            logger.error('task_priority parameter must be set to one of the allowed values defined by the '
+                         'TASK_PRIORITY literal: %s', TASK_PRIORITY.__args__)
+            return pandas.DataFrame()
+
+        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
+                    api_inputs.api_project_id)
+        logger.info('Task Framework is "%s"', str(task_framework))
+        logger.info('Task Priotiy is "%s"', str(task_priority))
+
+        payload = {
+            "dataFeedId": str(data_feed_id),
+            "driverId": str(task.id),
+            "name": data_feed_name,
+            "feedType": ",".join(task.mapping_entities),
+            "expectedDelivery": expected_delivery,
+            "sourceType": "Upload",
+            "queueName": queue_name,
+            "taskFramework": task_framework,
+            "taskPriority": task_priority,
+            "upload": {
+                "placeholder": ""
+            },
+        }
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
+
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.post(url, json=payload, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(StringIO(response.text), typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def deploy_as_blob_data_feed(task: BlobTask,
+                                 api_inputs: ApiInputs, data_feed_id: uuid.UUID, expected_delivery: EXPECTED_DELIVERY,
+                                 data_feed_name: str = None,
+                                 task_priority: TASK_PRIORITY = 'standard'):
+        """Deploy the BlobTask as a blob Datafeed.
+
+        Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can
+        be registered or deployed
+
+        Parameters
+        ----------
+        task : BlobTask
+            The custom driver class created from the Abstract Base Class 'BlobTask'
+        api_inputs : ApiInputs
+            Object returned by the initialize() function.
+        data_feed_id : uuid.UUID
+            The DataFeedId to update if existing, else will create a new record with the given value.
+        expected_delivery : EXPECTED_DELIVERY
+            The expected delivery frequency of the data.
+        data_feed_name : str, Optional
+            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
+            task name.
+        task_priority: TASK_PRIORITY
+            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be
+            alloted to run the task - 'standard' or 'advanced'. Defaults to 'standard'.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the details of the deployed blob data feed.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        if not issubclass(task.__class__, BlobTask):
+            logger.error("Only task derived from BlobTask can be deployed as Blob Data Feed.")
+            return pandas.DataFrame()
+
+        if data_feed_id is None and data_feed_name is None:
+            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
+            data_feed_name = task.__class__.__qualname__
+        elif data_feed_id is not None and data_feed_name is None:
+            data_feed_id = data_feed_id
+            data_feed_name = data_feed_name
+
+        headers = api_inputs.api_headers.default
+
+        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
+            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
+                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
+            return pandas.DataFrame()
+
+        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__).difference(set(["default"]))):
+            logger.error(f'task_priority parameter must be set to one of the allowed values defined by the '
+                         f'TASK_PRIORITY literal: {set(TASK_PRIORITY.__args__).difference(set(["default"]))}')
+            return pandas.DataFrame()
+
+        logger.info('Deploy %s (%s) as a data feed for ApiProjectID: %s', type(task).__name__, str(task.id),
+                    api_inputs.api_project_id)
+        logger.info('Task Framework is "%s"', 'TaskInsightsEngine')
+        logger.info('Task Priotiy is "%s"', str(task_priority))
+
+        payload = {
+            "dataFeedId": str(data_feed_id),
+            "driverId": str(task.id),
+            "name": data_feed_name,
+            "feedType": ",".join(task.mapping_entities),
+            "expectedDelivery": expected_delivery,
+            "sourceType": "Blob",
+            "queueName": "task",
+            "taskFramework": "TaskInsightsEngine",
+            "taskPriority": task_priority,
+        }
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
+
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.post(url, json=payload, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(StringIO(response.text), typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def deploy_on_timer(task: Union[Task, IntegrationTask, DiscoverableIntegrationTask, QueueTask,
+                                    AnalyticsTask, LogicModuleTask, EventWorkOrderTask],
+                        api_inputs: ApiInputs, data_feed_id: uuid.UUID, expected_delivery: EXPECTED_DELIVERY,
+                        cron_schedule: str, queue_name: QUEUE_NAME = "task", settings: dict = None,
+                        schedule_timezone: SCHEDULE_TIMEZONE = 'Local', timezone_offset_minutes: int = None,
+                        data_feed_name: str = None, task_priority: TASK_PRIORITY = 'default',
+                        task_framework: TASK_FRAMEWORK = 'PythonScriptFramework'):
+        """Deploy driver to run on timer.
+
+        Parameters
+        ----------
+        task : Task
+            The custom driver class created from the Abstract Base Class `Task`.
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        data_feed_id : uuid.UUID
+            The DataFeedId to update if existing, else will create a new record with the given value.
+        expected_delivery : EXPECTED_DELIVERY
+            The expected delivery frequency.
+        cron_schedule : str
+            The CRONOS cron object containing the required schedule for the driver to be run. For details on the
+            required format, see: https://crontab.cronhub.io/
+        queue_name : QUEUE_NAME, optional
+            The queue name (Default value = 'task').
+        settings : dict, Optional
+            List of settings used to deploy the driver. For example, may contain the user_name and password required to
+            authenticate calls to a third-party API (Default value = None).
+        schedule_timezone : SCHEDULE_TIMEZONE, optional
+            Whether the ``cron_schedule`` should be applied based on Local or Utc timezone. If set to `Local`, this is
+            taken as the timezone of the western-most site in the given portfolio (Default value = 'Local').
+        timezone_offset_minutes: int, Optional
+            Timezone offset in minutes (from UTC) to be used when applying the ``cron_schedule`` (Default value = None).
+        data_feed_name : str, Optional
+            The name of the data feed (to be displayed in Task Insights UI). If not provided, will default to the
+            task name.
+        task_priority: TASK_PRIORITY
+            Determines the priority of the datafeed tasks when processing. This equates to how much resources would be alloted
+            to run the task - 'default`, 'standard', or 'advanced'. Defaults to 'default'.
+        task_framework: TASK_FRAMEWORK
+            Determines the framework of the datafeed tasks when processing.
+            'PythonScriptFramework' for the old task runner engine.
+            'TaskInsightsEngine' for the new task running in container apps.
+
+        Returns
+        -------
+        pandas.Dataframe
+            A dataframe containing the details of the deployed data feed.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        if data_feed_id is None and data_feed_name is None:
+            data_feed_id = uuid.UUID('00000000-0000-0000-0000-000000000000')
+            data_feed_name = task.__class__.__qualname__
+        elif data_feed_id is not None and data_feed_name is None:
+            data_feed_id = data_feed_id
+            data_feed_name = data_feed_name
+
+        if timezone_offset_minutes is None:
+            timezone_offset_minutes = 0
+
+        headers = api_inputs.api_headers.default
+
+        if not set([expected_delivery]).issubset(set(EXPECTED_DELIVERY.__args__)):
+            logger.error('expected_delivery parameter must be set to one of the allowed values defined by the '
+                         'EXPECTED_DELIVERY literal: %s', EXPECTED_DELIVERY.__args__)
+            return pandas.DataFrame()
+
+        if not set([schedule_timezone]).issubset(set(SCHEDULE_TIMEZONE.__args__)):
+            logger.error('schedule_timezone parameter must be set to one of the allowed values defined by the '
+                         'SCHEDULE_TIMEZONE literal: %s', SCHEDULE_TIMEZONE.__args__)
+            return pandas.DataFrame()
+
+        if not set([queue_name]).issubset(set(QUEUE_NAME.__args__)):
+            logger.error('queue_name parameter must be set to one of the allowed values defined by the '
+                         'QUEUE_NAME literal: %s', QUEUE_NAME.__args__)
+            return pandas.DataFrame()
+
+        if 5 > len(cron_schedule.split(' ')) > 6:
+            logger.error("cron_schedule parameter must be in the format * * * * *")
+            return pandas.DataFrame()
+
+        # if task.__class__.__qualname__ == 'DiscoverableIntegrationTask':
+        #     config_props = task.integration_device_type_definition.config_properties
+        #     property_name_dict = dict
+        #     for i in range(len(config_props)):
+        #         property_name_dict[config_props[i].property_name] = config_props[i].default_value
+
+        # if task.__class__.__qualname__ == 'EventWorkOrderTask':
+        #     config_props = task.integration_settings_definition
+        #     property_name_list = []
+        #     property_defaults_dict = {}
+        #     for i in len(config_props):
+        #         property_name_list.append(config_props[i].property_name)
+        #         property_defaults_dict[config_props[i].property_name] = config_props[i].default_value
+        #
+        #     if settings is not None and not set(settings.keys()).issubset(set(property_name_list)):
+        #         missing_keys = list(set(property_name_list).difference(set(settings.keys())))
+        #         missing_required_keys = []
+        #         missing_optional_keys = []
+        #         for j in range(len(missing_keys)):
+        #             if property_defaults_dict[missing_keys[j]] is None or property_defaults_dict[missing_keys[j]] == '':
+        #                 missing_required_keys.append(missing_keys[j])
+        #                 missing_optional_keys
+        #             else:
+        #                 missing_optional_keys.append(missing_keys[j])
+        #                 missing_required_keys
+        #         if len(missing_required_keys) > 0 and len(missing_optional_keys) == 0:
+        #             loggger.error(f"settings parameter is missing the following required key(s): {missing_required_keys}.")
+
+        if not set([task_framework]).issubset(set(TASK_FRAMEWORK.__args__)):
+            logger.error('task_framework parameter must be set to one of the allowed values defined by the '
+                         'TASK_FRAMEWORK literal: %s', TASK_FRAMEWORK.__args__)
+            return pandas.DataFrame()
+
+        if not set([task_priority]).issubset(set(TASK_PRIORITY.__args__)):
+            logger.error('task_priority parameter must be set to one of the allowed values defined by the '
+                         'TASK_PRIORITY literal: %s', TASK_PRIORITY.__args__)
+            return pandas.DataFrame()
+
+        logger.info('Deploy %s (%s) on timer for ApiProjectID: %s and schedule: %s.', type(task).__name__,
+                    str(task.id), api_inputs.api_project_id, cron_schedule)
+        logger.info('Feed Type is %s', str(task.mapping_entities))
+        logger.info('Task Framework is "%s"', str(task_framework))
+        logger.info('Task Priotiy is "%s"', str(task_priority))
+        logger.info('Settings to be passed to the driver on start are: %s', str(settings))
+
+        payload = {
+            "dataFeedId": str(data_feed_id),
+            "driverId": str(task.id),
+            "name": data_feed_name,
+            "feedType": ",".join(task.mapping_entities),
+            "expectedDelivery": expected_delivery,
+            "sourceType": "Timer",
+            "queueName": queue_name,
+            "taskFramework": task_framework,
+            "taskPriority": task_priority,
+            "timer": {
+                "cronSchedule": cron_schedule,
+                "timezoneOffsetMinutes": timezone_offset_minutes,
+                "scheduleTimezone": schedule_timezone
+            },
+            "settings": settings
+        }
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/tasks/deployment"
+
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.post(url, json=payload, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200 and len(response.text) > 0:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            error_df = pandas.read_json(response.text)
+            return response_status, error_df
+        elif response.status_code != 200 and len(response.text) == 0:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text, typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def cancel_deployed_data_feed(api_inputs: ApiInputs, data_feed_id: uuid.UUID, deployment_type: List[DEPLOY_TYPE]):
+        """Cancel deployment for a given `data_feed_id` and `deployment_type`
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        data_feed_id: uuid.UUID
+            Datafeed Id to cancel deployment
+        deployment_type: List[DEPLOY_TYPE]
+
+
+        Returns
+        -------
+        str
+            A string containing the response text.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        if data_feed_id == None or data_feed_id == '':
+            logger.error('Data feed Id cannot be Empty or None')
+            return
+
+        if type(deployment_type) != list:
+            logger.error('deployment_type should be of type  list.')
+            return
+        elif deployment_type == None or len(deployment_type) == 0:
+            logger.error('deployment_type cannot be empty. Please specify deployment_type to cancel deployment.')
+            return
+        elif not set(deployment_type).issubset(set(DEPLOY_TYPE.__args__)):
+            logger.error('deployment_type item parameter must be set to one of the allowed values defined by the '
+                         'DEPLOY_TYPE literal: %s', DEPLOY_TYPE.__args__)
+            return
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/deployment/" \
+              f"{data_feed_id}/cancel"
+
+        logger.info("Cancel deployment type/s with DataFeedID: %s for ApiProjectID: %s", data_feed_id,
+                    api_inputs.api_project_id)
+        logger.info("Source Types: %s", ",".join(deployment_type))
+        logger.info("Sending request: POST %s", url)
+
+        payload = {
+            'sourceTypes': deployment_type
+        }
+
+        response = requests.post(url, headers=headers, json=deployment_type)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        # Check if there's any timer deploy_type being cancelled, delete it as well by calling delete method
+        if any("Timer" in deploy_type for deploy_type in deployment_type):
+            return Automation.delete_deployed_data_feed(api_inputs=api_inputs, data_feed_id=data_feed_id)
+
+        df = pandas.read_json(response.text, typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def delete_deployed_data_feed(api_inputs: ApiInputs, data_feed_id: uuid.UUID):
+        """Delete deployment for a given `data_feed_id`.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        data_feed_id: uuid.UUID
+            Datafeed Id to cancel deployment
+
+        Returns
+        -------
+        str
+            A string containing the response text.
+
+        """
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        if data_feed_id == None or data_feed_id == '':
+            logger.error('Data feed Id cannot be None')
+            return
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/deployment/" \
+              f"{data_feed_id}/delete"
+
+        logger.info("Delete deployment with DataFeedID: %s for ApiProjectID: %s", data_feed_id,
+                    api_inputs.api_project_id)
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.delete(url, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text, typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def list_deployments(api_inputs: ApiInputs, search_name_pattern='*'):
+        """Retrieve list of deployed drivers.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        search_name_pattern : str
+                A pattern that should be used as a filter when retrieving the list of deployed drivers
+                (Default value = '*').
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the drivers deployed for the given ApiProjectID that match the `search_name_pattern`.
+
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/search"
+
+        if search_name_pattern != '' and search_name_pattern != '*':
+            url += f"?name={search_name_pattern}"
+
+        logger.info("Sending request: GET %s", url)
+
+        response = requests.request("GET", url, timeout=20, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        # We need to wrap json results in StringIO when stringified JSON is present in the response.
+        # Symptoms of not doing this are: ValueError: Unexpected character found when decoding object value
+        # and incorrect suggestion in the error on installing a package called 'fsspec'
+        # Installing 'fsspec' package will not solve the issue.
+        df = pandas.read_json(StringIO(response.text))
+        df.columns = _column_name_cap(df.columns)
+        return df
+
+    @staticmethod
+    def list_data_feed_history(api_inputs: ApiInputs, data_feed_id: uuid.UUID, top_count: int = 10):
+        """Retrieve data feed history
+
+        Retrieves the `top_count` records for the given `data_feed_id`.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        data_feed_id : uuid.UUID
+            The unique identifier for the data feed that history should be retrieved for.
+        top_count : int, default = 10
+            The top record count to be retrieved. (Default value = 10).
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the `top_count` history records for the given `data_feed_id`.
+
+        """
+
+        if data_feed_id is None or data_feed_id == '':
+            logger.error('The data_feed_id parameter must be provided. ')
+            return
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
+              f"{data_feed_id}/history?topCount={top_count}"
+
+        logger.info("Sending request: GET %s", url)
+
+        response = requests.request("GET", url, timeout=20, headers=headers)
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text)
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def run_data_feed(api_inputs: ApiInputs, data_feed_id: uuid.UUID):
+        """Trigger an on-demand run of the python job based on data feed id. This will be sent to the queue for
+        processing and will undergo same procedure as the rest of the datafeed.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        data_feed_id : uuid.UUID
+            The unique identifier for the data feed that history should be retrieved for.
+        """
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        if data_feed_id is None or data_feed_id == '':
+            logger.error('The data_feed_id parameter must be provided. ')
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/datafeed/{data_feed_id}/execute"
+
+        logger.info("Sending request: POST %s", url)
+
+        response = requests.request("POST", url, timeout=20, headers=headers)
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        # We need to wrap json results in StringIO when stringified JSON is present in the response.
+        # Symptoms of not doing this are: ValueError: Unexpected character found when decoding object value
+        # and incorrect suggestion in the error on installing a package called 'fsspec'
+        # Installing 'fsspec' package will not solve the issue.
+        df = pandas.read_json(StringIO(response.text),
+                              typ='Series').to_frame().T
+        df.columns = _column_name_cap(df.columns)
+        return df
+
+    @staticmethod
+    def data_feed_history_process_output(api_inputs: ApiInputs, data_feed_id: uuid.UUID = None,
+                                         data_feed_file_status_id: uuid.UUID = None, row_number: int = None):
+        """Retrieve data feed history process output
+
+        Retrieves the `top_count` records for the given `data_feed_id`.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        data_feed_id : uuid.UUID, default = None
+            The unique identifier for the data feed that output should be retrieved for. This parameter works with the
+            `row_number`
+        data_feed_file_status_id : uuid.UUID, default = None
+            The unique identifier for the data feed history should be retrieved for. This UUID can be retrieved from
+            the list_data_feed_history() method.
+        row_number: int, default = 0
+            The row number from the list_data_feed_history method. It can be used in place of the file_status_id. Use
+            row_number=0 to retrieve the most recent process.
+
+        Returns
+        -------
+        log : str
+            Containing the full print and output log for the process
+
+        """
+
+        if data_feed_id is None and data_feed_file_status_id is None:
+            logger.error('Must supply either the data_feed_id + row_number, or the file_status_id')
+            return
+        elif data_feed_id is not None and data_feed_file_status_id is None and row_number is None:
+            logger.error('When supplying the data_feed_id, you must also provide the row_number to retrieve. ')
+            return
+        elif data_feed_id is None and data_feed_file_status_id is not None and row_number is not None:
+            logger.error('Please supply either the data_feed_id + row_number, or the file_status_id. ')
+            return
+        elif data_feed_id is not None and data_feed_file_status_id is not None and row_number is not None:
+            logger.error('Please supply either the data_feed_id + row_number, or the file_status_id. ')
+            return
+        elif data_feed_id is not None and data_feed_file_status_id is not None and row_number is None:
+            logger.error('Please supply either the data_feed_id + row_number or the file_status_id. ')
+            return
+
+        if row_number is None:
+            row_number = -1
+
+        if data_feed_id is None:
+            data_feed_id = '00000000-0000-0000-0000-000000000000'
+
+        if data_feed_file_status_id is None:
+            data_feed_file_status_id = '00000000-0000-0000-0000-000000000000'
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
+              f"{data_feed_id}/file-status/{data_feed_file_status_id}/process-output?rowCount={row_number}"
+        logger.info("Sending request: GET %s", url)
+
+        response = requests.request("GET", url, timeout=20, headers=headers)
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        data_feed_file_process_output = json.loads(response.text, object_hook=lambda d: DataFeedFileProcessOutput(
+            data_feed_id=d['dataFeedId'], data_feed_file_status_id=d['fileStatusId'],
+            client_tracking_id=d['clientTrackingId'],
+            source_type=d['sourceType'], file_name=d['fileName'], file_received=d['fileReceived'],
+            file_process_status=d['fileProcessStatus'], file_process_status_change=d['fileProcessStatusChange'],
+            process_started=d['processStarted'], process_completed=d['processCompleted'],
+            minutes_since_received=d['minutesSinceReceived'], minutes_since_processed=d['minutesSinceProcessed'],
+            file_size=d['fileSize'], log_file_path=d['logFile'], output=d['output'], error=d['error']))
+
+        return data_feed_file_process_output
+
+    @staticmethod
+    def data_feed_history_process_errors(api_inputs: ApiInputs, data_feed_id: uuid.UUID,
+                                         data_feed_file_status_id: uuid.UUID):
+        """Retrieve the unique error types for a given data feed file.
+
+        Retrieves the distinct error types present for the given `data_feed_id` and `data_feed_file_status_id`.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        data_feed_id : uuid.UUID
+            The unique identifier for the data feed that errors should be retrieved for.
+        data_feed_file_status_id : uuid.UUID
+            The unique identifier for the file processed for a given data feed that errors should be retrieved for.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the distinct error types present for the given data_feed_file_status_id.
+
+        """
+
+        if data_feed_id == '' or data_feed_file_status_id == '':
+            logger.error('The data_feed_id and data_feed_file_status_id parameters must be provided. ')
+            return
+
+        if api_inputs.api_base_url == '' or api_inputs.bearer_token == '':
+            logger.error("You must call initialize() before using API.")
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
+              f"{data_feed_id}/file-status/{data_feed_file_status_id}/process-errors"
+        logger.info("Sending request: GET %s", url)
+
+        response = requests.request("GET", url, timeout=20, headers=headers)
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        df = pandas.read_json(response.text)
+        df.columns = _column_name_cap(df.columns)
+
+        return df
+
+    @staticmethod
+    def data_feed_history_errors_by_type(api_inputs: ApiInputs, data_feed_id: uuid.UUID,
+                                         data_feed_file_status_id: uuid.UUID, error_type: ERROR_TYPE):
+        """Retrieve the encountered errors for a given error type, data feed & file.
+
+        Retrieves the errors identified for a given `error_type` for the `data_feed_id` and `data_feed_file_status_id`.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            Object returned by initialize.initialize() function
+        data_feed_id : uuid.UUID
+            The unique identifier for the data feed that errors should be retrieved for.
+        data_feed_file_status_id
+            The unique identifier for the data feed that errors should be retrieved for.
+        error_type: ERROR_TYPE
+            The error type of the structured logs to be retrieved.
+
+        Returns
+        -------
+        df : pandas.DataFrame
+            Dataframe containing the errors identified for the given `data_feed_file_status_id`.
+
+        """
+
+        if (data_feed_id == '' or data_feed_file_status_id == '' or data_feed_id is None or
+                data_feed_file_status_id is None):
+            logger.error('The data_feed_id and data_feed_file_status_id parameters must be provided. ')
+            return
+
+        if not set([error_type]).issubset(set(ERROR_TYPE.__args__)):
+            logger.error('error_type parameter must be set to one of the allowed values defined by the '
+                         'ERROR_TYPE literal: %s', ERROR_TYPE.__args__)
+            return pandas.DataFrame()
+
+        headers = api_inputs.api_headers.default
+
+        url = f"{api_inputs.api_projects_endpoint}/{api_inputs.api_project_id}/data-ingestion/data-feed/" \
+              f"{data_feed_id}/file-status/{data_feed_file_status_id}/process-errors/type/{error_type}"
+
+        logger.info("Sending request: GET %s", url)
+
+        response = requests.request("GET", url, timeout=20, headers=headers)
+
+        response_status = '{} {}'.format(response.status_code, response.reason)
+        if response.status_code != 200:
+            logger.error("API Call was not successful. Response Status: %s. Reason: %s.", response.status_code,
+                         response.reason)
+            return response_status, pandas.DataFrame()
+        elif len(response.text) == 0:
+            logger.error('No data returned for this API call. %s', response.request.url)
+            return response_status, pandas.DataFrame()
+
+        # , error_bad_lines=False)
+        df = pandas.read_csv(StringIO(response.text), sep=",")
+        return df
+
+    @staticmethod
+    def _get_task_code(task):
+        """
+
+        Parameters
+        ----------
+        task : Task
+            The custom driver class created from the Abstract Base Class `Task`.
+
+        Returns
+        -------
+        driver_code : str
+            The code used to create the `task`
+
+        """
+        task_type = type(task)
+        task_code = ''
+        parent_module = inspect.getmodule(task_type)
+        for codeLine in inspect.getsource(parent_module).split('\n'):
+            if codeLine.startswith('import ') or codeLine.startswith('from '):
+                task_code += codeLine + '\n'
+
+        task_code += '\n'
+        task_code += inspect.getsource(task_type)
+        task_code += ''
+        task_code += 'task = ' + task_type.__name__ + '()'
+
+        return task_code
```

### Comparing `switch_api-0.5.4b2/switch_api/pipeline/definitions.py` & `switch_api-0.5.4b3/switch_api/pipeline/definitions.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,771 +1,771 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""Module defining the various definitions used internally by the pipeline module classes.
-"""
-# import uuid
-from typing import List
-from .._utils._utils import convert_to_pascal_case
-from .._utils._constants import INTEGRATION_SETTINGS_EDITORS
-# from ..integration import get_templates, get_units_of_measure
-# from .. import initialize
-
-
-class BaseProperty:
-    """BaseProperty class definition
-
-    Parameters
-    ----------
-    property_name : str
-        Must be in pascal case
-    display_label : str
-        Pretty version of the property_name
-    editor : INTEGRATION_SETTINGS_EDITORS
-        The editor used in the UI
-    default_value : , default=None
-        The default value, if applicable, for the given property
-    allowed_values : list, default=None
-        the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be None.
-
-    """
-    def __init__(self, property_name: str, display_label: str, editor: INTEGRATION_SETTINGS_EDITORS,
-                 default_value: object = None, allowed_values: List[object] = None):
-        """
-        Parameters
-        ----------
-        property_name : str
-            Must be in pascal case
-        display_label : str
-            Pretty version of the property_name
-        editor : INTEGRATION_SETTINGS_EDITORS
-            The editor used in the UI
-        default_value : , default=None
-            The default value, if applicable, for the given property
-        allowed_values : list, default=None
-            the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
-            None.
-
-        """
-        self.property_name = property_name
-        self.display_label = display_label
-        self.editor = editor
-        self.default_value = default_value
-        self.allowed_values = allowed_values
-
-    @property
-    def property_name(self) -> str:
-        """property_name : str
-            Must be in pascal case"""
-        return self._property_name
-
-    @property_name.setter
-    def property_name(self, value) -> str:
-        """property_name : str
-                    Must be in pascal case"""
-        if type(value) != str:
-            raise TypeError(f"The property_name parameter only accepts strings. ")
-        elif type(value) == str and not 0 < len(value) < 50:
-            raise ValueError(
-                f"property_name length must be between 1 and 50. Supplied '{value}' "
-                f"contains {len(value)} characters. ")
-        elif type(value) == str and 0 < len(value) < 50 and value != convert_to_pascal_case(value):
-            raise ValueError(
-                f"property_name must be in pascal case - i.e. start with a capital letter and contain no spaces, "
-                f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
-            )
-        self._property_name = value
-
-    @property
-    def display_label(self) -> str:
-        """The display label for the given property"""
-        return self._display_label
-
-    @display_label.setter
-    def display_label(self, value) -> str:
-        """The display label for the given property"""
-        if type(value) != str:
-            raise TypeError(f"The display_label parameter only accepts strings. ")
-        elif type(value) == str and not 0 < len(value) <= 255:
-            raise ValueError(
-                f"display_label length must be between 1 and 50. Supplied '{value}' "
-                f"contains {len(value)} characters. ")
-        self._display_label = value
-
-    @property
-    def editor(self) -> INTEGRATION_SETTINGS_EDITORS:
-        """The editor to be used in the UI"""
-        return self._editor
-
-    @editor.setter
-    def editor(self, value) -> INTEGRATION_SETTINGS_EDITORS:
-        """The editor to be used in the UI"""
-        if not set([value]).issubset(set(INTEGRATION_SETTINGS_EDITORS.__args__)):
-            raise ValueError(
-                f"Supplied value '{value}' is invalid. The editor parameter must be set to one of the allowed values "
-                f"defined by the INTEGRATION_SETTINGS_EDITORS literal: {INTEGRATION_SETTINGS_EDITORS.__args__}")
-        self._editor = value
-
-
-class EventWorkOrderFieldDefinition(BaseProperty):
-    """EventWorkOrderFieldDefinition class definition
-
-    Used to define the set of fields available in Events UI for creation of work orders in 3rd party systems.
-
-    Parameters
-    ----------
-    property_name : str
-        Must be in pascal case
-    display_label : str
-        Pretty version of the property_name (what is displayed in Events UI).
-    editor : INTEGRATION_SETTINGS_EDITORS
-        The editor used in the UI
-    default_value : , default=None
-        The default value, if applicable, for the given property
-    allowed_values : list, default=None
-        the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be None.
-    """
-    def __init__(self, property_name, display_label, editor, default_value, allowed_values):
-        """
-
-        Parameters
-        ----------
-        property_name : str
-            Must be in pascal case
-        display_label : str
-            Pretty version of the property_name
-        editor : INTEGRATION_SETTINGS_EDITORS
-            The editor used in the UI
-        default_value : , default=None
-            The default value, if applicable, for the given property
-        allowed_values : list, default=None
-            The set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
-            None.
-        """
-        super(EventWorkOrderFieldDefinition, self).__init__(
-            property_name=property_name, display_label=display_label, editor=editor, default_value=default_value,
-            allowed_values=allowed_values)
-
-
-class AnalyticsSettings(BaseProperty):
-    """
-
-    """
-    def __init__(self, property_name, display_label, editor, default_value, allowed_values):
-        """
-        Parameters
-        ----------
-        property_name : str
-            Must be in pascal case
-        display_label : str
-            Pretty version of the property_name
-        editor : INTEGRATION_SETTINGS_EDITORS
-            The editor used in the UI
-        default_value : , default=None
-            The default value, if applicable, for the given property
-        allowed_values : list, default=None
-            the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
-            None.
-        """
-        super(AnalyticsSettings, self).__init__(
-            property_name=property_name, display_label=display_label, editor=editor, default_value=default_value,
-            allowed_values=allowed_values)
-
-
-class IntegrationSettings(BaseProperty):
-    """Class used for defining the UI editors, default values, etc for the key-values passed to integration_settings
-    dictionaries.
-
-    Parameters
-    ----------
-    property_name : str
-        Must be in pascal case
-    display_label : str
-        Pretty version of the property_name
-    editor : INTEGRATION_SETTINGS_EDITORS
-        The editor used in the UI
-    default_value : , default=None
-        The default value, if applicable, for the given property
-    allowed_values : list, default=None
-        the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
-        None.
-    """
-    def __init__(self, property_name, display_label, editor, default_value, allowed_values):
-        """
-
-        Parameters
-        ----------
-        property_name : str
-            Must be in pascal case
-        display_label : str
-            Pretty version of the property_name
-        editor : INTEGRATION_SETTINGS_EDITORS
-            The editor used in the UI
-        default_value : , default=None
-            The default value, if applicable, for the given property
-        allowed_values : list, default=None
-            the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
-            None.
-        """
-        super(IntegrationSettings, self).__init__(
-            property_name=property_name, display_label=display_label, editor=editor, default_value=default_value,
-            allowed_values=allowed_values)
-
-
-# class IntegrationDevicePropertyDefinition2:
-#     def __init__(self, property_name: str, display_label: str, default_value: object = None,
-#                  editor: INTEGRATION_SETTINGS_EDITORS = 'text_box', values: List[object] = None,
-#                  required_for_task: bool = False, use_for_discovery: bool = False, editable_in_discovery: bool = False):
-#         self.property_name = property_name
-#         self.display_label = display_label
-#         self.default_value = default_value
-#         self.editor = editor
-#         self.values = values
-#         self.required_for_task = required_for_task
-#         self.use_for_discovery = use_for_discovery
-#         self.editable_in_discovery = editable_in_discovery
-#
-#     @property
-#     def property_name(self):
-#         return self._property_name
-#
-#     @property_name.setter
-#     def property_name(self, value):
-#         if type(value) != str:
-#             raise TypeError(f"The property_name parameter only accepts strings. ")
-#         elif type(value) == str and not 0 < len(value) < 50:
-#             raise ValueError(
-#                 f"property_name length must be between 1 and 50. Supplied '{value}' "
-#                 f"contains {len(value)} characters. ")
-#         elif type(value) == str and 0 < len(value) < 50 and value != convert_to_pascal_case(value):
-#             raise ValueError(
-#                 f"property_name must be in pascal case - i.e. start with a capital letter and contain no spaces, "
-#                 f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
-#             )
-#         self._property_name = value
-#
-#     @property
-#     def display_label(self):
-#         return self._display_label
-#
-#     @display_label.setter
-#     def display_label(self, value):
-#         if type(value) != str:
-#             raise TypeError(f"The display_label parameter only accepts strings. ")
-#         elif type(value) == str and not 0 < len(value) <= 255:
-#             raise ValueError(
-#                 f"display_label length must be between 1 and 50. Supplied '{value}' "
-#                 f"contains {len(value)} characters. ")
-#         self._display_label = value
-#
-#     @property
-#     def editor(self):
-#         return self._editor
-#
-#     @editor.setter
-#     def editor(self, value):
-#         if not set([value]).issubset(set(INTEGRATION_SETTINGS_EDITORS.__args__)):
-#             raise ValueError(
-#                 f"The editor parameter must be set to one of the allowed values defined by the "
-#                 f"INTEGRATION_SETTINGS_EDITORS literal: {INTEGRATION_SETTINGS_EDITORS.__args__}")
-#         self._editor = value
-#
-#     @property
-#     def required_for_task(self):
-#         return self._required_for_task
-#
-#     @required_for_task.setter
-#     def required_for_task(self, value):
-#         if type(value) != bool:
-#             raise TypeError(f"The required_for_task parameter must be passed a boolean. ")
-#         self._required_for_task = value
-#
-#     @property
-#     def use_for_discovery(self):
-#         return self._use_for_discovery
-#
-#     @use_for_discovery.setter
-#     def use_for_discovery(self, value):
-#         if type(value) != bool:
-#             raise TypeError(f"The use_for_discovery parameter must be passed a boolean. ")
-#         self._use_for_discovery = value
-#
-#     @property
-#     def editable_in_discovery(self):
-#         return self._editable_in_discovery
-#
-#     @editable_in_discovery.setter
-#     def editable_in_discovery(self, value):
-#         if type(value) != bool:
-#             raise TypeError(f"The editable_in_discovery parameter must be passed a boolean. ")
-#         self._editable_in_discovery = value
-
-
-class IntegrationDeviceConfigPropertyDefinition(BaseProperty):
-    """
-    Class corresponding to the configuration properties for the Integration device type for the given task.
-
-    Parameters
-    ----------
-    property_name : str
-        Must be in pascal case
-    display_label : str
-        Pretty version of the property_name
-    editor : INTEGRATION_SETTINGS_EDITORS
-        The editor used in the UI
-    default_value : , default=None
-        The default value, if applicable, for the given property
-    allowed_values : list, default=None
-        the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
-        None.
-    required_for_task : bool, default=False
-        Determines whether the given property is required for the task to run (Default Value = False).
-    use_for_discovery: bool, default=False
-        Determines whether the given property is required for the discovery to run (Default Value = False).
-    editable_in_discovery: bool, default=False
-        Determines whether the given property is editable when triggering discovery (Default value = False).
-    """
-    def __init__(self, property_name, display_label, editor, default_value, allowed_values,
-                 required_for_task: bool = False, use_for_discovery: bool = False, editable_in_discovery: bool = False):
-        """
-
-        Parameters
-        ----------
-        property_name : str
-            Must be in pascal case
-        display_label : str
-            Pretty version of the property_name
-        editor : INTEGRATION_SETTINGS_EDITORS
-            The editor used in the UI
-        default_value : , default=None
-            The default value, if applicable, for the given property
-        allowed_values : list, default=None
-            the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
-            None.
-        required_for_task : bool, default=False
-            Determines whether the given property is required for the task to run (Default Value = False).
-        use_for_discovery: bool, default=False
-            Determines whether the given property is required for the discovery to run (Default Value = False).
-        editable_in_discovery: bool, default=False
-            Determines whether the given property is editable when triggering discovery (Default value = False).
-        """
-        super(IntegrationDeviceConfigPropertyDefinition, self).__init__(
-            property_name=property_name, display_label=display_label, editor=editor, default_value=default_value,
-            allowed_values=allowed_values)
-        self.required_for_task = required_for_task
-        self.use_for_discovery = use_for_discovery
-        self.editable_in_discovery = editable_in_discovery
-        self._is_configuration = True
-
-    @property
-    def required_for_task(self) -> bool:
-        """"Determines whether the given property is required for the task to run (Default Value = False). """
-        return self._required_for_task
-
-    @required_for_task.setter
-    def required_for_task(self, value) -> bool:
-        """Determines whether the given property is required for the task to run. """
-        if type(value) != bool:
-            raise TypeError(f"The required_for_task parameter must be passed a boolean. ")
-        self._required_for_task = value
-
-    @property
-    def use_for_discovery(self) -> bool:
-        """Defines whether the given property should be used when triggering a discovery. """
-        return self._use_for_discovery
-
-    @use_for_discovery.setter
-    def use_for_discovery(self, value: bool) -> bool:
-        """Defines whether the given property should be used when triggering a discovery. """
-        if type(value) != bool:
-            raise TypeError(f"The use_for_discovery parameter must be passed a boolean. ")
-        self._use_for_discovery = value
-
-    @property
-    def editable_in_discovery(self) -> bool:
-        """Defines whether the given property is editable when triggering a discovery. """
-        return self._editable_in_discovery
-
-    @editable_in_discovery.setter
-    def editable_in_discovery(self, value) -> bool:
-        """Defines whether the given property is editable when triggering a discovery. """
-        if type(value) != bool:
-            raise TypeError(f"The editable_in_discovery parameter must be passed a boolean. ")
-        self._editable_in_discovery = value
-
-    @property
-    def is_configuration(self):
-        return self._is_configuration
-
-
-class IntegrationDeviceDefinition:
-    """
-    Aligns with the definition of the Integration DriverDeviceType.
-    """
-    def __init__(self, device_type: str, default_device_name: str,
-                 config_properties: List[IntegrationDeviceConfigPropertyDefinition],
-                 expose_address: bool, address_label: str):
-        """
-
-        Parameters
-        ----------
-        device_type : str
-            The DriverDeviceType of the integration device type. Must be in pascal-case.
-        default_device_name : str
-            The default device name for this DriverDeviceType.
-        config_properties : List[IntegrationDeviceConfigPropertyDefinition]
-            A list of the configuration properties associated with the device type. The items in the list should be
-            instances of the `IntegrationDeviceConfigPropertyDefinition` class.
-        expose_address : bool
-            Defines whether the Address is exposed to the user & platform UI.
-        address_label : str or None
-            If `expose_address` is True, tben this defines the label shown to user in UI.
-        """
-        self.device_type = device_type
-        self.default_device_name = default_device_name
-        self.config_properties = config_properties
-        self._is_integration = True
-        self.expose_address = expose_address
-        self.address_label = address_label
-
-    @property
-    def device_type(self) -> str:
-        """The DriverDeviceType of the integration device type. Must be in pascal-case. """
-        return self._device_type
-
-    @device_type.setter
-    def device_type(self, value) -> str:
-        """The DriverDeviceType of the integration device type. Must be in pascal-case. """
-        if type(value) != str:
-            raise TypeError(f"The device_type parameter only accepts strings. ")
-        elif type(value) == str and not 0 < len(value) <= 100:
-            raise ValueError(f"DeviceTypeDefinition.device_type length must be between 1 and 100. Supplied '{value}' "
-                             f"contains {len(value)} characters. ")
-        self._device_type = value
-
-    @property
-    def default_device_name(self) -> str:
-        """The default device name for this DriverDeviceType. """
-        return self._default_device_name
-
-    @default_device_name.setter
-    def default_device_name(self, value) -> str:
-        """The default device name for this DriverDeviceType. """
-        if type(value) != str:
-            raise TypeError(f"The default_device_name parameter only accepts strings. ")
-        elif type(value) == str and not 0 < len(value) < 50:
-            raise ValueError(f"IntegrationDeviceTypeDefinition.default_device_name length must be between 1 and 50. "
-                             f"Supplied '{value}' contains {len(value)} characters. ")
-        self._default_device_name = value
-
-    @property
-    def config_properties(self) -> List[IntegrationDeviceConfigPropertyDefinition]:
-        """A list of the configuration properties associated with the device type. The items in the list should be
-            instances of the `IntegrationDeviceConfigPropertyDefinition` class. """
-        return self._config_properties
-
-    @config_properties.setter
-    def config_properties(self, value) -> List[IntegrationDeviceConfigPropertyDefinition]:
-        """A list of the configuration properties associated with the device type. The items in the list should be
-            instances of the `IntegrationDeviceConfigPropertyDefinition` class. """
-        if type(value) != list:
-            raise TypeError(f"The sensor_properties parameter must be passed a list. ")
-        elif type(value) == list:
-            for i in range(len(value)):
-                if isinstance(value[i], IntegrationDeviceConfigPropertyDefinition) == False:
-                    raise TypeError(f"Each item in the list passed to the config_properties parameter must be an "
-                                    f"instance of the IntegrationDeviceConfigPropertyDefinition class")
-        self._config_properties = value
-
-    @property
-    def is_integration(self):
-        return self._is_integration
-
-    @property
-    def expose_address(self) -> bool:
-        """Defines whether the Address is exposed to the user & platform UI. """
-        return self._expose_address
-
-    @expose_address.setter
-    def expose_address(self, value) -> bool:
-        """Defines whether the Address is exposed to the user & platform UI. """
-        if type(value) != bool:
-            raise TypeError(f"The expose_address parameter must be passed a boolean. ")
-        self._expose_address = value
-
-    @property
-    def address_label(self) -> str:
-        """If `expose_address` is True, tben this defines the label shown to user in UI."""
-        return self._address_label
-
-    @address_label.setter
-    def address_label(self, value) -> str:
-        """If `expose_address` is True, tben this defines the label shown to user in UI."""
-        if type(value) != str and self.expose_address == False and value is not None:
-            raise TypeError(f"The address_label parameter only accepts strings. ")
-        elif value is None and self.expose_address == True:
-            raise ValueError(f"address_label must be set to a string (in PascalCase) when expose_address is True. The "
-                             f"parameter address_label can only be set to None when expose_address is False. ")
-        elif type(value) == str and value != convert_to_pascal_case(value):
-            raise ValueError(
-                f"address_label must be in PascalCase - i.e. start with a capital letter and contain no spaces, "
-                f"dashes, underscores, etc. The supplied '{value}' is not in PascalCase. "
-            )
-        self._address_label = value
-
-
-# class SensorDefinition:
-#     def __init__(self, property_name: str, default_sensor_name: str, object_property_template_name, unit_of_measure,
-#                  is_configuration: bool = False):
-#         self.property_name = property_name
-#         # self.is_configuration = is_configuration
-#         self._is_configuration = False
-#         self.default_sensor_name = default_sensor_name
-#         self.object_property_template_name = object_property_template_name
-#         self.unit_of_measure = unit_of_measure
-#
-#     @property
-#     def property_name(self):
-#         return self._property_name
-#
-#     @property_name.setter
-#     def property_name(self, value):
-#         if type(value) != str:
-#             raise TypeError(f"The property_name parameter only accepts strings. ")
-#         elif type(value) == str and not 0 < len(value) < 50:
-#             raise ValueError(
-#                 f"property_name length must be between 1 and 50. Supplied '{value}' "
-#                 f"contains {len(value)} characters. ")
-#         elif type(value) == str and 0 < len(value) < 50 and value != convert_to_pascal_case(value):
-#             raise ValueError(
-#                 f"property_name must be in pascal case - i.e. start with a capital letter and contain no spaces, "
-#                 f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
-#             )
-#         self._property_name = value
-#
-#     @property
-#     def is_configuration(self):
-#         return self._is_configuration
-#
-#     # @is_configuration.setter
-#     # def is_configuration(self, value):
-#     #     if type(value) != bool:
-#     #         raise TypeError(f"The is_configuration parameter only accepts boolean values - i.e. True or False ")
-#     #     self._is_configuration = value
-#
-#     @property
-#     def object_property_template_name(self):
-#         return self._object_property_template_name
-#
-#     @object_property_template_name.setter
-#     def object_property_template_name(self, value):  # max characters = 255, pascal case
-#         allowed_values = get_templates(api_inputs=api_inputs)
-#         if self.is_configuration == False and not set([value]).issubset(set(allowed_values[
-#                                                                                 'ObjectPropertyTemplateName'])):
-#             raise ValueError(f"{value} is not an allowed value for ObjectPropertyTemplateName. Please use "
-#                              f"sw.integration.get_templates() function to retrieve list of the allowed values. ")
-#         elif self.is_configuration == True and value is not None:
-#             raise ValueError(f"If the defined property has IsConfiguration set to True, the value for "
-#                              f"ObjectPropertyTemplateName should be set to None. ")
-#         elif self.is_configuration == True and value is None:
-#             self._object_property_type = None
-#         elif self.is_configuration == False and set([value]).issubset(set(
-#                 allowed_values['ObjectPropertyTemplateName'])):
-#             self._object_property_type = allowed_values[
-#                 allowed_values['ObjectPropertyTemplateName'] == value].ObjectPropertyType.item()
-#         self._object_property_template_name = value
-#
-#     # @object_property_template_name.setter
-#     # def object_property_template_name(self, value):
-#     #     allowed_values = get_templates(api_inputs=api_inputs)
-#     #     if self.is_configuration == False and not set([value]).issubset(set(allowed_values[
-#     #                                                                             'ObjectPropertyTemplateName'])):
-#     #         raise ValueError(f"{value} is not an allowed value for ObjectPropertyTemplateName. Please use "
-#     #                          f"sw.integration.get_templates() function to retrieve list of the allowed values. ")
-#     #     elif self.is_configuration==True and value is not None:
-#     #         raise ValueError(f"If the defined property has IsConfiguration set to True, the value for "
-#     #                          f"ObjectPropertyTemplateName should be set to None. ")
-#     #     elif self.is_configuration==True and value is None:
-#     #         self._object_property_type = None
-#     #     elif self.is_configuration == False and set([value]).issubset(set(
-#     #             allowed_values['ObjectPropertyTemplateName'])):
-#     #         self._object_property_type = allowed_values[
-#     #             allowed_values['ObjectPropertyTemplateName'] == value].ObjectPropertyType.item()
-#     #     self._object_property_template_name = value
-#
-#     @property
-#     def unit_of_measure(self):
-#         return self._unit_of_measure
-#
-#     @unit_of_measure.setter
-#     def unit_of_measure(self, value):  # max chars = 50, sentence case
-#         if self._object_property_type is not None:
-#             allowed_values = get_units_of_measure(api_inputs=initialize(),
-#                                                   object_property_type=self._object_property_type)
-#             if self.is_configuration == False and not set([value]).issubset(set(allowed_values[
-#                                                                                     'UnitOfMeasureDescription'])):
-#                 raise ValueError(f"{value} is not an allowed value for the Unit of Measure. Please use "
-#                                  f"sw.integration.get_units_of_measure() function to retrieve list of the "
-#                                  f"allowed values. ")
-#             elif self.is_configuration == True and value is not None:
-#                 raise ValueError(f"If the defined property has IsConfiguration set to True, the value for "
-#                                  f"UnitOfMeasure should be set to None. ")
-#         self._unit_of_measure = value
-#       # if self._object_property_type is not None:
-#       #       allowed_values = sw.integration.get_units_of_measure(api_inputs=api_inputs,
-#       #                                                            object_property_type=self._object_property_type)
-#       #       if self.is_configuration == False and not set([value]).issubset(set(allowed_values[
-#       #                                                                               'UnitOfMeasureDescription'])):
-#       #           raise ValueError(f"{value} is not an allowed value for the Unit of Measure. Please use "
-#       #                            f"sw.integration.get_units_of_measure() function to retrieve list of the "
-#       #                            f"allowed values. ")
-#       #       elif self.is_configuration==True:
-#       #           raise ValueError(f"If the defined property has IsConfiguration set to True, the value for "
-#       #                            f"UnitOfMeasure should be set to None. ")
-#       #   elif self._object_property_type is None:
-#       #       if self.is_configuration == False:
-#       #           raise ValueError(f"unit_of_measure can only be set to None if the is_configuration parameter is set"
-#       #                            f" to True. ")
-#
-#     @property
-#     def default_sensor_name(self):
-#         return self._default_sensor_name
-#
-#     @default_sensor_name.setter
-#     def default_sensor_name(self, value):
-#         if type(value) != str:
-#             raise TypeError(f"The default_sensor_name parameter only accepts strings. ")
-#         elif type(value) == str and not 0 < len(value) <= 255:
-#             raise ValueError(
-#                 f"default_sensor_name length must be between 1 and 255. Supplied '{value}' "
-#                 f"contains {len(value)} characters. ")
-#         self._default_sensor_name = value
-#
-#
-# class DeviceTypeConfigPropertyDefinition(BaseProperty):
-#     def __init__(self, property_name, display_label, default_value,
-#                  editor, allowed_values):
-#         super(DeviceTypeConfigPropertyDefinition, self).__init__(property_name, display_label, default_value,
-#                                                                  editor, allowed_values)
-#         self._is_configuration = True
-#         self._object_property_template_name = 'Configuration'
-#         self._unit_of_measure = None
-#
-#     @property
-#     def is_configuration(self):
-#         return self._is_configuration
-#
-#     @property
-#     def object_property_template_name(self):
-#         return self._object_property_template_name
-#
-#     @property
-#     def unit_of_measure(self):
-#         return self._unit_of_measure
-#
-#
-# class DeviceTypeDefinition:
-#     def __init__(self, device_type: str, default_device_name: str, sensor_properties: List[SensorDefinition],
-#                  expose_address: bool, address_label: str,
-#                  config_properties: List[DeviceTypeConfigPropertyDefinition] = None):
-#         self.device_type = device_type
-#         self.default_device_name = default_device_name
-#         self.sensor_properties = sensor_properties
-#         self._is_integration = False
-#         self.expose_address = expose_address
-#         self.address_label = address_label
-#         self.config_properties = config_properties
-#
-#     @property
-#     def device_type(self):
-#         return self._device_type
-#
-#     @device_type.setter
-#     def device_type(self, value):
-#         if type(value) != str:
-#             raise TypeError(f"The device_type parameter only accepts strings. ")
-#         elif type(value) == str and not 0 < len(value) < 50:
-#             raise ValueError(f"DeviceTypeDefinition.device_type length must be between 1 and 50. Supplied '{value}' "
-#                              f"contains {len(value)} characters. ")
-#         elif type(value) == str and 0 < len(value) < 50 and value != convert_to_pascal_case(value):
-#             raise ValueError(
-#                 f"device_type must be in pascal case - i.e. start with a capital letter and contain no spaces, "
-#                 f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
-#             )
-#         self._device_type = value
-#
-#     @property
-#     def default_device_name(self):
-#         return self._default_device_name
-#
-#     @default_device_name.setter
-#     def default_device_name(self, value):
-#         if type(value) != str:
-#             raise TypeError(f"The default_device_name parameter only accepts strings. ")
-#         elif type(value) == str and len(value) > 50:
-#             raise ValueError(f"The default_device_name has a maximum character count of 100. Supplied '{value}' "
-#                              f"contains {len(value)} characters. ")
-#         elif type(value) == str and len(value) == 0:
-#             raise ValueError(f"The default_device_name must not be an empty string. ")
-#         self._default_device_name = value
-#
-#     @property
-#     def sensor_properties(self):
-#         return self._sensor_properties
-#
-#     @sensor_properties.setter
-#     def sensor_properties(self, value):
-#         if type(value) != list:
-#             raise TypeError(f"The sensor_properties parameter must be passed a list. ")
-#         elif type(value) == list:
-#             for i in range(len(value)):
-#                 if isinstance(value[i], SensorDefinition) == False:
-#                     raise TypeError(f"Each item in the list passed to the sensor_properties parameter must be an "
-#                                     f"instance of the SensorDefinition class")
-#         self._sensor_properties = value
-#
-#     @property
-#     def is_integration(self):
-#         return self._is_integration
-#
-#     @property
-#     def expose_address(self):
-#         return self._expose_address
-#
-#     @expose_address.setter
-#     def expose_address(self, value):
-#         if type(value) != bool:
-#             raise TypeError(f"The expose_address parameter must be passed a boolean. ")
-#         self._expose_address = value
-#
-#     @property
-#     def address_label(self):
-#         return self._address_label
-#
-#     @address_label.setter
-#     def address_label(self, value):
-#         if type(value) != str:
-#             raise TypeError(f"The address_label parameter only accepts strings. ")
-#         elif type(value) == str and value != convert_to_pascal_case(value):
-#             raise ValueError(
-#                 f"address_label must be in pascal case - i.e. start with a capital letter and contain no spaces, "
-#                 f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
-#             )
-#         self._address_label = value
-#
-#     @property
-#     def config_properties(self):
-#         return self._config_properties
-#
-#     @config_properties.setter
-#     def config_properties(self, value):
-#         if type(value) != list and value is not None:
-#             raise TypeError(f"The config_properties parameter must be passed a list. ")
-#         elif type(value) == list:
-#             for i in range(len(value)):
-#                 if isinstance(value[i], DeviceTypeConfigPropertyDefinition) == False:
-#                     raise TypeError(f"Each item in the list passed to the config_properties parameter must be an "
-#                                     f"instance of the DeviceTypeConfigPropertyDefinition class")
-#         self._config_properties = value
-#
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""Module defining the various definitions used internally by the pipeline module classes.
+"""
+# import uuid
+from typing import List
+from .._utils._utils import convert_to_pascal_case
+from .._utils._constants import INTEGRATION_SETTINGS_EDITORS
+# from ..integration import get_templates, get_units_of_measure
+# from .. import initialize
+
+
+class BaseProperty:
+    """BaseProperty class definition
+
+    Parameters
+    ----------
+    property_name : str
+        Must be in pascal case
+    display_label : str
+        Pretty version of the property_name
+    editor : INTEGRATION_SETTINGS_EDITORS
+        The editor used in the UI
+    default_value : , default=None
+        The default value, if applicable, for the given property
+    allowed_values : list, default=None
+        the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be None.
+
+    """
+    def __init__(self, property_name: str, display_label: str, editor: INTEGRATION_SETTINGS_EDITORS,
+                 default_value: object = None, allowed_values: List[object] = None):
+        """
+        Parameters
+        ----------
+        property_name : str
+            Must be in pascal case
+        display_label : str
+            Pretty version of the property_name
+        editor : INTEGRATION_SETTINGS_EDITORS
+            The editor used in the UI
+        default_value : , default=None
+            The default value, if applicable, for the given property
+        allowed_values : list, default=None
+            the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
+            None.
+
+        """
+        self.property_name = property_name
+        self.display_label = display_label
+        self.editor = editor
+        self.default_value = default_value
+        self.allowed_values = allowed_values
+
+    @property
+    def property_name(self) -> str:
+        """property_name : str
+            Must be in pascal case"""
+        return self._property_name
+
+    @property_name.setter
+    def property_name(self, value) -> str:
+        """property_name : str
+                    Must be in pascal case"""
+        if type(value) != str:
+            raise TypeError(f"The property_name parameter only accepts strings. ")
+        elif type(value) == str and not 0 < len(value) < 50:
+            raise ValueError(
+                f"property_name length must be between 1 and 50. Supplied '{value}' "
+                f"contains {len(value)} characters. ")
+        elif type(value) == str and 0 < len(value) < 50 and value != convert_to_pascal_case(value):
+            raise ValueError(
+                f"property_name must be in pascal case - i.e. start with a capital letter and contain no spaces, "
+                f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
+            )
+        self._property_name = value
+
+    @property
+    def display_label(self) -> str:
+        """The display label for the given property"""
+        return self._display_label
+
+    @display_label.setter
+    def display_label(self, value) -> str:
+        """The display label for the given property"""
+        if type(value) != str:
+            raise TypeError(f"The display_label parameter only accepts strings. ")
+        elif type(value) == str and not 0 < len(value) <= 255:
+            raise ValueError(
+                f"display_label length must be between 1 and 50. Supplied '{value}' "
+                f"contains {len(value)} characters. ")
+        self._display_label = value
+
+    @property
+    def editor(self) -> INTEGRATION_SETTINGS_EDITORS:
+        """The editor to be used in the UI"""
+        return self._editor
+
+    @editor.setter
+    def editor(self, value) -> INTEGRATION_SETTINGS_EDITORS:
+        """The editor to be used in the UI"""
+        if not set([value]).issubset(set(INTEGRATION_SETTINGS_EDITORS.__args__)):
+            raise ValueError(
+                f"Supplied value '{value}' is invalid. The editor parameter must be set to one of the allowed values "
+                f"defined by the INTEGRATION_SETTINGS_EDITORS literal: {INTEGRATION_SETTINGS_EDITORS.__args__}")
+        self._editor = value
+
+
+class EventWorkOrderFieldDefinition(BaseProperty):
+    """EventWorkOrderFieldDefinition class definition
+
+    Used to define the set of fields available in Events UI for creation of work orders in 3rd party systems.
+
+    Parameters
+    ----------
+    property_name : str
+        Must be in pascal case
+    display_label : str
+        Pretty version of the property_name (what is displayed in Events UI).
+    editor : INTEGRATION_SETTINGS_EDITORS
+        The editor used in the UI
+    default_value : , default=None
+        The default value, if applicable, for the given property
+    allowed_values : list, default=None
+        the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be None.
+    """
+    def __init__(self, property_name, display_label, editor, default_value, allowed_values):
+        """
+
+        Parameters
+        ----------
+        property_name : str
+            Must be in pascal case
+        display_label : str
+            Pretty version of the property_name
+        editor : INTEGRATION_SETTINGS_EDITORS
+            The editor used in the UI
+        default_value : , default=None
+            The default value, if applicable, for the given property
+        allowed_values : list, default=None
+            The set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
+            None.
+        """
+        super(EventWorkOrderFieldDefinition, self).__init__(
+            property_name=property_name, display_label=display_label, editor=editor, default_value=default_value,
+            allowed_values=allowed_values)
+
+
+class AnalyticsSettings(BaseProperty):
+    """
+
+    """
+    def __init__(self, property_name, display_label, editor, default_value, allowed_values):
+        """
+        Parameters
+        ----------
+        property_name : str
+            Must be in pascal case
+        display_label : str
+            Pretty version of the property_name
+        editor : INTEGRATION_SETTINGS_EDITORS
+            The editor used in the UI
+        default_value : , default=None
+            The default value, if applicable, for the given property
+        allowed_values : list, default=None
+            the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
+            None.
+        """
+        super(AnalyticsSettings, self).__init__(
+            property_name=property_name, display_label=display_label, editor=editor, default_value=default_value,
+            allowed_values=allowed_values)
+
+
+class IntegrationSettings(BaseProperty):
+    """Class used for defining the UI editors, default values, etc for the key-values passed to integration_settings
+    dictionaries.
+
+    Parameters
+    ----------
+    property_name : str
+        Must be in pascal case
+    display_label : str
+        Pretty version of the property_name
+    editor : INTEGRATION_SETTINGS_EDITORS
+        The editor used in the UI
+    default_value : , default=None
+        The default value, if applicable, for the given property
+    allowed_values : list, default=None
+        the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
+        None.
+    """
+    def __init__(self, property_name, display_label, editor, default_value, allowed_values):
+        """
+
+        Parameters
+        ----------
+        property_name : str
+            Must be in pascal case
+        display_label : str
+            Pretty version of the property_name
+        editor : INTEGRATION_SETTINGS_EDITORS
+            The editor used in the UI
+        default_value : , default=None
+            The default value, if applicable, for the given property
+        allowed_values : list, default=None
+            the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
+            None.
+        """
+        super(IntegrationSettings, self).__init__(
+            property_name=property_name, display_label=display_label, editor=editor, default_value=default_value,
+            allowed_values=allowed_values)
+
+
+# class IntegrationDevicePropertyDefinition2:
+#     def __init__(self, property_name: str, display_label: str, default_value: object = None,
+#                  editor: INTEGRATION_SETTINGS_EDITORS = 'text_box', values: List[object] = None,
+#                  required_for_task: bool = False, use_for_discovery: bool = False, editable_in_discovery: bool = False):
+#         self.property_name = property_name
+#         self.display_label = display_label
+#         self.default_value = default_value
+#         self.editor = editor
+#         self.values = values
+#         self.required_for_task = required_for_task
+#         self.use_for_discovery = use_for_discovery
+#         self.editable_in_discovery = editable_in_discovery
+#
+#     @property
+#     def property_name(self):
+#         return self._property_name
+#
+#     @property_name.setter
+#     def property_name(self, value):
+#         if type(value) != str:
+#             raise TypeError(f"The property_name parameter only accepts strings. ")
+#         elif type(value) == str and not 0 < len(value) < 50:
+#             raise ValueError(
+#                 f"property_name length must be between 1 and 50. Supplied '{value}' "
+#                 f"contains {len(value)} characters. ")
+#         elif type(value) == str and 0 < len(value) < 50 and value != convert_to_pascal_case(value):
+#             raise ValueError(
+#                 f"property_name must be in pascal case - i.e. start with a capital letter and contain no spaces, "
+#                 f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
+#             )
+#         self._property_name = value
+#
+#     @property
+#     def display_label(self):
+#         return self._display_label
+#
+#     @display_label.setter
+#     def display_label(self, value):
+#         if type(value) != str:
+#             raise TypeError(f"The display_label parameter only accepts strings. ")
+#         elif type(value) == str and not 0 < len(value) <= 255:
+#             raise ValueError(
+#                 f"display_label length must be between 1 and 50. Supplied '{value}' "
+#                 f"contains {len(value)} characters. ")
+#         self._display_label = value
+#
+#     @property
+#     def editor(self):
+#         return self._editor
+#
+#     @editor.setter
+#     def editor(self, value):
+#         if not set([value]).issubset(set(INTEGRATION_SETTINGS_EDITORS.__args__)):
+#             raise ValueError(
+#                 f"The editor parameter must be set to one of the allowed values defined by the "
+#                 f"INTEGRATION_SETTINGS_EDITORS literal: {INTEGRATION_SETTINGS_EDITORS.__args__}")
+#         self._editor = value
+#
+#     @property
+#     def required_for_task(self):
+#         return self._required_for_task
+#
+#     @required_for_task.setter
+#     def required_for_task(self, value):
+#         if type(value) != bool:
+#             raise TypeError(f"The required_for_task parameter must be passed a boolean. ")
+#         self._required_for_task = value
+#
+#     @property
+#     def use_for_discovery(self):
+#         return self._use_for_discovery
+#
+#     @use_for_discovery.setter
+#     def use_for_discovery(self, value):
+#         if type(value) != bool:
+#             raise TypeError(f"The use_for_discovery parameter must be passed a boolean. ")
+#         self._use_for_discovery = value
+#
+#     @property
+#     def editable_in_discovery(self):
+#         return self._editable_in_discovery
+#
+#     @editable_in_discovery.setter
+#     def editable_in_discovery(self, value):
+#         if type(value) != bool:
+#             raise TypeError(f"The editable_in_discovery parameter must be passed a boolean. ")
+#         self._editable_in_discovery = value
+
+
+class IntegrationDeviceConfigPropertyDefinition(BaseProperty):
+    """
+    Class corresponding to the configuration properties for the Integration device type for the given task.
+
+    Parameters
+    ----------
+    property_name : str
+        Must be in pascal case
+    display_label : str
+        Pretty version of the property_name
+    editor : INTEGRATION_SETTINGS_EDITORS
+        The editor used in the UI
+    default_value : , default=None
+        The default value, if applicable, for the given property
+    allowed_values : list, default=None
+        the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
+        None.
+    required_for_task : bool, default=False
+        Determines whether the given property is required for the task to run (Default Value = False).
+    use_for_discovery: bool, default=False
+        Determines whether the given property is required for the discovery to run (Default Value = False).
+    editable_in_discovery: bool, default=False
+        Determines whether the given property is editable when triggering discovery (Default value = False).
+    """
+    def __init__(self, property_name, display_label, editor, default_value, allowed_values,
+                 required_for_task: bool = False, use_for_discovery: bool = False, editable_in_discovery: bool = False):
+        """
+
+        Parameters
+        ----------
+        property_name : str
+            Must be in pascal case
+        display_label : str
+            Pretty version of the property_name
+        editor : INTEGRATION_SETTINGS_EDITORS
+            The editor used in the UI
+        default_value : , default=None
+            The default value, if applicable, for the given property
+        allowed_values : list, default=None
+            the set of allowed values (if applicable) for the given property_name. If editor=text_box, this should be
+            None.
+        required_for_task : bool, default=False
+            Determines whether the given property is required for the task to run (Default Value = False).
+        use_for_discovery: bool, default=False
+            Determines whether the given property is required for the discovery to run (Default Value = False).
+        editable_in_discovery: bool, default=False
+            Determines whether the given property is editable when triggering discovery (Default value = False).
+        """
+        super(IntegrationDeviceConfigPropertyDefinition, self).__init__(
+            property_name=property_name, display_label=display_label, editor=editor, default_value=default_value,
+            allowed_values=allowed_values)
+        self.required_for_task = required_for_task
+        self.use_for_discovery = use_for_discovery
+        self.editable_in_discovery = editable_in_discovery
+        self._is_configuration = True
+
+    @property
+    def required_for_task(self) -> bool:
+        """"Determines whether the given property is required for the task to run (Default Value = False). """
+        return self._required_for_task
+
+    @required_for_task.setter
+    def required_for_task(self, value) -> bool:
+        """Determines whether the given property is required for the task to run. """
+        if type(value) != bool:
+            raise TypeError(f"The required_for_task parameter must be passed a boolean. ")
+        self._required_for_task = value
+
+    @property
+    def use_for_discovery(self) -> bool:
+        """Defines whether the given property should be used when triggering a discovery. """
+        return self._use_for_discovery
+
+    @use_for_discovery.setter
+    def use_for_discovery(self, value: bool) -> bool:
+        """Defines whether the given property should be used when triggering a discovery. """
+        if type(value) != bool:
+            raise TypeError(f"The use_for_discovery parameter must be passed a boolean. ")
+        self._use_for_discovery = value
+
+    @property
+    def editable_in_discovery(self) -> bool:
+        """Defines whether the given property is editable when triggering a discovery. """
+        return self._editable_in_discovery
+
+    @editable_in_discovery.setter
+    def editable_in_discovery(self, value) -> bool:
+        """Defines whether the given property is editable when triggering a discovery. """
+        if type(value) != bool:
+            raise TypeError(f"The editable_in_discovery parameter must be passed a boolean. ")
+        self._editable_in_discovery = value
+
+    @property
+    def is_configuration(self):
+        return self._is_configuration
+
+
+class IntegrationDeviceDefinition:
+    """
+    Aligns with the definition of the Integration DriverDeviceType.
+    """
+    def __init__(self, device_type: str, default_device_name: str,
+                 config_properties: List[IntegrationDeviceConfigPropertyDefinition],
+                 expose_address: bool, address_label: str):
+        """
+
+        Parameters
+        ----------
+        device_type : str
+            The DriverDeviceType of the integration device type. Must be in pascal-case.
+        default_device_name : str
+            The default device name for this DriverDeviceType.
+        config_properties : List[IntegrationDeviceConfigPropertyDefinition]
+            A list of the configuration properties associated with the device type. The items in the list should be
+            instances of the `IntegrationDeviceConfigPropertyDefinition` class.
+        expose_address : bool
+            Defines whether the Address is exposed to the user & platform UI.
+        address_label : str or None
+            If `expose_address` is True, tben this defines the label shown to user in UI.
+        """
+        self.device_type = device_type
+        self.default_device_name = default_device_name
+        self.config_properties = config_properties
+        self._is_integration = True
+        self.expose_address = expose_address
+        self.address_label = address_label
+
+    @property
+    def device_type(self) -> str:
+        """The DriverDeviceType of the integration device type. Must be in pascal-case. """
+        return self._device_type
+
+    @device_type.setter
+    def device_type(self, value) -> str:
+        """The DriverDeviceType of the integration device type. Must be in pascal-case. """
+        if type(value) != str:
+            raise TypeError(f"The device_type parameter only accepts strings. ")
+        elif type(value) == str and not 0 < len(value) <= 100:
+            raise ValueError(f"DeviceTypeDefinition.device_type length must be between 1 and 100. Supplied '{value}' "
+                             f"contains {len(value)} characters. ")
+        self._device_type = value
+
+    @property
+    def default_device_name(self) -> str:
+        """The default device name for this DriverDeviceType. """
+        return self._default_device_name
+
+    @default_device_name.setter
+    def default_device_name(self, value) -> str:
+        """The default device name for this DriverDeviceType. """
+        if type(value) != str:
+            raise TypeError(f"The default_device_name parameter only accepts strings. ")
+        elif type(value) == str and not 0 < len(value) < 50:
+            raise ValueError(f"IntegrationDeviceTypeDefinition.default_device_name length must be between 1 and 50. "
+                             f"Supplied '{value}' contains {len(value)} characters. ")
+        self._default_device_name = value
+
+    @property
+    def config_properties(self) -> List[IntegrationDeviceConfigPropertyDefinition]:
+        """A list of the configuration properties associated with the device type. The items in the list should be
+            instances of the `IntegrationDeviceConfigPropertyDefinition` class. """
+        return self._config_properties
+
+    @config_properties.setter
+    def config_properties(self, value) -> List[IntegrationDeviceConfigPropertyDefinition]:
+        """A list of the configuration properties associated with the device type. The items in the list should be
+            instances of the `IntegrationDeviceConfigPropertyDefinition` class. """
+        if type(value) != list:
+            raise TypeError(f"The sensor_properties parameter must be passed a list. ")
+        elif type(value) == list:
+            for i in range(len(value)):
+                if isinstance(value[i], IntegrationDeviceConfigPropertyDefinition) == False:
+                    raise TypeError(f"Each item in the list passed to the config_properties parameter must be an "
+                                    f"instance of the IntegrationDeviceConfigPropertyDefinition class")
+        self._config_properties = value
+
+    @property
+    def is_integration(self):
+        return self._is_integration
+
+    @property
+    def expose_address(self) -> bool:
+        """Defines whether the Address is exposed to the user & platform UI. """
+        return self._expose_address
+
+    @expose_address.setter
+    def expose_address(self, value) -> bool:
+        """Defines whether the Address is exposed to the user & platform UI. """
+        if type(value) != bool:
+            raise TypeError(f"The expose_address parameter must be passed a boolean. ")
+        self._expose_address = value
+
+    @property
+    def address_label(self) -> str:
+        """If `expose_address` is True, tben this defines the label shown to user in UI."""
+        return self._address_label
+
+    @address_label.setter
+    def address_label(self, value) -> str:
+        """If `expose_address` is True, tben this defines the label shown to user in UI."""
+        if type(value) != str and self.expose_address == False and value is not None:
+            raise TypeError(f"The address_label parameter only accepts strings. ")
+        elif value is None and self.expose_address == True:
+            raise ValueError(f"address_label must be set to a string (in PascalCase) when expose_address is True. The "
+                             f"parameter address_label can only be set to None when expose_address is False. ")
+        elif type(value) == str and value != convert_to_pascal_case(value):
+            raise ValueError(
+                f"address_label must be in PascalCase - i.e. start with a capital letter and contain no spaces, "
+                f"dashes, underscores, etc. The supplied '{value}' is not in PascalCase. "
+            )
+        self._address_label = value
+
+
+# class SensorDefinition:
+#     def __init__(self, property_name: str, default_sensor_name: str, object_property_template_name, unit_of_measure,
+#                  is_configuration: bool = False):
+#         self.property_name = property_name
+#         # self.is_configuration = is_configuration
+#         self._is_configuration = False
+#         self.default_sensor_name = default_sensor_name
+#         self.object_property_template_name = object_property_template_name
+#         self.unit_of_measure = unit_of_measure
+#
+#     @property
+#     def property_name(self):
+#         return self._property_name
+#
+#     @property_name.setter
+#     def property_name(self, value):
+#         if type(value) != str:
+#             raise TypeError(f"The property_name parameter only accepts strings. ")
+#         elif type(value) == str and not 0 < len(value) < 50:
+#             raise ValueError(
+#                 f"property_name length must be between 1 and 50. Supplied '{value}' "
+#                 f"contains {len(value)} characters. ")
+#         elif type(value) == str and 0 < len(value) < 50 and value != convert_to_pascal_case(value):
+#             raise ValueError(
+#                 f"property_name must be in pascal case - i.e. start with a capital letter and contain no spaces, "
+#                 f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
+#             )
+#         self._property_name = value
+#
+#     @property
+#     def is_configuration(self):
+#         return self._is_configuration
+#
+#     # @is_configuration.setter
+#     # def is_configuration(self, value):
+#     #     if type(value) != bool:
+#     #         raise TypeError(f"The is_configuration parameter only accepts boolean values - i.e. True or False ")
+#     #     self._is_configuration = value
+#
+#     @property
+#     def object_property_template_name(self):
+#         return self._object_property_template_name
+#
+#     @object_property_template_name.setter
+#     def object_property_template_name(self, value):  # max characters = 255, pascal case
+#         allowed_values = get_templates(api_inputs=api_inputs)
+#         if self.is_configuration == False and not set([value]).issubset(set(allowed_values[
+#                                                                                 'ObjectPropertyTemplateName'])):
+#             raise ValueError(f"{value} is not an allowed value for ObjectPropertyTemplateName. Please use "
+#                              f"sw.integration.get_templates() function to retrieve list of the allowed values. ")
+#         elif self.is_configuration == True and value is not None:
+#             raise ValueError(f"If the defined property has IsConfiguration set to True, the value for "
+#                              f"ObjectPropertyTemplateName should be set to None. ")
+#         elif self.is_configuration == True and value is None:
+#             self._object_property_type = None
+#         elif self.is_configuration == False and set([value]).issubset(set(
+#                 allowed_values['ObjectPropertyTemplateName'])):
+#             self._object_property_type = allowed_values[
+#                 allowed_values['ObjectPropertyTemplateName'] == value].ObjectPropertyType.item()
+#         self._object_property_template_name = value
+#
+#     # @object_property_template_name.setter
+#     # def object_property_template_name(self, value):
+#     #     allowed_values = get_templates(api_inputs=api_inputs)
+#     #     if self.is_configuration == False and not set([value]).issubset(set(allowed_values[
+#     #                                                                             'ObjectPropertyTemplateName'])):
+#     #         raise ValueError(f"{value} is not an allowed value for ObjectPropertyTemplateName. Please use "
+#     #                          f"sw.integration.get_templates() function to retrieve list of the allowed values. ")
+#     #     elif self.is_configuration==True and value is not None:
+#     #         raise ValueError(f"If the defined property has IsConfiguration set to True, the value for "
+#     #                          f"ObjectPropertyTemplateName should be set to None. ")
+#     #     elif self.is_configuration==True and value is None:
+#     #         self._object_property_type = None
+#     #     elif self.is_configuration == False and set([value]).issubset(set(
+#     #             allowed_values['ObjectPropertyTemplateName'])):
+#     #         self._object_property_type = allowed_values[
+#     #             allowed_values['ObjectPropertyTemplateName'] == value].ObjectPropertyType.item()
+#     #     self._object_property_template_name = value
+#
+#     @property
+#     def unit_of_measure(self):
+#         return self._unit_of_measure
+#
+#     @unit_of_measure.setter
+#     def unit_of_measure(self, value):  # max chars = 50, sentence case
+#         if self._object_property_type is not None:
+#             allowed_values = get_units_of_measure(api_inputs=initialize(),
+#                                                   object_property_type=self._object_property_type)
+#             if self.is_configuration == False and not set([value]).issubset(set(allowed_values[
+#                                                                                     'UnitOfMeasureDescription'])):
+#                 raise ValueError(f"{value} is not an allowed value for the Unit of Measure. Please use "
+#                                  f"sw.integration.get_units_of_measure() function to retrieve list of the "
+#                                  f"allowed values. ")
+#             elif self.is_configuration == True and value is not None:
+#                 raise ValueError(f"If the defined property has IsConfiguration set to True, the value for "
+#                                  f"UnitOfMeasure should be set to None. ")
+#         self._unit_of_measure = value
+#       # if self._object_property_type is not None:
+#       #       allowed_values = sw.integration.get_units_of_measure(api_inputs=api_inputs,
+#       #                                                            object_property_type=self._object_property_type)
+#       #       if self.is_configuration == False and not set([value]).issubset(set(allowed_values[
+#       #                                                                               'UnitOfMeasureDescription'])):
+#       #           raise ValueError(f"{value} is not an allowed value for the Unit of Measure. Please use "
+#       #                            f"sw.integration.get_units_of_measure() function to retrieve list of the "
+#       #                            f"allowed values. ")
+#       #       elif self.is_configuration==True:
+#       #           raise ValueError(f"If the defined property has IsConfiguration set to True, the value for "
+#       #                            f"UnitOfMeasure should be set to None. ")
+#       #   elif self._object_property_type is None:
+#       #       if self.is_configuration == False:
+#       #           raise ValueError(f"unit_of_measure can only be set to None if the is_configuration parameter is set"
+#       #                            f" to True. ")
+#
+#     @property
+#     def default_sensor_name(self):
+#         return self._default_sensor_name
+#
+#     @default_sensor_name.setter
+#     def default_sensor_name(self, value):
+#         if type(value) != str:
+#             raise TypeError(f"The default_sensor_name parameter only accepts strings. ")
+#         elif type(value) == str and not 0 < len(value) <= 255:
+#             raise ValueError(
+#                 f"default_sensor_name length must be between 1 and 255. Supplied '{value}' "
+#                 f"contains {len(value)} characters. ")
+#         self._default_sensor_name = value
+#
+#
+# class DeviceTypeConfigPropertyDefinition(BaseProperty):
+#     def __init__(self, property_name, display_label, default_value,
+#                  editor, allowed_values):
+#         super(DeviceTypeConfigPropertyDefinition, self).__init__(property_name, display_label, default_value,
+#                                                                  editor, allowed_values)
+#         self._is_configuration = True
+#         self._object_property_template_name = 'Configuration'
+#         self._unit_of_measure = None
+#
+#     @property
+#     def is_configuration(self):
+#         return self._is_configuration
+#
+#     @property
+#     def object_property_template_name(self):
+#         return self._object_property_template_name
+#
+#     @property
+#     def unit_of_measure(self):
+#         return self._unit_of_measure
+#
+#
+# class DeviceTypeDefinition:
+#     def __init__(self, device_type: str, default_device_name: str, sensor_properties: List[SensorDefinition],
+#                  expose_address: bool, address_label: str,
+#                  config_properties: List[DeviceTypeConfigPropertyDefinition] = None):
+#         self.device_type = device_type
+#         self.default_device_name = default_device_name
+#         self.sensor_properties = sensor_properties
+#         self._is_integration = False
+#         self.expose_address = expose_address
+#         self.address_label = address_label
+#         self.config_properties = config_properties
+#
+#     @property
+#     def device_type(self):
+#         return self._device_type
+#
+#     @device_type.setter
+#     def device_type(self, value):
+#         if type(value) != str:
+#             raise TypeError(f"The device_type parameter only accepts strings. ")
+#         elif type(value) == str and not 0 < len(value) < 50:
+#             raise ValueError(f"DeviceTypeDefinition.device_type length must be between 1 and 50. Supplied '{value}' "
+#                              f"contains {len(value)} characters. ")
+#         elif type(value) == str and 0 < len(value) < 50 and value != convert_to_pascal_case(value):
+#             raise ValueError(
+#                 f"device_type must be in pascal case - i.e. start with a capital letter and contain no spaces, "
+#                 f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
+#             )
+#         self._device_type = value
+#
+#     @property
+#     def default_device_name(self):
+#         return self._default_device_name
+#
+#     @default_device_name.setter
+#     def default_device_name(self, value):
+#         if type(value) != str:
+#             raise TypeError(f"The default_device_name parameter only accepts strings. ")
+#         elif type(value) == str and len(value) > 50:
+#             raise ValueError(f"The default_device_name has a maximum character count of 100. Supplied '{value}' "
+#                              f"contains {len(value)} characters. ")
+#         elif type(value) == str and len(value) == 0:
+#             raise ValueError(f"The default_device_name must not be an empty string. ")
+#         self._default_device_name = value
+#
+#     @property
+#     def sensor_properties(self):
+#         return self._sensor_properties
+#
+#     @sensor_properties.setter
+#     def sensor_properties(self, value):
+#         if type(value) != list:
+#             raise TypeError(f"The sensor_properties parameter must be passed a list. ")
+#         elif type(value) == list:
+#             for i in range(len(value)):
+#                 if isinstance(value[i], SensorDefinition) == False:
+#                     raise TypeError(f"Each item in the list passed to the sensor_properties parameter must be an "
+#                                     f"instance of the SensorDefinition class")
+#         self._sensor_properties = value
+#
+#     @property
+#     def is_integration(self):
+#         return self._is_integration
+#
+#     @property
+#     def expose_address(self):
+#         return self._expose_address
+#
+#     @expose_address.setter
+#     def expose_address(self, value):
+#         if type(value) != bool:
+#             raise TypeError(f"The expose_address parameter must be passed a boolean. ")
+#         self._expose_address = value
+#
+#     @property
+#     def address_label(self):
+#         return self._address_label
+#
+#     @address_label.setter
+#     def address_label(self, value):
+#         if type(value) != str:
+#             raise TypeError(f"The address_label parameter only accepts strings. ")
+#         elif type(value) == str and value != convert_to_pascal_case(value):
+#             raise ValueError(
+#                 f"address_label must be in pascal case - i.e. start with a capital letter and contain no spaces, "
+#                 f"dashes, underscores, etc. The supplied '{value}' is not in pascal case. "
+#             )
+#         self._address_label = value
+#
+#     @property
+#     def config_properties(self):
+#         return self._config_properties
+#
+#     @config_properties.setter
+#     def config_properties(self, value):
+#         if type(value) != list and value is not None:
+#             raise TypeError(f"The config_properties parameter must be passed a list. ")
+#         elif type(value) == list:
+#             for i in range(len(value)):
+#                 if isinstance(value[i], DeviceTypeConfigPropertyDefinition) == False:
+#                     raise TypeError(f"Each item in the list passed to the config_properties parameter must be an "
+#                                     f"instance of the DeviceTypeConfigPropertyDefinition class")
+#         self._config_properties = value
+#
```

### Comparing `switch_api-0.5.4b2/switch_api/pipeline/pipeline.py` & `switch_api-0.5.4b3/switch_api/pipeline/pipeline.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,776 +1,776 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""Module defining the Task abstract base class which is inherited by the following specific task types:
-
-- IntegrationTask
-- DiscoverableIntegrationTask
-- AnalyticsTask
-- LogicModuleTask
-- QueueTask
-- EventWorkOrderTask
-
-Also includes the Automation class which contains helper functions.
-
----------------
-IntegrationTask
----------------
-Base class used to create integrations between the Switch Automation Platform and other platforms, low-level services,
-or hardware.
-
-Examples include:
-    - Pulling readings or other types of data from REST APIs
-    - Protocol Translators which ingest data sent to the platform via email, ftp or direct upload within platform.
-
----------------------------
-DiscoverableIntegrationTask
----------------------------
-Base class used to create integrations between the Switch Automation Platform and 3rd party APIs.
-
-Similar to the IntegrationTask, but includes a secondary method `run_discovery()` which triggers discovery of available
-points on the 3rd party API and upserts these records to the Switch Platform backend so that the records are available
-in Build - Discovery & Selection UI.
-
-Examples include:
-    - Pulling readings or other types of data from REST APIs
-
--------------
-AnalyticsTask
--------------
-Base class used to create specific analytics functionality which may leverage existing data from the platform. Each
-task may add value to, or supplement, this data and write it back.
-
-Examples include:
-    - Anomaly Detection
-    - Leaky Pipes
-    - Peer Tracking
-
----------------
-LogicModuleTask
----------------
-Base class that handles the running, reprocessing and scheduling of the legacy logic modules in a way which enables
-integration with other platform functionality.
-
----------
-QueueTask
----------
-Base class used to create data pipelines that are fed via a queue.
-
-----------
-Automation
-----------
-This class contains the helper methods used to register, deploy, and test the created tasks. Additional helper functions
- for retrieving details of existing tasks on the Switch Automation Platform are also included in this module.
-
-"""
-import enum
-import sys
-# # import re
-# import pandas
-# import requests
-# import inspect
-import uuid
-import logging
-import datetime
-# import json
-from typing import List
-from abc import ABC, abstractmethod
-# from azure.servicebus import ServiceBusClient, ServiceBusReceiveMode  # , ServiceBusMessage
-# , _is_valid_regex, generate_password, _column_name_cap, DataFeedFileProcessOutput
-from .._utils._utils import ApiInputs, DiscoveryIntegrationInput
-from .._utils._constants import (MAPPING_ENTITIES  # , api_prefix, argus_prefix, EXPECTED_DELIVERY, DEPLOY_TYPE,
-                                 # QUEUE_NAME, ERROR_TYPE, SCHEDULE_TIMEZONE)
-                                 )
-from ..integration.helpers import get_templates, get_units_of_measure
-from .definitions import (IntegrationDeviceDefinition, EventWorkOrderFieldDefinition, AnalyticsSettings,  # BaseProperty
-                          IntegrationSettings)  # , DeviceTypeDefinition
-# from .._utils._platform import _get_ingestion_connection_string
-# from io import StringIO
-
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.DEBUG)
-consoleHandler = logging.StreamHandler(sys.stdout)
-consoleHandler.setLevel(logging.INFO)
-
-logger.handlers.clear()  # Prevent stacking of consoleHandlers
-logger.addHandler(consoleHandler)
-formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-                              datefmt='%Y-%m-%dT%H:%M:%S')
-consoleHandler.setFormatter(formatter)
-
-
-# Base Definitions in Switch package
-class Task(ABC):
-    """An Abstract Base Class called Task.
-
-    Attributes
-    ----------
-    id : uuid.UUID
-        Unique identifier of the task. This is an abstract property that needs to be overwritten when sub-classing.
-        A new unique identifier can be created using uuid.uuid4()
-    description : str
-        Brief description of the task
-    mapping_entities : List[MAPPING_ENTITIES]
-        The type of entities being mapped. An example is:
-
-        ``['Installations', 'Devices', 'Sensors']``
-    author : str
-        The author of the task.
-    version : str
-        The version of the task.
-
-    """
-
-    @property
-    @abstractmethod
-    def id(self) -> uuid.UUID:
-        """Unique identifier of the task. Create a new unique identifier using uuid.uuid4() """
-        pass
-
-    @property
-    @abstractmethod
-    def description(self) -> str:
-        """Brief description of the task"""
-        pass
-
-    @property
-    @abstractmethod
-    def mapping_entities(self) -> List[MAPPING_ENTITIES]:
-        """The type of entities being mapped."""
-        pass
-
-    @property
-    @abstractmethod
-    def author(self) -> str:
-        """"The author of the task."""
-        pass
-
-    @property
-    @abstractmethod
-    def version(self) -> str:
-        """The version of the task"""
-        pass
-
-
-class IntegrationTask(Task):
-    """Integration Task
-
-    This class is used to create integrations that post data to the Switch Automation Platform.
-
-
-    Only one of the following methods should be implemented per class, based on the type of integration required:
-
-    - process_file()
-    - process_stream()
-    - process()
-
-    """
-
-    @abstractmethod
-    def process_file(self, api_inputs: ApiInputs, file_path_to_process: str):
-        """Method to be implemented if a file will be processed by the Integration Task.
-
-        The method should contain all code used to cleanse, reformat & post the data contained in the file.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            object returned by call to initialize()
-        file_path_to_process : str
-            the file path
-
-        """
-        pass
-
-    @abstractmethod
-    def process_stream(self, api_inputs: ApiInputs, stream_to_process):
-        """Method to be implemented if data received via stream
-
-        The method should contain all code used to cleanse, reformat & post the data received via the stream.
-
-        Parameters
-        ----------
-        api_inputs: ApiInputs
-            The object returned by call to initialize()
-        stream_to_process
-            The details of the stream to be processed.
-        """
-        pass
-
-    # # TODO - before this version can be deployed, all tasks that subclass the "IntegrationTask" type need to have another
-    # #  method instanstiated on them "integration_settings_defintion" just pass if not using process() method, otherwise
-    # #  define the integration_settings as per analytics_settings_definition for AnalyticsTask type
-    # @property
-    # @abstractmethod
-    # def integration_settings_definition(self) -> List[IntegrationSettings]:
-    #     """Define the process() method's integration_settings dictionary requirements & defaults.
-    #
-    #     The definition of the dictionary keys, display labels in Task Insights UI, default value & allowed values
-    #     for the process() method's ``integration_settings`` input parameter.
-    #
-    #     property_name - the integration_settings dictionary key
-    #     display_label - the display label for the given property_name in Task Insights UI
-    #     editor - the editor shown in Task Insights UI
-    #     default_value - default value for this property_name (if applicable)
-    #     allowed_values - the set of allowed values (if applicable) for the given property_name. If editor=text_box, this
-    #     should be None.
-    #
-    #     """
-    #     pass
-    #
-    # def check_integration_settings_valid(self, integration_settings: dict):
-    #     required_integration_settings_keys= set()
-    #
-    #     for setting in self.integration_settings_definition:
-    #         required_integration_settings_keys.add(setting.property_name)
-    #
-    #     if not required_integration_settings_keys.issubset(set(integration_settings.keys())):
-    #         logger.error(f'The analytics_setting passed to the task do not contain the required keys: '
-    #                      f'{required_integration_settings_keys} ')
-    #         return False
-    #     else:
-    #         return True
-
-    @abstractmethod
-    def process(self, api_inputs: ApiInputs, integration_settings: dict):
-        """Method to be implemented if data
-
-        The method should contain all code used to cleanse, reformat & post the data pulled via the integration.
-
-        Parameters
-        ----------
-        api_inputs: ApiInputs
-            object returned by call to initialize()
-        integration_settings : dict
-            Any settings required to be passed to the integration to run. For example, username & password, api key,
-            auth token, etc.
-
-        Notes
-        -----
-        The method should first check the integration_settings passed to the task are valid. Pseudo code below:
-         >>> if self.check_integration_settings_valid(integration_settings=integration_settings) == True:
-         >>>    # Your actual task code here - i.e. proceed with the task if valid analytics_settings passed.
-         >>> else:
-         >>>    sw.pipeline.logger.error('Invalid integration_settings passed to driver. ')
-         >>>    sw.error_handlers.post_errors(api_inputs=api_inputs,
-         >>>                                  errors="Invalid integration_settings passed to the task. ",
-         >>>                                  error_type="InvalidInputSettings",
-         >>>                                  process_status="Failed")
-
-        """
-        pass
-
-
-class DiscoverableIntegrationTask(Task):
-    """Discoverable Integration Task
-
-    This class is used to create integrations that post data to the Switch Automation Platform from 3rd party APIs that
-    have discovery functionality.
-
-    The `process()` method should contain the code required to post data for the integration. The `run_discovery()`
-    method upserts records into the Integrations table so that end users can configure the points and import as
-    devices/sensors within the Build - Discovery and Selection UI in the Switch Automation Platform.
-
-    Additional properties are required to be created to support both the discovery functionality & the subsequent
-    device/sensor creation from the discovery records.
-
-    """
-
-    @property
-    @abstractmethod
-    def integration_device_type_definition(self) -> IntegrationDeviceDefinition:
-        """The IntegrationDeviceDefinition used to create the DriverDevices records required for the Integration
-        DeviceType to be available to drag & drop in the Build - Integration Schematic UI. Contains the properties that
-        define the minimum set of required fields to be passed to the integration_settings dictionaries for the
-        `process()` and `run_discovery()` methods"""
-        pass
-
-    # @property
-    # @abstractmethod
-    # def device_type_definitions(self) -> List[DeviceTypeDefinition]:
-    #     """List of DeviceTypeDefinition classes used to create the Device Types available for selection in the
-    #     Build - Discovery & Selection UI in the Switch Automation Platform. """
-    #     pass
-
-    def check_integration_settings_valid(self, integration_settings: dict):
-        required_integration_keys = set()
-
-        if self.integration_device_type_definition.expose_address == True:
-            required_integration_keys.add(self.integration_device_type_definition.address_label)
-
-        for setting in self.integration_device_type_definition.config_properties:
-            if setting.required_for_task == True:
-                required_integration_keys.add(setting.property_name)
-
-        if not required_integration_keys.issubset(set(integration_settings.keys())):
-            logger.error(f'The integration_settings passed to the task do not contain the required keys: '
-                         f'{required_integration_keys} ')
-            return False
-        else:
-            return True
-
-    @abstractmethod
-    def run_discovery(self, api_inputs: ApiInputs, integration_settings: dict,
-                      integration_input: DiscoveryIntegrationInput):
-        """Method to implement discovery of available points from 3rd party API.
-
-        The method should contain all code used to retrieve available points, reformat & post information to populate
-        the Build - Discovery & Selection UI in the platform and allows users to configure discovered points prior to
-        import.
-
-        Parameters
-        ----------
-        api_inputs: ApiInputs
-            object returned by call to initialize()
-        integration_settings : dict
-            Any settings required to be passed to the integration to run. For example, username & password, api key,
-            auth token, etc.
-        integration_input : DiscoveryIntegrationInput
-            The information required to be sent to the container when the `run_discovery` method is triggered by a user
-            from the UI. This information is the ApiProjectID, InstallationID, NetworkDeviceID and IntegrationDeviceID.
-
-        """
-        pass
-
-    @abstractmethod
-    def process(self, api_inputs: ApiInputs, integration_settings: dict):
-        """Method to be implemented if data
-
-        The method should contain all code used to cleanse, reformat & post the data pulled via the integration.
-
-        Parameters
-        ----------
-        api_inputs: ApiInputs
-            object returned by call to initialize()
-        integration_settings : dict
-            Any settings required to be passed to the integration to run. For example, username & password, api key,
-            auth token, etc.
-
-        """
-        pass
-
-
-class EventWorkOrderTask(Task):
-    """Event Work Order Task
-
-    This class is used to create work orders in 3rd party systems via tasks that are created in the Events UI of the
-    Switch Automation Platform.
-
-    """
-
-    @property
-    @abstractmethod
-    def work_order_fields_definition(self) -> List[EventWorkOrderFieldDefinition]:
-        """Define the fields available in Events UI when creating a work order in 3rd Party System.
-
-        The definition of the dictionary keys, display labels in Events UI, default value & allowed values
-        for the generate_work_order() method's ``work_order_input`` parameter.
-
-            property_name - the ``work_order_input`` dictionary key
-            display_label - the display label for the given property_name in Events UI Work Order creation screen
-            editor - the editor shown in Events UI Work Order creation screen
-            default_value - default value for this property_name (if applicable)
-            allowed_values - the set of allowed values (if applicable) for the given property_name. If editor=text_box,
-            this should be None.
-
-        """
-        pass
-
-    @property
-    @abstractmethod
-    def integration_settings_definition(self) -> List[IntegrationSettings]:
-        """Define the generate_work_order() method's integration_settings dictionary requirements & defaults.
-
-        The definition of the dictionary keys, display labels in Task Insights UI, default value & allowed values
-        for the generate_work_order() method's ``integration_settings`` input parameter.
-
-            property_name - the ``integration_settings`` dictionary key
-            display_label - the display label for the given property_name in Task Insights UI
-            editor - the editor shown in Task Insights UI
-            default_value - default value for this property_name (if applicable)
-            allowed_values - the set of allowed values (if applicable) for the given property_name. If editor=text_box,
-            this should be None.
-
-        """
-        pass
-
-    def check_work_order_input_valid(self, work_order_input: dict):
-        required_work_order_input_keys = set(['EventTaskId', 'Description', 'IntegrationId', 'DueDate', 'EventLink',
-                                              'EventSummary', 'InstallationId'])
-
-        for setting in self.work_order_fields_definition:
-            required_work_order_input_keys.add(setting.property_name)
-
-        if not required_work_order_input_keys.issubset(set(work_order_input.keys())):
-            logger.error(f'The work_order_input passed to the task do not contain the required keys: '
-                         f'{required_work_order_input_keys} ')
-            return False
-        else:
-            return True
-
-    def check_integration_settings_valid(self, integration_settings: dict):
-        required_integration_keys = set()
-
-        for setting in self.integration_settings_definition:
-            required_integration_keys.add(setting.property_name)
-
-        if not required_integration_keys.issubset(set(integration_settings.keys())):
-            logger.error(f'The integration_settings passed to the task do not contain the required keys: '
-                         f'{required_integration_keys} ')
-            return False
-        else:
-            return True
-
-    @abstractmethod
-    def generate_work_order(self, api_inputs: ApiInputs, integration_settings: dict, work_order_input: dict):
-        """Generate work order in 3rd party system via Events UI
-
-        Method to generate work order in 3rd party system based on a work order task created in Events UI in the Switch
-        Automation platform.
-
-        Notes
-        -----
-        In addition to the defined `work_order_fields_definition` fields, the `work_order_input` dictionary passed to
-        this method will contain the following keys:
-
-        - `EventTaskId`
-            - unique identifier (uuid.UUID) for the given record in the Switch Automation Platform
-        - `Description`
-            - free text field describing the work order to be generated.
-        - `IntegrationId`
-            - if linked to an existing work order in the 3rd party API, this will contain that system's identifier for
-            the workorder. If generating a net new workorder, this field will be null (None).
-        - `DueDate`
-            - The due date for the work order as set in the Switch Automation Platform.
-        - `EventLink`
-            - The URL link to the given Event in the Switch Automation Platform UI.
-        - `EventSummary`
-            - The Summary text associated with the given Event in the Switch Automation Platform UI.
-        - `InstallationId`
-            - The unique identifier of the site in the Switch Automation Platform that the work order is associated
-            with.
-
-        Parameters
-        ----------
-        api_inputs: ApiInputs
-            object returned by call to initialize()
-        integration_settings : dict
-            Any settings required to be passed to the integration to run. For example, username & password, api key,
-            auth token, etc.
-        work_order_input : dict
-            The work order defined by the task created in the Events UI of the Switch Automation Platform. To be sent
-            to 3rd party system for creation.
-        """
-
-        pass
-
-
-class QueueTask(Task):
-    """Queue Task
-
-    This class is used to create integrations that post data to the Switch Automation Platform using a Queue as the
-    data source.
-
-    Only one of the following methods should be implemented per class, based on the type of integration required:
-
-    - process_queue()
-
-    """
-
-    @property
-    @abstractmethod
-    def queue_name(self) -> str:
-        """The name of the queue to receive Data .. Name will actually be constructed as {ApiProjectId}_{queue_name} """
-        pass
-
-    @property
-    def queue_type(self) -> str:
-        """Type of the queue to receive data from"""
-        return 'DataIngestion'
-
-    @property
-    @abstractmethod
-    def maximum_message_count_per_call(self) -> int:
-        """ The maximum amount of messages which should be passed to the process_queue at any one time
-            set to zero to consume all
-        """
-        pass
-
-    @abstractmethod
-    def start(self, api_inputs: ApiInputs):
-        """Method to be implemented if a file will be processed by the QueueTask Task.
-        This will run once at the start of the processing and should contain
-
-        """
-        pass
-
-    @abstractmethod
-    def process_queue(self, api_inputs: ApiInputs, messages: List):
-        """Method to be implemented if a file will be processed by the QueueTask Task.
-
-        The method should contain all code used to consume the messages
-
-        Parameter
-        _________
-        api_inputs : ApiInputs
-            object returned by call to initialize()
-        messages:List)
-            list of serialized json strings which have been consumed from the queue
-
-        """
-        pass
-
-
-class AnalyticsTask(Task):
-
-    @property
-    @abstractmethod
-    def analytics_settings_definition(self) -> List[AnalyticsSettings]:
-        """Define the start() method's analytics_settings dictionary requirements & defaults.
-
-        The definition of the dictionary keys, display labels in Task Insights UI, default value & allowed values
-        for the start() method's ``analytics_settings`` input parameter.
-
-        property_name - the analytics_settings dictionary key
-        display_label - the display label for the given property_name in Task Insights UI
-        editor - the editor shown in Task Insights UI
-        default_value - default value for this property_name (if applicable)
-        allowed_values - the set of allowed values (if applicable) for the given property_name. If editor=text_box, this
-        should be None.
-
-        """
-        pass
-
-    def check_analytics_settings_valid(self, analytics_settings: dict):
-        # required_analytics_settings_keys = set(['task_id'])
-        required_analytics_settings_keys = set()
-
-        for setting in self.analytics_settings_definition:
-            required_analytics_settings_keys.add(setting.property_name)
-
-        if not required_analytics_settings_keys.issubset(set(analytics_settings.keys())):
-            logger.error(f'The analytics_setting passed to the task do not contain the required keys: '
-                         f'{required_analytics_settings_keys} ')
-            return False
-        else:
-            return True
-
-    @abstractmethod
-    def start(self, api_inputs: ApiInputs, analytics_settings: dict):
-        """Start.
-
-        The method should contain all code used by the task.
-
-        Notes
-        -----
-        The method should first check the analytics_settings passed to the task are valid. Pseudo code below:
-         >>> if self.check_analytics_settings_valid(analytics_settings=analytics_settings) == True:
-         >>>    # Your actual task code here - i.e. proceed with the task if valid analytics_settings passed.
-         >>> else:
-         >>>    sw.pipeline.logger.error('Invalid analytics_settings passed to driver. ')
-         >>>    sw.error_handlers.post_errors(api_inputs=api_inputs,
-         >>>                                  errors="Invalid analytics_settings passed to the task. ",
-         >>>                                  error_type="InvalidInputSettings",
-         >>>                                  process_status="Failed")
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            the object returned by call to initialize()
-        analytics_settings : dict
-            any setting required by the AnalyticsTask
-
-        """
-        pass
-
-
-class BlobTask(Task):
-    """Blob Task
-
-    This class is used to create integrations that post data to the Switch Automation Platform using a blob container &
-    Event Hub Queue as the source.
-
-    Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can be
-    registered or deployed.
-
-    """
-
-    @abstractmethod
-    def process_file(self, api_inputs: ApiInputs, file_path_to_process: str):
-        """The method should contain all code used to cleanse, reformat & post the data contained in the file.
-
-        Parameters
-        ----------
-        api_inputs : ApiInputs
-            object returned by call to initialize()
-        file_path_to_process : str
-            the file path
-
-        """
-        pass
-
-
-class LogicModuleTask(Task):
-
-    @abstractmethod
-    def start(self, start_date_time: datetime.date, end_date_time: datetime.date, installation_id: uuid.UUID,
-              share_tag_group: str, share_tags: list):
-        pass
-
-    # Needs to be implemented into the deploy_on_timer
-
-
-class RunAt(enum.Enum):
-    """ """
-    Every15Minutes = 1
-    Every1Hour = 3
-    EveryDay = 4
-    EveryWeek = 5
-
-
-class Guide(ABC):
-    """An Abstract Base Class called Guide.
-
-    To be used in concert with one of the Task sub-classes when deploying a guide to the marketplace. Syntax is like:
-    ``ExemplarGuide(DiscoverableIntegrationTask, Guide):
-    ``
-
-    Attributes
-    ----------
-    marketplace_name : str
-        Clean display name to be used for the guide within the Marketplace.
-    marketplace_version : str
-        The version of the task
-    short_description : str
-        Short description of the guide for the marketplace.
-    author : str
-        The author of the task.
-
-    Methods
-    -------
-    deploy(api_inputs, settings)
-        Method to call when Guides form reaches the final step in acquiring data from user.
-    """
-
-    @property
-    @abstractmethod
-    def marketplace_name(self) -> str:
-        """ Clean display name to be used for the guide within the Marketplace. """
-        pass
-
-    @property
-    @abstractmethod
-    def guide_version(self) -> str:
-        """Version number for the guide. """
-        pass
-
-    @property
-    @abstractmethod
-    def guide_short_description(self) -> str:
-        """Short description for the guide to be displayed in the Marketplace. Character limit of 250 applies. """
-        pass
-
-    def check_short_desc_length(self):
-        short_desc = self.guide_short_description
-        if type(short_desc) != str:
-            logger.exception(
-                "guide_short_description must be a string. Please update. ")
-            return "guide_short_description is not a string!"
-
-        if len(short_desc) > 250:
-            return len(short_desc)
-        else:
-            return True
-
-    @property
-    @abstractmethod
-    def guide_description(self) -> str:
-        """Full detailed description for the guide to be displayed in the Marketplace. No character limit applies. """
-        pass
-
-    @property
-    @abstractmethod
-    def card_image_file_name(self) -> str:
-        """Card Image URL for Marketplace Items."""
-        pass
-
-    @property
-    @abstractmethod
-    def image_file_name(self) -> str:
-        """For Marketplace item image."""
-        pass
-
-    @property
-    @abstractmethod
-    def folder_path_repo(self) -> str:
-        """Path to folder containing guide forms and images in the DevOps repo. """
-        pass
-
-    @property
-    @abstractmethod
-    def guide_tags(self) -> dict:
-        """Tags for Marketplace item registration. Required at least 1 tag.
-            A dictionary having the properties as the key and
-            the value being a list as the subcategory/ies that are applicable
-            Sample value: { "Connections": ["API"] }"""
-        pass
-
-    def minimum_tags_met(self):
-        if type(self.guide_tags) != dict:
-            logger.exception("guide_tags must be a dictionary. Please update. ")
-            return "guide_tags must be a dictionary!"
-
-        if len(self.guide_tags.keys()) >= 1:
-            return True
-        else:
-            return False
-
-    @abstractmethod
-    def deploy(self, api_inputs: ApiInputs, settings: any):
-        """Method to call when Guides form reaches the final step in acquiring data from user.
-
-        Args:
-            api_inputs: ApiInputs
-                The object returned by call to initialize()
-            settings (any):
-                Any settings required to be passed to the deploy of driver
-        """
-        pass
-
-
-# class GuideTask(Task):
-#
-#     def __init__(self) -> None:
-#         self.logger = logging.getLogger(__name__)
-#         self.logger.setLevel(logging.DEBUG)
-#         consoleHandler = logging.StreamHandler(sys.stdout)
-#         consoleHandler.setLevel(logging.INFO)
-#         self.logger.handlers.clear()
-#         self.logger.addHandler(consoleHandler)
-#         formatter = logging.Formatter('%(asctime)s  switch_guides.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-#                                       datefmt='%Y-%m-%dT%H:%M:%S')
-#         consoleHandler.setFormatter(formatter)
-#
-#     @abstractmethod
-#     def deploy(self, api_inputs: ApiInputs, settings: any):
-#         """Method to call when Guides form reaches the final step in acquiring data from user.
-#
-#         Args:
-#             api_inputs: ApiInputs
-#                 The object returned by call to initialize()
-#             settings (any):
-#                 Any settings required to be passed to the deploy of driver
-#         """
-#         pass
-#
-#     @abstractmethod
-#     def process(self, api_inputs: ApiInputs, settings: dict):
-#         """Method called when datafeed is ran by the engine.
-#
-#         Args:
-#             api_inputs (ApiInputs): _description_
-#             settings (dict): _description_
-#         """
-#         pass
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""Module defining the Task abstract base class which is inherited by the following specific task types:
+
+- IntegrationTask
+- DiscoverableIntegrationTask
+- AnalyticsTask
+- LogicModuleTask
+- QueueTask
+- EventWorkOrderTask
+
+Also includes the Automation class which contains helper functions.
+
+---------------
+IntegrationTask
+---------------
+Base class used to create integrations between the Switch Automation Platform and other platforms, low-level services,
+or hardware.
+
+Examples include:
+    - Pulling readings or other types of data from REST APIs
+    - Protocol Translators which ingest data sent to the platform via email, ftp or direct upload within platform.
+
+---------------------------
+DiscoverableIntegrationTask
+---------------------------
+Base class used to create integrations between the Switch Automation Platform and 3rd party APIs.
+
+Similar to the IntegrationTask, but includes a secondary method `run_discovery()` which triggers discovery of available
+points on the 3rd party API and upserts these records to the Switch Platform backend so that the records are available
+in Build - Discovery & Selection UI.
+
+Examples include:
+    - Pulling readings or other types of data from REST APIs
+
+-------------
+AnalyticsTask
+-------------
+Base class used to create specific analytics functionality which may leverage existing data from the platform. Each
+task may add value to, or supplement, this data and write it back.
+
+Examples include:
+    - Anomaly Detection
+    - Leaky Pipes
+    - Peer Tracking
+
+---------------
+LogicModuleTask
+---------------
+Base class that handles the running, reprocessing and scheduling of the legacy logic modules in a way which enables
+integration with other platform functionality.
+
+---------
+QueueTask
+---------
+Base class used to create data pipelines that are fed via a queue.
+
+----------
+Automation
+----------
+This class contains the helper methods used to register, deploy, and test the created tasks. Additional helper functions
+ for retrieving details of existing tasks on the Switch Automation Platform are also included in this module.
+
+"""
+import enum
+import sys
+# # import re
+# import pandas
+# import requests
+# import inspect
+import uuid
+import logging
+import datetime
+# import json
+from typing import List
+from abc import ABC, abstractmethod
+# from azure.servicebus import ServiceBusClient, ServiceBusReceiveMode  # , ServiceBusMessage
+# , _is_valid_regex, generate_password, _column_name_cap, DataFeedFileProcessOutput
+from .._utils._utils import ApiInputs, DiscoveryIntegrationInput
+from .._utils._constants import (MAPPING_ENTITIES  # , api_prefix, argus_prefix, EXPECTED_DELIVERY, DEPLOY_TYPE,
+                                 # QUEUE_NAME, ERROR_TYPE, SCHEDULE_TIMEZONE)
+                                 )
+from ..integration.helpers import get_templates, get_units_of_measure
+from .definitions import (IntegrationDeviceDefinition, EventWorkOrderFieldDefinition, AnalyticsSettings,  # BaseProperty
+                          IntegrationSettings)  # , DeviceTypeDefinition
+# from .._utils._platform import _get_ingestion_connection_string
+# from io import StringIO
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
+consoleHandler = logging.StreamHandler(sys.stdout)
+consoleHandler.setLevel(logging.INFO)
+
+logger.handlers.clear()  # Prevent stacking of consoleHandlers
+logger.addHandler(consoleHandler)
+formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+                              datefmt='%Y-%m-%dT%H:%M:%S')
+consoleHandler.setFormatter(formatter)
+
+
+# Base Definitions in Switch package
+class Task(ABC):
+    """An Abstract Base Class called Task.
+
+    Attributes
+    ----------
+    id : uuid.UUID
+        Unique identifier of the task. This is an abstract property that needs to be overwritten when sub-classing.
+        A new unique identifier can be created using uuid.uuid4()
+    description : str
+        Brief description of the task
+    mapping_entities : List[MAPPING_ENTITIES]
+        The type of entities being mapped. An example is:
+
+        ``['Installations', 'Devices', 'Sensors']``
+    author : str
+        The author of the task.
+    version : str
+        The version of the task.
+
+    """
+
+    @property
+    @abstractmethod
+    def id(self) -> uuid.UUID:
+        """Unique identifier of the task. Create a new unique identifier using uuid.uuid4() """
+        pass
+
+    @property
+    @abstractmethod
+    def description(self) -> str:
+        """Brief description of the task"""
+        pass
+
+    @property
+    @abstractmethod
+    def mapping_entities(self) -> List[MAPPING_ENTITIES]:
+        """The type of entities being mapped."""
+        pass
+
+    @property
+    @abstractmethod
+    def author(self) -> str:
+        """"The author of the task."""
+        pass
+
+    @property
+    @abstractmethod
+    def version(self) -> str:
+        """The version of the task"""
+        pass
+
+
+class IntegrationTask(Task):
+    """Integration Task
+
+    This class is used to create integrations that post data to the Switch Automation Platform.
+
+
+    Only one of the following methods should be implemented per class, based on the type of integration required:
+
+    - process_file()
+    - process_stream()
+    - process()
+
+    """
+
+    @abstractmethod
+    def process_file(self, api_inputs: ApiInputs, file_path_to_process: str):
+        """Method to be implemented if a file will be processed by the Integration Task.
+
+        The method should contain all code used to cleanse, reformat & post the data contained in the file.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            object returned by call to initialize()
+        file_path_to_process : str
+            the file path
+
+        """
+        pass
+
+    @abstractmethod
+    def process_stream(self, api_inputs: ApiInputs, stream_to_process):
+        """Method to be implemented if data received via stream
+
+        The method should contain all code used to cleanse, reformat & post the data received via the stream.
+
+        Parameters
+        ----------
+        api_inputs: ApiInputs
+            The object returned by call to initialize()
+        stream_to_process
+            The details of the stream to be processed.
+        """
+        pass
+
+    # # TODO - before this version can be deployed, all tasks that subclass the "IntegrationTask" type need to have another
+    # #  method instanstiated on them "integration_settings_defintion" just pass if not using process() method, otherwise
+    # #  define the integration_settings as per analytics_settings_definition for AnalyticsTask type
+    # @property
+    # @abstractmethod
+    # def integration_settings_definition(self) -> List[IntegrationSettings]:
+    #     """Define the process() method's integration_settings dictionary requirements & defaults.
+    #
+    #     The definition of the dictionary keys, display labels in Task Insights UI, default value & allowed values
+    #     for the process() method's ``integration_settings`` input parameter.
+    #
+    #     property_name - the integration_settings dictionary key
+    #     display_label - the display label for the given property_name in Task Insights UI
+    #     editor - the editor shown in Task Insights UI
+    #     default_value - default value for this property_name (if applicable)
+    #     allowed_values - the set of allowed values (if applicable) for the given property_name. If editor=text_box, this
+    #     should be None.
+    #
+    #     """
+    #     pass
+    #
+    # def check_integration_settings_valid(self, integration_settings: dict):
+    #     required_integration_settings_keys= set()
+    #
+    #     for setting in self.integration_settings_definition:
+    #         required_integration_settings_keys.add(setting.property_name)
+    #
+    #     if not required_integration_settings_keys.issubset(set(integration_settings.keys())):
+    #         logger.error(f'The analytics_setting passed to the task do not contain the required keys: '
+    #                      f'{required_integration_settings_keys} ')
+    #         return False
+    #     else:
+    #         return True
+
+    @abstractmethod
+    def process(self, api_inputs: ApiInputs, integration_settings: dict):
+        """Method to be implemented if data
+
+        The method should contain all code used to cleanse, reformat & post the data pulled via the integration.
+
+        Parameters
+        ----------
+        api_inputs: ApiInputs
+            object returned by call to initialize()
+        integration_settings : dict
+            Any settings required to be passed to the integration to run. For example, username & password, api key,
+            auth token, etc.
+
+        Notes
+        -----
+        The method should first check the integration_settings passed to the task are valid. Pseudo code below:
+         >>> if self.check_integration_settings_valid(integration_settings=integration_settings) == True:
+         >>>    # Your actual task code here - i.e. proceed with the task if valid analytics_settings passed.
+         >>> else:
+         >>>    sw.pipeline.logger.error('Invalid integration_settings passed to driver. ')
+         >>>    sw.error_handlers.post_errors(api_inputs=api_inputs,
+         >>>                                  errors="Invalid integration_settings passed to the task. ",
+         >>>                                  error_type="InvalidInputSettings",
+         >>>                                  process_status="Failed")
+
+        """
+        pass
+
+
+class DiscoverableIntegrationTask(Task):
+    """Discoverable Integration Task
+
+    This class is used to create integrations that post data to the Switch Automation Platform from 3rd party APIs that
+    have discovery functionality.
+
+    The `process()` method should contain the code required to post data for the integration. The `run_discovery()`
+    method upserts records into the Integrations table so that end users can configure the points and import as
+    devices/sensors within the Build - Discovery and Selection UI in the Switch Automation Platform.
+
+    Additional properties are required to be created to support both the discovery functionality & the subsequent
+    device/sensor creation from the discovery records.
+
+    """
+
+    @property
+    @abstractmethod
+    def integration_device_type_definition(self) -> IntegrationDeviceDefinition:
+        """The IntegrationDeviceDefinition used to create the DriverDevices records required for the Integration
+        DeviceType to be available to drag & drop in the Build - Integration Schematic UI. Contains the properties that
+        define the minimum set of required fields to be passed to the integration_settings dictionaries for the
+        `process()` and `run_discovery()` methods"""
+        pass
+
+    # @property
+    # @abstractmethod
+    # def device_type_definitions(self) -> List[DeviceTypeDefinition]:
+    #     """List of DeviceTypeDefinition classes used to create the Device Types available for selection in the
+    #     Build - Discovery & Selection UI in the Switch Automation Platform. """
+    #     pass
+
+    def check_integration_settings_valid(self, integration_settings: dict):
+        required_integration_keys = set()
+
+        if self.integration_device_type_definition.expose_address == True:
+            required_integration_keys.add(self.integration_device_type_definition.address_label)
+
+        for setting in self.integration_device_type_definition.config_properties:
+            if setting.required_for_task == True:
+                required_integration_keys.add(setting.property_name)
+
+        if not required_integration_keys.issubset(set(integration_settings.keys())):
+            logger.error(f'The integration_settings passed to the task do not contain the required keys: '
+                         f'{required_integration_keys} ')
+            return False
+        else:
+            return True
+
+    @abstractmethod
+    def run_discovery(self, api_inputs: ApiInputs, integration_settings: dict,
+                      integration_input: DiscoveryIntegrationInput):
+        """Method to implement discovery of available points from 3rd party API.
+
+        The method should contain all code used to retrieve available points, reformat & post information to populate
+        the Build - Discovery & Selection UI in the platform and allows users to configure discovered points prior to
+        import.
+
+        Parameters
+        ----------
+        api_inputs: ApiInputs
+            object returned by call to initialize()
+        integration_settings : dict
+            Any settings required to be passed to the integration to run. For example, username & password, api key,
+            auth token, etc.
+        integration_input : DiscoveryIntegrationInput
+            The information required to be sent to the container when the `run_discovery` method is triggered by a user
+            from the UI. This information is the ApiProjectID, InstallationID, NetworkDeviceID and IntegrationDeviceID.
+
+        """
+        pass
+
+    @abstractmethod
+    def process(self, api_inputs: ApiInputs, integration_settings: dict):
+        """Method to be implemented if data
+
+        The method should contain all code used to cleanse, reformat & post the data pulled via the integration.
+
+        Parameters
+        ----------
+        api_inputs: ApiInputs
+            object returned by call to initialize()
+        integration_settings : dict
+            Any settings required to be passed to the integration to run. For example, username & password, api key,
+            auth token, etc.
+
+        """
+        pass
+
+
+class EventWorkOrderTask(Task):
+    """Event Work Order Task
+
+    This class is used to create work orders in 3rd party systems via tasks that are created in the Events UI of the
+    Switch Automation Platform.
+
+    """
+
+    @property
+    @abstractmethod
+    def work_order_fields_definition(self) -> List[EventWorkOrderFieldDefinition]:
+        """Define the fields available in Events UI when creating a work order in 3rd Party System.
+
+        The definition of the dictionary keys, display labels in Events UI, default value & allowed values
+        for the generate_work_order() method's ``work_order_input`` parameter.
+
+            property_name - the ``work_order_input`` dictionary key
+            display_label - the display label for the given property_name in Events UI Work Order creation screen
+            editor - the editor shown in Events UI Work Order creation screen
+            default_value - default value for this property_name (if applicable)
+            allowed_values - the set of allowed values (if applicable) for the given property_name. If editor=text_box,
+            this should be None.
+
+        """
+        pass
+
+    @property
+    @abstractmethod
+    def integration_settings_definition(self) -> List[IntegrationSettings]:
+        """Define the generate_work_order() method's integration_settings dictionary requirements & defaults.
+
+        The definition of the dictionary keys, display labels in Task Insights UI, default value & allowed values
+        for the generate_work_order() method's ``integration_settings`` input parameter.
+
+            property_name - the ``integration_settings`` dictionary key
+            display_label - the display label for the given property_name in Task Insights UI
+            editor - the editor shown in Task Insights UI
+            default_value - default value for this property_name (if applicable)
+            allowed_values - the set of allowed values (if applicable) for the given property_name. If editor=text_box,
+            this should be None.
+
+        """
+        pass
+
+    def check_work_order_input_valid(self, work_order_input: dict):
+        required_work_order_input_keys = set(['EventTaskId', 'Description', 'IntegrationId', 'DueDate', 'EventLink',
+                                              'EventSummary', 'InstallationId'])
+
+        for setting in self.work_order_fields_definition:
+            required_work_order_input_keys.add(setting.property_name)
+
+        if not required_work_order_input_keys.issubset(set(work_order_input.keys())):
+            logger.error(f'The work_order_input passed to the task do not contain the required keys: '
+                         f'{required_work_order_input_keys} ')
+            return False
+        else:
+            return True
+
+    def check_integration_settings_valid(self, integration_settings: dict):
+        required_integration_keys = set()
+
+        for setting in self.integration_settings_definition:
+            required_integration_keys.add(setting.property_name)
+
+        if not required_integration_keys.issubset(set(integration_settings.keys())):
+            logger.error(f'The integration_settings passed to the task do not contain the required keys: '
+                         f'{required_integration_keys} ')
+            return False
+        else:
+            return True
+
+    @abstractmethod
+    def generate_work_order(self, api_inputs: ApiInputs, integration_settings: dict, work_order_input: dict):
+        """Generate work order in 3rd party system via Events UI
+
+        Method to generate work order in 3rd party system based on a work order task created in Events UI in the Switch
+        Automation platform.
+
+        Notes
+        -----
+        In addition to the defined `work_order_fields_definition` fields, the `work_order_input` dictionary passed to
+        this method will contain the following keys:
+
+        - `EventTaskId`
+            - unique identifier (uuid.UUID) for the given record in the Switch Automation Platform
+        - `Description`
+            - free text field describing the work order to be generated.
+        - `IntegrationId`
+            - if linked to an existing work order in the 3rd party API, this will contain that system's identifier for
+            the workorder. If generating a net new workorder, this field will be null (None).
+        - `DueDate`
+            - The due date for the work order as set in the Switch Automation Platform.
+        - `EventLink`
+            - The URL link to the given Event in the Switch Automation Platform UI.
+        - `EventSummary`
+            - The Summary text associated with the given Event in the Switch Automation Platform UI.
+        - `InstallationId`
+            - The unique identifier of the site in the Switch Automation Platform that the work order is associated
+            with.
+
+        Parameters
+        ----------
+        api_inputs: ApiInputs
+            object returned by call to initialize()
+        integration_settings : dict
+            Any settings required to be passed to the integration to run. For example, username & password, api key,
+            auth token, etc.
+        work_order_input : dict
+            The work order defined by the task created in the Events UI of the Switch Automation Platform. To be sent
+            to 3rd party system for creation.
+        """
+
+        pass
+
+
+class QueueTask(Task):
+    """Queue Task
+
+    This class is used to create integrations that post data to the Switch Automation Platform using a Queue as the
+    data source.
+
+    Only one of the following methods should be implemented per class, based on the type of integration required:
+
+    - process_queue()
+
+    """
+
+    @property
+    @abstractmethod
+    def queue_name(self) -> str:
+        """The name of the queue to receive Data .. Name will actually be constructed as {ApiProjectId}_{queue_name} """
+        pass
+
+    @property
+    def queue_type(self) -> str:
+        """Type of the queue to receive data from"""
+        return 'DataIngestion'
+
+    @property
+    @abstractmethod
+    def maximum_message_count_per_call(self) -> int:
+        """ The maximum amount of messages which should be passed to the process_queue at any one time
+            set to zero to consume all
+        """
+        pass
+
+    @abstractmethod
+    def start(self, api_inputs: ApiInputs):
+        """Method to be implemented if a file will be processed by the QueueTask Task.
+        This will run once at the start of the processing and should contain
+
+        """
+        pass
+
+    @abstractmethod
+    def process_queue(self, api_inputs: ApiInputs, messages: List):
+        """Method to be implemented if a file will be processed by the QueueTask Task.
+
+        The method should contain all code used to consume the messages
+
+        Parameter
+        _________
+        api_inputs : ApiInputs
+            object returned by call to initialize()
+        messages:List)
+            list of serialized json strings which have been consumed from the queue
+
+        """
+        pass
+
+
+class AnalyticsTask(Task):
+
+    @property
+    @abstractmethod
+    def analytics_settings_definition(self) -> List[AnalyticsSettings]:
+        """Define the start() method's analytics_settings dictionary requirements & defaults.
+
+        The definition of the dictionary keys, display labels in Task Insights UI, default value & allowed values
+        for the start() method's ``analytics_settings`` input parameter.
+
+        property_name - the analytics_settings dictionary key
+        display_label - the display label for the given property_name in Task Insights UI
+        editor - the editor shown in Task Insights UI
+        default_value - default value for this property_name (if applicable)
+        allowed_values - the set of allowed values (if applicable) for the given property_name. If editor=text_box, this
+        should be None.
+
+        """
+        pass
+
+    def check_analytics_settings_valid(self, analytics_settings: dict):
+        # required_analytics_settings_keys = set(['task_id'])
+        required_analytics_settings_keys = set()
+
+        for setting in self.analytics_settings_definition:
+            required_analytics_settings_keys.add(setting.property_name)
+
+        if not required_analytics_settings_keys.issubset(set(analytics_settings.keys())):
+            logger.error(f'The analytics_setting passed to the task do not contain the required keys: '
+                         f'{required_analytics_settings_keys} ')
+            return False
+        else:
+            return True
+
+    @abstractmethod
+    def start(self, api_inputs: ApiInputs, analytics_settings: dict):
+        """Start.
+
+        The method should contain all code used by the task.
+
+        Notes
+        -----
+        The method should first check the analytics_settings passed to the task are valid. Pseudo code below:
+         >>> if self.check_analytics_settings_valid(analytics_settings=analytics_settings) == True:
+         >>>    # Your actual task code here - i.e. proceed with the task if valid analytics_settings passed.
+         >>> else:
+         >>>    sw.pipeline.logger.error('Invalid analytics_settings passed to driver. ')
+         >>>    sw.error_handlers.post_errors(api_inputs=api_inputs,
+         >>>                                  errors="Invalid analytics_settings passed to the task. ",
+         >>>                                  error_type="InvalidInputSettings",
+         >>>                                  process_status="Failed")
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            the object returned by call to initialize()
+        analytics_settings : dict
+            any setting required by the AnalyticsTask
+
+        """
+        pass
+
+
+class BlobTask(Task):
+    """Blob Task
+
+    This class is used to create integrations that post data to the Switch Automation Platform using a blob container &
+    Event Hub Queue as the source.
+
+    Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can be
+    registered or deployed.
+
+    """
+
+    @abstractmethod
+    def process_file(self, api_inputs: ApiInputs, file_path_to_process: str):
+        """The method should contain all code used to cleanse, reformat & post the data contained in the file.
+
+        Parameters
+        ----------
+        api_inputs : ApiInputs
+            object returned by call to initialize()
+        file_path_to_process : str
+            the file path
+
+        """
+        pass
+
+
+class LogicModuleTask(Task):
+
+    @abstractmethod
+    def start(self, start_date_time: datetime.date, end_date_time: datetime.date, installation_id: uuid.UUID,
+              share_tag_group: str, share_tags: list):
+        pass
+
+    # Needs to be implemented into the deploy_on_timer
+
+
+class RunAt(enum.Enum):
+    """ """
+    Every15Minutes = 1
+    Every1Hour = 3
+    EveryDay = 4
+    EveryWeek = 5
+
+
+class Guide(ABC):
+    """An Abstract Base Class called Guide.
+
+    To be used in concert with one of the Task sub-classes when deploying a guide to the marketplace. Syntax is like:
+    ``ExemplarGuide(DiscoverableIntegrationTask, Guide):
+    ``
+
+    Attributes
+    ----------
+    marketplace_name : str
+        Clean display name to be used for the guide within the Marketplace.
+    marketplace_version : str
+        The version of the task
+    short_description : str
+        Short description of the guide for the marketplace.
+    author : str
+        The author of the task.
+
+    Methods
+    -------
+    deploy(api_inputs, settings)
+        Method to call when Guides form reaches the final step in acquiring data from user.
+    """
+
+    @property
+    @abstractmethod
+    def marketplace_name(self) -> str:
+        """ Clean display name to be used for the guide within the Marketplace. """
+        pass
+
+    @property
+    @abstractmethod
+    def guide_version(self) -> str:
+        """Version number for the guide. """
+        pass
+
+    @property
+    @abstractmethod
+    def guide_short_description(self) -> str:
+        """Short description for the guide to be displayed in the Marketplace. Character limit of 250 applies. """
+        pass
+
+    def check_short_desc_length(self):
+        short_desc = self.guide_short_description
+        if type(short_desc) != str:
+            logger.exception(
+                "guide_short_description must be a string. Please update. ")
+            return "guide_short_description is not a string!"
+
+        if len(short_desc) > 250:
+            return len(short_desc)
+        else:
+            return True
+
+    @property
+    @abstractmethod
+    def guide_description(self) -> str:
+        """Full detailed description for the guide to be displayed in the Marketplace. No character limit applies. """
+        pass
+
+    @property
+    @abstractmethod
+    def card_image_file_name(self) -> str:
+        """Card Image URL for Marketplace Items."""
+        pass
+
+    @property
+    @abstractmethod
+    def image_file_name(self) -> str:
+        """For Marketplace item image."""
+        pass
+
+    @property
+    @abstractmethod
+    def folder_path_repo(self) -> str:
+        """Path to folder containing guide forms and images in the DevOps repo. """
+        pass
+
+    @property
+    @abstractmethod
+    def guide_tags(self) -> dict:
+        """Tags for Marketplace item registration. Required at least 1 tag.
+            A dictionary having the properties as the key and
+            the value being a list as the subcategory/ies that are applicable
+            Sample value: { "Connections": ["API"] }"""
+        pass
+
+    def minimum_tags_met(self):
+        if type(self.guide_tags) != dict:
+            logger.exception("guide_tags must be a dictionary. Please update. ")
+            return "guide_tags must be a dictionary!"
+
+        if len(self.guide_tags.keys()) >= 1:
+            return True
+        else:
+            return False
+
+    @abstractmethod
+    def deploy(self, api_inputs: ApiInputs, settings: any):
+        """Method to call when Guides form reaches the final step in acquiring data from user.
+
+        Args:
+            api_inputs: ApiInputs
+                The object returned by call to initialize()
+            settings (any):
+                Any settings required to be passed to the deploy of driver
+        """
+        pass
+
+
+# class GuideTask(Task):
+#
+#     def __init__(self) -> None:
+#         self.logger = logging.getLogger(__name__)
+#         self.logger.setLevel(logging.DEBUG)
+#         consoleHandler = logging.StreamHandler(sys.stdout)
+#         consoleHandler.setLevel(logging.INFO)
+#         self.logger.handlers.clear()
+#         self.logger.addHandler(consoleHandler)
+#         formatter = logging.Formatter('%(asctime)s  switch_guides.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+#                                       datefmt='%Y-%m-%dT%H:%M:%S')
+#         consoleHandler.setFormatter(formatter)
+#
+#     @abstractmethod
+#     def deploy(self, api_inputs: ApiInputs, settings: any):
+#         """Method to call when Guides form reaches the final step in acquiring data from user.
+#
+#         Args:
+#             api_inputs: ApiInputs
+#                 The object returned by call to initialize()
+#             settings (any):
+#                 Any settings required to be passed to the deploy of driver
+#         """
+#         pass
+#
+#     @abstractmethod
+#     def process(self, api_inputs: ApiInputs, settings: dict):
+#         """Method called when datafeed is ran by the engine.
+#
+#         Args:
+#             api_inputs (ApiInputs): _description_
+#             settings (dict): _description_
+#         """
+#         pass
```

### Comparing `switch_api-0.5.4b2/switch_api/platform_insights/__init__.py` & `switch_api-0.5.4b3/switch_api/platform_insights/__init__.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,12 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for interacting with the insights platform services, etc of the Switch Automation Platform.
-"""
-
-# from .platform_insights import get_current_insights_by_equipment
-
-# __all__ = ['get_current_insights_by_equipment']
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for interacting with the insights platform services, etc of the Switch Automation Platform.
+"""
+
+# from .platform_insights import get_current_insights_by_equipment
+
+# __all__ = ['get_current_insights_by_equipment']
```

### Comparing `switch_api-0.5.4b2/switch_api/platform_insights/platform_insights.py` & `switch_api-0.5.4b3/switch_api/platform_insights/platform_insights.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-# -------------------------------------------------------------------------
-# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
-# Licensed under the MIT License. See License.txt in the project root for
-# license information.
-# --------------------------------------------------------------------------
-"""
-A module for interacting with the insights platform services, etc of the Switch Automation Platform.
-"""
-# import sys
-# import pandas
-# import logging
-# from .._utils._platform import Blob
-# from .._utils._utils import ApiInputs
-# from io import StringIO
-#
-# logger = logging.getLogger(__name__)
-# logger.setLevel(logging.DEBUG)
-# consoleHandler = logging.StreamHandler(stream=sys.stdout)
-# consoleHandler.setLevel(logging.INFO)
-#
-# logger.addHandler(consoleHandler)
-# formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
-#                               datefmt='%Y-%m-%dT%H:%M:%S')
-# consoleHandler.setFormatter(formatter)
-#
-#
-# def get_current_insights_by_equipment(api_inputs: ApiInputs):
-#     """Get current insights by equipment.
-#
-#     Parameters
-#     ----------
-#     api_inputs : ApiInputs
-#         Object returned by initialize() function.
-#
-#     Returns
-#     -------
-#     df : pandas.DataFrame
-#
-#
-#     """
-#     # payload = {}
-#     headers = {
-#         'x-functions-key': api_inputs.api_key,
-#         'Content-Type': 'application/json; charset=utf-8',
-#         'user-key': api_inputs.user_id
-#     }
-#
-#     if api_inputs.datacentre == '' or api_inputs.api_key == '':
-#         logger.error("You must call initialize() before using API.")
-#         return pandas.DataFrame()
-#
-#     # Get Live Insights for specific Portfolio
-#     path = f'live-insights/{str(api_inputs.api_project_id)}.csv'
-#     live_insights_bytes = Blob.download(api_inputs=api_inputs, account='SwitchStorage', container='data-ingestion-adx',
-#                                         blob_name=path)
-#     if len(live_insights_bytes) == 0:
-#         logger.error(f'No data returned for this API call. File: {path}')
-#         return pandas.DataFrame()
-#
-#     df = pandas.read_csv(StringIO(str(live_insights_bytes, 'utf-8')))
-#
-#     return df
+# -------------------------------------------------------------------------
+# Copyright (c) Switch Automation Pty Ltd. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+"""
+A module for interacting with the insights platform services, etc of the Switch Automation Platform.
+"""
+# import sys
+# import pandas
+# import logging
+# from .._utils._platform import Blob
+# from .._utils._utils import ApiInputs
+# from io import StringIO
+#
+# logger = logging.getLogger(__name__)
+# logger.setLevel(logging.DEBUG)
+# consoleHandler = logging.StreamHandler(stream=sys.stdout)
+# consoleHandler.setLevel(logging.INFO)
+#
+# logger.addHandler(consoleHandler)
+# formatter = logging.Formatter('%(asctime)s  switch_api.%(module)s.%(funcName)s  %(levelname)s: %(message)s',
+#                               datefmt='%Y-%m-%dT%H:%M:%S')
+# consoleHandler.setFormatter(formatter)
+#
+#
+# def get_current_insights_by_equipment(api_inputs: ApiInputs):
+#     """Get current insights by equipment.
+#
+#     Parameters
+#     ----------
+#     api_inputs : ApiInputs
+#         Object returned by initialize() function.
+#
+#     Returns
+#     -------
+#     df : pandas.DataFrame
+#
+#
+#     """
+#     # payload = {}
+#     headers = {
+#         'x-functions-key': api_inputs.api_key,
+#         'Content-Type': 'application/json; charset=utf-8',
+#         'user-key': api_inputs.user_id
+#     }
+#
+#     if api_inputs.datacentre == '' or api_inputs.api_key == '':
+#         logger.error("You must call initialize() before using API.")
+#         return pandas.DataFrame()
+#
+#     # Get Live Insights for specific Portfolio
+#     path = f'live-insights/{str(api_inputs.api_project_id)}.csv'
+#     live_insights_bytes = Blob.download(api_inputs=api_inputs, account='SwitchStorage', container='data-ingestion-adx',
+#                                         blob_name=path)
+#     if len(live_insights_bytes) == 0:
+#         logger.error(f'No data returned for this API call. File: {path}')
+#         return pandas.DataFrame()
+#
+#     df = pandas.read_csv(StringIO(str(live_insights_bytes, 'utf-8')))
+#
+#     return df
```

### Comparing `switch_api-0.5.4b2/switch_api.egg-info/PKG-INFO` & `switch_api-0.5.4b3/switch_api.egg-info/PKG-INFO`

 * *Files 16% similar despite different names*

```diff
@@ -1,979 +1,962 @@
-Metadata-Version: 2.1
-Name: switch-api
-Version: 0.5.4b2
-Summary: A complete package for data ingestion into the Switch Automation Platform.
-Home-page: UNKNOWN
-Author: Switch Automation Pty Ltd.
-License: MIT License
-Platform: UNKNOWN
-Classifier: Development Status :: 2 - Pre-Alpha
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Intended Audience :: Other Audience
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Natural Language :: English
-Requires-Python: >=3.8.0
-Description-Content-Type: text/markdown
-License-File: LICENCE
-License-File: AUTHORS.rst
-
-# Switch Automation library for Python
-This is a package for data ingestion into the Switch Automation software platform. 
-
-You can find out more about the platform on [Switch Automation](https://www.switchautomation.com)
-
-## Getting started
-
-### Prerequisites
-* Python 3.8 or later is required to use this package. 
-* You must have a [Switch Automation user account](https://www.switchautomation.com/our-solution/) to use this package. 
-
-### Install the package
-Install the Switch Automation library for Python with [pip](https://pypi.org/project/pip/):
-
-```bash
-pip install switch_api
-```
-
-# History
-
-## 0.5.4-b2
-
-### Added
-
-In the `integration` module:
-- Added new function `upsert_reservations()`
-  - Upserts data to the ReservationHistory table
-  - Two attributes added to assist with creation of the input dataframe:
-    - `upsert_reservations.df_required_columns` - returns list of required columns for the input `df`
-    - `upsert_reservations.df_optional_columns` - returns list of required columns for the input `df`
-  - The following datetime fields are required and must use the ``local_date_time_cols`` and ``utc_date_time_cols``
-    parameters to define whether their values are in site-local timezone or UTC timezone:
-    - ``CreatedDate``
-    - ``LastModifiedDate``
-    - ``ReservationStart``
-    - ``ReservationEnd``
-
-## 0.5.3
-
-### Added
-
-- In the `integration` module:
-  - Added `override_existing` parameter in `upsert_discovered_records`
-    - Flag if it the values passed to df will override existing integration records. Only valid if running locally,
-      not on a deployed task where it is triggered via UI.
-    - Defaults to False
-
-## 0.5
-
-### Added
-
-- In the `pipeline` module:
-  - Added a new task type called `Guide`.
-    - this task type should be sub-classed in concert with one of the Task sub-classes when deploying a guide to the
-      marketplace.
-  - Added a new method to the `Automation` class called `register_guide_task()`
-    - this method is used to register tasks that sub-class the `Guide` task and also posts form files to blob and
-      registers the guide to the Marketplace.
-- New `_guide` module - only to be referenced when doing initial development of a Guide
-  - `guide`'s `local_start' method
-    - Allows to run mock guides engine locally that ables to debug `Guide` task types with Form Kit playground.
-
-### Fixed
-
-- In `controls` module:
-  - modify `submit_control` method parameters - typings
-  - remove extra columns from payload to IoT API requests
-
-## 0.4.9
-
-### Added
-
-- New method added in `automation` module:
-  - `run_data_feed()` - Run python job based on data feed id. This will be sent to the queue for processing and will
-    undergo same procedure as the rest of the datafeed.
-    - Required parameters are `api_inputs` and `data_feed_id`
-    - This has a restriction of only allowing an AnalyticsTask type datafeed to be run and deployed as a Timer
-- New method added in `analytics` module:
-  - `upsert_performance_statistics` - this method should only be used by tasks used to populate the Portfolio
-    Benchmarking feature in the Switch Automation platform
-- New `controls` module added and new method added to this module:
-  - `submit_control()` - method to submit control of sensors
-    - this method returns a tuple: `(control_response, missing_response)`:
-      - `control_response` - is the list of sensors that are acknowledged and process by the MQTTT message broker
-      - `missing_response` = is the list of sensors that are sensors that were caught by the connection `time_out` -
-        default to 30 secs - meaning the response were no longer waited to be received by the python package.
-        Increasing the time out can potentially help with this.
-
-### Fixed
-
-- In the `integration` module, minor fixes to:
-  - An unhandled exception when using `pandas==2.1.1` on the following functions:
-    - `upsert_sites()`
-    - `upsert_device_sensors()`
-    - `upsert_device_sensors_ext()`
-    - `upsert_workorders()`
-    - `upsert_timeseries_ds()`
-    - `upsert_timeseries()`
-  - Handle deprecation of `pandas.DataFrame.append()` on the following functions:
-    - `upsert_device_sensors()`
-    - `upsert_device_sensors_ext()`
-  - An unhandled exception for `connect_to_sql()` function when the internal API call within
-    `_get_sql_connection_string()` fails.
-
-## 0.4.8
-
-### Added
-
-- New class added to the `pipeline` module:
-  - `BlobTask` - This class is used to create integrations that post data to the Switch Automation Platform using a
-    blob container & Event Hub Queue as the source.
-    - Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can be
-      registered or deployed.
-    - requires `process_file()` abstract method to be created when sub-classing
-- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
-  - this new method is only applicable for tasks that subclass the `BlobTask` base class.
-- In the `integration` module, new helper methods have been added:
-  - `connect_to_sql()` method creates a pyodbc connection object to enable easier querying of the SQL database via the
-    `pyodbc` library
-  - `amortise_across_days()` method enables easier amortisation of data across days in a period, either inclusive or
-    exclusive of end date.
-  - `get_metadata_where_clause()` method enables creation of `sql_where_clause` for the `get_device_sensors`() method
-    where for each metadata key the sql checks its not null.
-- In the `error_handlers` module:
-  - `check_duplicates()` method added to check for duplicates & post appropriate errors to Task Insights UI in the
-    Switch Automation platform.
-- In the `_utils._utils` module:
-  - `requests_retry_session2` helper function added to enable automatic retries of API calls
-
-### Updated
-
-- In the `integration` module:
-
-  - New parameter `include_removed_sites` added to the `get_sites()` function.
-    - Determines whether or not to include sites marked as "IsRemoved" in the returned dataframe.
-    - Defaults to False, indicating removed sites will not be included.
-  - Updated the`get_device_sensor()` method to check if requested metadata keys or requested
-    tag groups exist for the portfolio and exception if they don't.
-  - New parameter `send_notification` added to the `upsert_timeseries()` function.
-    - This enables Iq Notification messages to be sent when set to `True`
-    - Defaults to `False`
-  - For the `get_sites()`, `get_device_sensors()` and `get_data()` functions, additional parameters have
-    been added to allow customisation of the newly implemented retry logic:
-    - `retries : int`
-      - Number of retries performed beforereturning last retry instance's response status. Max retries = 10.
-        Defaults to 0 currently for backwards compatibility.
-    - `backoff_factor`
-      - If A backoff factor to apply between attempts after the second try (most errors are resolved immediately by a
-        second try without a delay).
-        {_backoff factor_} \* (2 \*\* ({_retry count_} - 1)) seconds
-
-- In the `error_handlers` module:
-  - For the `validate_datetime` function, added two new parameters to enable automatic
-    posting of errors to the Switch Platform:
-    - `errors` : boolean, defaults to False. To enable posting of errors, set to True.
-    - `api_inputs`: defaults to None. Needs to be set to the object returned from switch_api.initialize() if `errors=True`.
-
-### Fixed
-
-- In the `integration` module:
-  - Resolved outlier scenario resulting in unhandled exception on the `upsert_sites()` function.
-  - Minor fix to the `upsert_discovered_records()` method to handle the case when unexpected columns
-    are present in the dataframe passed to `df` input parameter
-
-## 0.4.6
-
-### Added
-
-- Task Priority and Task Framework data feed deployment settings
-  - Task Priority and Task Framework are now available to set when deploying data feeds
-    - Task Priority
-      - Determines the priority of the datafeed tasks when processing.
-      - This equates to how much resources would be alloted to run the task
-      - Available options are: `default`, `standard`, or `advanced`.
-        - set to `advanced` for higher resource when processing data feed task
-      - Defaults to 'default'.
-    - Task Framework
-      - Determines the framework of the datafeed tasks when processing.
-        - 'PythonScriptFramework' for the old task runner engine.
-        - 'TaskInsightsEngine' for the new task running in container apps.
-        - Defaults to 'PythonScriptFramework'
-
-## 0.4.5
-
-### Added
-
-- Email Sender Module
-  - Send emails to active users within a Portfolio in Switch Automation Platform
-  - Limitations:
-    - Emails cannot be sent to users outside of the Portfolio including other users within the platform
-    - Maximum of five attachments per email
-    - Each attachment has a maximum size of 5mb
-  - See function code documentation and usage example below
-- New `generate_filepath` method to provide a filepath where files can be stored
-  - Works well with the attachment feature of the Email Sender Module. Store files in the generated filepath of this method and pass into email attachments
-  - See function code documentation and usage example below
-
-### Email Sender Usage
-
-```python
-import switch_api as sw
-
-sw.email.send_email(
-    api_inputs=api_inputs,
-    subject='',
-    body='',
-    to_recipients=[],
-    cc_recipients=[], # Optional
-    bcc_recipients=[], # Optional
-    attachments=['/file/path/to/attachment.csv'], # Optional
-    conversation_id='' # Optional
-)
-```
-
-### generate_filepath Usage
-
-```python
-import switch_api as sw
-
-generated_attachment_filepath = sw.generate_filepath(api_inputs=api_inputs, filename='generated_attachment.txt')
-
-# Example of where it could be used
-sw.email.send_email(
-    ...
-    attachments=[generated_attachment_filepath]
-    ...
-)
-```
-
-### Fixed
-
-- Issue where `upsert_device_sensors_ext` method was not posting metadata and tag_columns to API
-
-## 0.3.3
-
-### Added
-
-- New `upsert_device_sensors_ext` method to the `integration` module.
-  - Compared to existing `upsert_device_sensors` following are supported:
-    - Installation Code or Installation Id may be provided
-      - BUT cannot provide mix of the two, all must have either code or id and not both.
-    - DriverClassName
-    - DriverDeviceType
-    - PropertyName
-
-### Added Feature - Switch Python Extensions
-
-- Extensions may be used in Task Insights and Switch Guides for code reuse
-- Extensions maybe located in any directory structure within the repo where the usage scripts are located
-- May need to adjust your environment to detect the files if you're not running a project environment
-  - Tested on VSCode and PyCharm - contact Switch Support for issues.
-
-#### Extensions Usage
-
-```python
-import switch_api as sw
-
-# Single import line per extension
-from extensions.my_extension import MyExtension
-
-@sw.extensions.provide(field="some_extension")
-class MyTask:
-    some_extension: MyExtension
-
-if __name__ == "__main__":
-    task = MyTask()
-    task.some_extension.do_something()
-```
-
-#### Extensions Registration
-
-```python
-import uuid
-import switch_api as sw
-
-class SimpleExtension(sw.extensions.ExtensionTask):
-    @property
-    def id(self) -> uuid.UUID:
-        # Unique ID for the extension.
-        # Generate in CLI using:
-        #   python -c 'import uuid; print(uuid.uuid4())'
-        return '46759cfe-68fa-440c-baa9-c859264368db'
-
-    @property
-    def description(self) -> str:
-        return 'Extension with a simple get_name function.'
-
-    @property
-    def author(self) -> str:
-        return 'Amruth Akoju'
-
-    @property
-    def version(self) -> str:
-        return '1.0.1'
-
-    def get_name(self):
-        return "Simple Extension"
-
-# Scaffold code for registration. This will not be persisted in the extension.
-if __name__ == '__main__':
-    task = SimpleExtension()
-
-    api_inputs = sw.initialize(api_project_id='<portfolio-id>')
-
-    # Usage test
-    print(task.get_name())
-
-    # =================================================================
-    # REGISTER TASK & DATAFEED ========================================
-    # =================================================================
-    register = sw.pipeline.Automation.register_task(api_inputs, task)
-    print(register)
-
-```
-
-### Updated
-
-- get_data now has an optional parameter to return a pandas.DataFrame or JSON
-
-## 0.2.27
-
-### Fix
-
-- Issue where Timezone DST Offsets API response of `upsert_timeseries` in `integration` module was handled incorrectly
-
-## 0.2.26
-
-### Updated
-
-- Optional `table_def` parameter on `upsert_data`, `append_data`, and `replace_data` in `integration` module
-  - Enable clients to specify the table structure. It will be merged to the inferred table structure.
-- `list_deployments` in Automation module now provides `Settings` and `DriverId` associated with the deployments
-
-## 0.2.25
-
-### Updated
-
-- Update handling of empty Timezone DST Offsets of `upsert_timeseries` in `integration` module
-
-## 0.2.24
-
-### Updated
-
-- Fix default `ingestion_mode` parameter value to 'Queue' instead of 'Queued' on `upsert_timeseries` in `integration` module
-
-## 0.2.23
-
-### Updated
-
-- Optional `ingestion_mode` parameter on `upsert_timeseries` in `integration` module
-  - Include `ingestionMode` in json payload passed to backend API
-  - `IngestionMode` type must be `Queue` or `Stream`
-  - Default `ingestion_mode` parameter value in `upsert_timeseries` is `Queue`
-  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
-
-## 0.2.22
-
-### Updated
-
-- Optional `ingestion_mode` parameter on `upsert_data` in `integration` module
-  - Include `ingestionMode` in json payload passed to backend API
-  - `IngestionMode` type must be `Queue` or `Stream`
-  - Default `ingestion_mode` parameter value in `upsert_data` is `Queue`
-  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
-
-### Fix
-
-- sw.pipeline.logger handlers stacking
-
-## 0.2.21
-
-### Updated
-
-- Fix on `get_data` method in `dataset` module
-  - Sync parameter structure to backend API for `get_data`
-  - List of dict containing properties of `name`, `value`, and `type` items
-  - `type` property must be one of subset of the new Literal `DATA_SET_QUERY_PARAMETER_TYPES`
-
-## 0.2.20
-
-### Added
-
-- Newly supported Azure Storage Account: GatewayMqttStorage
-- An optional property on QueueTask to specific QueueType
-  - Default: DataIngestion
-
-## 0.2.19
-
-### Fixed
-
-- Fix on `upsert_timeseries` method in `integration` module
-  - Normalized TimestampId and TimestampLocalId seconds
-- Minor fix on `upsert_entities_affected` method in `integration` utils module
-  - Prevent upsert entities affected count when data feed file status Id is not valid
-- Minor fix on `get_metadata_keys` method in `integration` helper module
-  - Fix for issue when a portfolio does not contain any values in the ApiMetadata table
-
-## 0.2.18
-
-### Added
-
-- Added new `is_specific_timezone` parameter in `upsert_timeseries` method of `integration` module
-
-  - Accepts a timezone name as the specific timezone used by the source data.
-  - Can either be of type str or bool and defaults to the value of False.
-  - Cannot have value if 'is_local_time' is set to True.
-  - Retrieve list of available timezones using 'get_timezones' method in `integration` module
-
-    | is_specific_timezone | is_local_time | Description                                                                                                                                                     |
-    | -------------------- | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
-    | False                | False         | Datetimes in provided data is already in UTC and should remain as the value of Timestamp. The TimestampLocal (conversion to site-local Timezone) is calculated. |
-    | False                | True          | Datetimes in provided data is already in the site-local Timezone & should be used to set the value of the TimestampLocal field. The UTC Timestamp is calculated |
-    | Has Value            | True          | NOT ALLOWED                                                                                                                                                     |
-    | Has Value            | False         | Both Timestamp and TimestampLocal fields will are calculated. Datetime is converted to UTC then to Local.                                                       |
-    | True                 |               | NOT ALLOWED                                                                                                                                                     |
-    | '' (empty string)    |               | NOT ALLOWED                                                                                                                                                     |
-
-### Fixed
-
-- Minor fix on `upsert_tags` and `upsert_device_metadata` methods in `integration` module
-  - List of required_columns was incorrectly being updated when these functions were called
-- Minor fix on `upsert_event_work_order_id` method in `integration` module when attempting to update status of an Event
-
-### Updated
-
-- Update on `DiscoveryIntegrationInput` namedtuple - added `job_id`
-- Update `upsert_discovered_records` method required columns in `integration` module
-  - add required `JobId` column for Data Frame parameter
-
-## 0.2.17
-
-### Fixed
-
-- Fix on `upsert_timeseries()` method in `integration` module for duplicate records in ingestion files
-  - records whose Timestamp falls in the exact DST start created 2 records with identical values but different TimestampLocal
-    - one has the TimestampLocal of a DST and the other does not
-
-### Updated
-
-- Update on `get_sites()` method in `integration` module for `InstallationCode` column
-  - when the `InstallationCode' value is null in the database it returns an empty string
-  - `InstallationCode` column is explicity casted to dtype 'str'
-
-## 0.2.16
-
-### Added
-
-- Added new 5 minute interval for `EXPECTED_DELIVERY` Literal in `automation` module
-  - support for data feed deployments Email, FTP, Upload, and Timer
-  - usage: expected_delivery='5min'
-
-### Fixed
-
-- Minor fix on `upsert_timeseries()` method using `data_feed_file_status_id` parameter in `integration` module.
-  - `data_feed_file_status_id` parameter value now synced between process records and ingestion files when supplied
-
-### Updated
-
-- Reduced ingestion files records chunking by half in `upsert_timeseries()` method in `integration` module.
-  - from 100k records chunk down to 50k records chunk
-
-## 0.2.15
-
-### Updated
-
-- Optimized `upsert_timeseries()` method memory upkeep in `integration` module.
-
-## 0.2.14
-
-### Fixed
-
-- Minor fix on `invalid_file_format()` method creating structured logs in `error_handlers` module.
-
-## 0.2.13
-
-### Updated
-
-- Freeze Pandera[io] version to 0.7.1
-  - PandasDtype has been deprecated since 0.8.0
-
-### Compatibility
-
-- Ensure local environment is running Pandera==0.7.1 to match cloud container state
-- Downgrade/Upgrade otherwise by running:
-  - pip uninstall pandera
-  - pip install switch_api
-
-## 0.2.12
-
-### Added
-
-- Added `upsert_tags()` method to the `integration` module.
-  - Upsert tags to existing sites, devices, and sensors
-  - Upserting of tags are categorised by the tagging level which are Site, Device, and Sensor level
-  - Input dataframe requires `Identifier' column whose value depends on the tagging level specified
-    - For Site tag level, InstallationIds are expected to be in the `Identifier` column
-    - For Device tag level, DeviceIds are expected to be in the `Identifier` column
-    - For Sensor tag level, ObjectPropertyIds are expected to be in the `Identifier` column
-- Added `upsert_device_metadata()` method to the `integration` module.
-  - Upsert metadata to existing devices
-
-### Usage
-
-- `upsert_tags()`
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Device')
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Sensor')
-  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Site')
-- `upsert_device_metadata()`
-  - sw.integration.upsert_device_metadata(api_inputs=api_inputs, df=raw_df)
-
-## 0.2.11
-
-### Added
-
-- New `cache` module that handles cache data related transactions
-  - `set_cache` method that stores data to cache
-  - `get_cache` method that gets stored data from cache
-  - Stored data can be scoped / retrieved into three categories namely Task, Portfolio, and DataFeed scopes
-    - For Task scope,
-      - Data cache can be retrieved by any Portfolio or Datafeed that runs in same Task
-      - provide TaskId (self.id when calling from the driver)
-    - For DataFeed scope,
-      - Data cache can be retrieved (or set) within the Datafeed deployed in portfolio
-      - Provide UUID4 for local testing. api_inputs.data_feed_id will be used when running in the cloud.
-    - For Portfolio scope:
-      - Data cache can be retrieved (or set) by any Datafeed deployed in portfolio
-      - scope_id will be ignored and api_inputs.api_project_id will be used.
-
-## 0.2.10
-
-### Fixed
-
-- Fixed issue with `upsert_timeseries_ds()` method in the `integration` module where required fields such as
-  `Timestamp`, `ObjectPropertyId`, `Value` were being removed.
-
-## 0.2.9
-
-### Added
-
-- Added `upsert_timeseries()` method to the `integration` module.
-  - Data ingested into table storage in addition to ADX Timeseries table
-  - Carbon calculation performed where appropriate
-    - Please note: If carbon or cost are included as fields in the `Meta` column then no carbon / cost calculation will be performed
-
-### Changed
-
-- Added `DriverClassName` to required columns for `upsert_discovered_records()` method in the `integration` module
-
-### Fixed
-
-- A minor fix to 15-minute interval in `upsert_timeseries_ds()` method in the `integration` module.
-
-## 0.2.8
-
-### Changed
-
-- For the `EventWorkOrderTask` class in the `pipeline` module, the `check_work_order_input_valid()` and the
-  `generate_work_order()` methods expect an additional 3 keys to be included by default in the dictionary passed to
-  the `work_order_input` parameter:
-  - `InstallationId`
-  - `EventLink`
-  - `EventSummary`
-
-### Fixed
-
-- Issue with the header/payload passed to the API within the `upsert_event_work_order_id()`
-  function of the `integration` module.
-
-## 0.2.7
-
-### Added
-
-- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
-  - this new method is only applicable for tasks that subclass the `EventWorkOrderTask` base class.
-
-### Changed
-
-- The `data_feed_id` is now a required parameter, not optional, for the following methods on the `Automation` class of
-  the `pipeline` module:
-  - `deploy_on_timer()`
-  - `deploy_as_email_data_feed()`
-  - `deploy_as_ftp_data_feed()`
-  - `deploy_as_upload_data_feed()`
-- The `email_address_domain` is now a required parameter, not optional, for the `deploy_as_email_data_feed()` method
-  on the `Automation` class of the `pipeline` module.
-
-### Fixed
-
-- issue with payload on `switch_api.pipeline.Automation.register_task()` method for `AnalyticsTask` and
-  `EventWorkOrderTask` base classes.
-
-## 0.2.6
-
-### Fixed
-
-- Fixed issues on 2 methods in the `Automation` class of the `pipeline` module:
-  - `delete_data_feed()`
-  - `cancel_deployed_data_feed()`
-
-### Added
-
-In the `pipeline` module:
-
-- Added new class `EventWorkOrderTask`
-  - This task type is for generation of work orders in 3rd party systems via the Switch Automation Platform's Events UI.
-
-### Changed
-
-In the `pipeline` module:
-
-- `AnalyticsTask` - added a new method & a new abstract property:
-  - `analytics_settings_definition` abstract property - defines the required inputs (& how these are displayed in the
-    Switch Automation Platform UI) for the task to successfully run
-  - added `check_analytics_settings_valid()` method that should be used to validate the
-    `analytics_settings` dictionary passed to the `start()` method contains the required keys for the task to
-    successfully run (as defined by the `analytics_settings_definition`)
-
-In the `error_handlers` module:
-
-- In the `post_errors()` function, the parameter `errors_df` is renamed to `errors` and now accepts strings in
-  addition to pandas.DataFrame
-
-### Removed
-
-Due to cutover to a new backend, the following have been removed:
-
-- `run_clone_modules()` function from the `analytics` module
-- the entire `platform_insights` module including the :
-  - `get_current_insights_by_equipment()` function
-
-## 0.2.5
-
-### Added
-
-- The `Automation` class of the `pipeline` module has 2 new methods added: -`delete_data_feed()`
-  - Used to delete an existing data feed and all related deployment settings
-  - `cancel_deployed_data_feed()`
-    - used to cancel the specified `deployment_type` for a given `data_feed_id`
-    - replaces and expands the functionality previously provided in the `cancel_deployed_timer()` method which has been
-      removed.
-
-### Removed
-
-- Removed the `cancel_deployed_timer()` method from the `Automation` class of the `pipeline` module
-  - this functionality is available through the new `cancel_deployed_data_feed()` method when `deployment_type`
-    parameter set to `['Timer']`
-
-## 0.2.4
-
-### Changed
-
-- New parameter `data_feed_name` added to the 4 deployment methods in the `pipeline` module's `Automation` class
-  - `deploy_as_email_data_feed()`
-  - `deploy_as_ftp_data_feed()`
-  - `deploy_as_upload_data_feed()`
-  - `deploy_on_timer()`
-
-## 0.2.3
-
-### Fixed
-
-- Resolved minor issue on `register_task()` method for the `Automation` class in the `pipeline` module.
-
-## 0.2.2
-
-### Fixed
-
-- Resolved minor issue on `upsert_discovered_records()` function in `integration` module related to device-level
-  and sensor-level tags.
-
-## 0.2.1
-
-### Added
-
-- New class added to the `pipeline` module
-  - `DiscoverableIntegrationTask` - for API integrations that are discoverable.
-    - requires `process()` & `run_discovery()` abstract methods to be created when sub-classing
-    - additional abstract property, `integration_device_type_definition`, required compared to base `Task`
-- New function `upsert_discovered_records()` added to the `integration` module
-  - Required for the `DiscoverableIntegrationTask.run_discovery()` method to upsert discovery records to Build -
-    Discovery & Selection UI
-
-### Fixed
-
-- Set minimum msal version required for the switch_api package to be installed.
-
-## 0.2.0
-
-Major overhaul done of the switch_api package. A complete replacement of the API used by the package was done.
-
-### Changed
-
-- The `user_id` parameter has been removed from the `switch_api.initialise()` function.
-  - Authentication of the user is now done via Switch Platform SSO. The call to initialise will trigger a web browser
-    window to open to the platform login screen.
-    - Note: each call to initialise for a portfolio in a different datacentre will open up browser and requires user to
-      input their username & password.
-    - for initialise on a different portfolio within the same datacentre, the authentication is cached so user will not
-      be asked to login again.
-- `api_inputs` is now a required parameter for the `switch_api.pipeline.Automation.register_task()`
-- The `deploy_on_timer()`, `deploy_as_email_data_feed()`, `deploy_as_upload_data_feed()`, and
-  `deploy_as_ftp_data_feed()` methods on the `switch_api.pipeline.Automation` class have an added parameter:
-  `data_feed_id`
-  - This new parameter allows user to update an existing deployment for the portfolio specified in the `api_inputs`.
-  - If `data_feed_id` is not supplied, a new data feed instance will be created (even if portfolio already has that
-    task deployed to it)
-
-## 0.1.18
-
-### Changed
-
-- removed rebuild of the ObjectProperties table in ADX on call to `upsert_device_sensors()`
-- removed rebuild of the Installation table in ADX on call to `upsert_sites()`
-
-## 0.1.17
-
-### Fixed
-
-- Fixed issue with `deploy_on_timer()` method of the `Automation` class in the `pipeline` module.
-- Fixed column header issue with the `get_tag_groups()` function of the `integration` module.
-- Fixed missing Meta column on table generated via `upsert_workorders()` function of the `integration` module.
-
-### Added
-
-- New method for uploading custom data to blob `Blob.custom_upload()`
-
-### Updated
-
-- Updated the `upsert_device_sensors()` to improve performance and aid release of future functionality.
-
-## 0.1.16
-
-### Added
-
-To the `pipeline` module:
-
-- New method `data_feed_history_process_errors()`, to the `Automation` class.
-  - This method returns a dataframe containing the distinct set of error types encountered for a specific
-    `data_feed_file_status_id`
-- New method `data_feed_history_errors_by_type` , to the `Automation` class.
-  - This method returns a dataframe containing the actual errors identified for the specified `error_type` and
-    `data_feed_file_status_id`
-
-Additional logging was also incorporated in the backend to support the Switch Platform UI.
-
-### Fixed
-
-- Fixed issue with `register()` method of the `Automation` class in the `pipeline` module.
-
-### Changed
-
-For the `pipeline` module:
-
-- Standardised the following methods of the `Automation` class to return pandas.DataFrame objects.
-- Added additional error checks to ensure only allowed values are passed to the various `Automation` class methods
-  for the parameters:
-  - `expected_delivery`
-  - `deploy_type`
-  - `queue_name`
-  - `error_type`
-
-For the `integration` module:
-
-- Added additional error checks to ensure only allowed values are passed to `post_errors` function for the parameters:
-  - `error_type`
-  - `process_status`
-
-For the `dataset` module:
-
-- Added additional error check to ensure only allowed values are provided for the `query_language` parameter of the
-  `get_data` function.
-
-For the `_platform` module:
-
-- Added additional error checks to ensure only allowed values are provided for the `account` parameter.
-
-## 0.1.14
-
-### Changed
-
-- updated get_device_sensors() to not auto-detect the data type - to prevent issues such as stripping leading zeroes,
-  etc from metadata values.
-
-## 0.1.13
-
-### Added
-
-To the `pipeline` module:
-
-- Added a new method, `data_feed_history_process_output`, to the `Automation` class
-
-## 0.1.11
-
-### Changed
-
-- Update to access to `logger` - now available as `switch_api.pipeline.logger()`
-- Update to function documentation
-
-## 0.1.10
-
-### Changed
-
-- Updated the calculation of min/max date (for timezone conversions) inside the `upsert_device_sensors` function as
-  the previous calculation method will not be supported in a future release of numpy.
-
-### Fixed
-
-- Fixed issue with retrieval of tag groups and tags via the functions:
-  - `get_sites`
-  - `get_device_sensors`
-
-## 0.1.9
-
-### Added
-
-- New module `platform_insights`
-
-In the `integration` module:
-
-- New function `get_sites` added to lookup site information (optionally with site-level tags)
-- New function `get_device_sensors` added to assist with lookup of device/sensor information, optionally including
-  either metadata or tags.
-- New function `get_tag_groups` added to lookup list of sensor-level tag groups
-- New function `get_metadata_keys` added to lookup list of device-level metadata keys
-
-### Changed
-
-- Modifications to connections to storage accounts.
-- Additional parameter `queue_name` added to the following methods of the `Automation` class of the `pipeline`
-  module:
-  - `deploy_on_timer`
-  - `deploy_as_email_data_feed`
-  - `deploy_as_upload_data_feed`
-  - `deploy_as_ftp_data_feed`
-
-### Fixed
-
-In the `pipeline` module:
-
-- Addressed issue with the schema validation for the `upsert_workorders` function
-
-## 0.1.8
-
-### Changed
-
-In the `integrations` module:
-
-- Updated to batch upserts by DeviceCode to improve reliability & performance of the `upsert_device_sensors` function.
-
-### Fixed
-
-In the `analytics` module:
-
-- typing issue that caused error in the import of the switch_api package for python 3.8
-
-## 0.1.7
-
-### Added
-
-In the `integrations` module:
-
-- Added new function `upsert_workorders`
-  - Provides ability to ingest work order data into the Switch Automation Platform.
-  - Documentation provides details on required & optional fields in the input dataframe and also provides information
-    on allowed values for some fields.
-  - Two attributes available for function, added to assist with creation of scripts by providing list of required &
-    optional fields:
-    - `upsert_workorders.df_required_columns`
-    - `upsert_workorders.df_optional_columns`
-- Added new function `get_states_by_country`:
-  - Retrieves the list of states for a given country. Returns a dataframe containing both the state name and
-    abbreviation.
-- Added new function `get_equipment_classes`:
-  - Retrieves the list of allowed values for Equipment Class.
-    - EquipmentClass is a required field for the upsert_device_sensors function
-
-### Changed
-
-In the `integrations` module:
-
-- For the `upsert_device_sensors` function:
-  - New attributes added to assist with creation of tasks:
-    - `upsert_device_sensors.df_required_columns` - returns list of required columns for the input `df`
-  - Two new fields required to be present in the dataframe passed to function by parameter `df`:
-    - `EquipmentClass`
-    - `EquipmentLabel`
-  - Fix to documentation so required fields in documentation match.
-- For the `upsert_sites` function:
-  - New attributes added to assist with creation of tasks:
-    - `upsert_sites.df_required_columns` - returns list of required columns for the input `df`
-    - `upsert_sites.df_optional_columns` - returns list of required columns for the input `df`
-- For the `get_templates` function:
-  - Added functionality to filter by type via new parameter `object_property_type`
-  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
-    converted to lowercase.
-- For the `get_units_of_measure` function:
-  - Added functionality to filter by type via new parameter `object_property_type`
-  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
-    converted to lowercase.
-
-In the `analytics` module:
-
-- Modifications to type hints and documentation for the functions:
-  - `get_clone_modules_list`
-  - `run_clone_modules`
-- Additional logging added to `run_clone_modules`
-
-## 0.1.6
-
-### Added
-
-- Added new function `upsert_timeseries_ds()` to the `integrations` module
-
-### Changed
-
-- Additional logging added to `invalid_file_format()` function from the `error_handlers` module.
-
-### Removed
-
-- Removed `append_timeseries()` function
-
-## 0.1.5
-
-### Fixed
-
-- bug with `upsert_sites()` function that caused optional columns to be treated as required columns.
-
-### Added
-
-Added additional functions to the `error_handlers` module:
-
-- `validate_datetime()` - which checks whether the values of the datetime column(s) of the source file are valid. Any
-  datetime errors identified by this function should be passed to the `post_errors()` function.
-- `post_errors()` - used to post errors (apart from those identified by the `invalid_file_format()` function) to
-  the data feed dashboard.
-
-## 0.1.4
-
-### Changed
-
-Added additional required properties to the Abstract Base Classes (ABC): Task, IntegrationTask, AnalyticsTask,
-LogicModuleTask. These properties are:
-
-- Author
-- Version
-
-Added additional parameter `query_language` to the `switch.integration.get_data()` function. Allowed values for this
-parameter are:
-
-- `sql`
-- `kql`
-
-Removed the `name_as_filename` and `treat_as_timeseries` parameter from the following functions:
-
-- `switch.integration.replace_data()`
-- `switch.integration.append_data()`
-- `switch.integration.upload_data()`
-
-
+Metadata-Version: 2.1
+Name: switch-api
+Version: 0.5.4b3
+Summary: A complete package for data ingestion into the Switch Automation Platform.
+Home-page: UNKNOWN
+Author: Switch Automation Pty Ltd.
+License: MIT License
+Platform: UNKNOWN
+Classifier: Development Status :: 2 - Pre-Alpha
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Intended Audience :: Other Audience
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Natural Language :: English
+Requires-Python: >=3.8.0
+Description-Content-Type: text/markdown
+License-File: LICENCE
+License-File: AUTHORS.rst
+
+# Switch Automation library for Python
+This is a package for data ingestion into the Switch Automation software platform. 
+
+You can find out more about the platform on [Switch Automation](https://www.switchautomation.com)
+
+## Getting started
+
+### Prerequisites
+* Python 3.8 or later is required to use this package. 
+* You must have a [Switch Automation user account](https://www.switchautomation.com/our-solution/) to use this package. 
+
+### Install the package
+Install the Switch Automation library for Python with [pip](https://pypi.org/project/pip/):
+
+```bash
+pip install switch_api
+```
+
+# History
+
+## 0.5.3
+
+### Added
+
+- In the `integration` module:
+  - Added `override_existing` parameter in `upsert_discovered_records`
+  - Flag if it the values passed to df will override existing integration records. Only valid if running locally,
+    not on a deployed task where it is triggered via UI.
+  - Defaults to False
+
+## 0.5
+
+### Added
+
+- In the `pipeline` module:
+  - Added a new task type called `Guide`.
+    - this task type should be sub-classed in concert with one of the Task sub-classes when deploying a guide to the
+      marketplace.
+  - Added a new method to the `Automation` class called `register_guide_task()`
+    - this method is used to register tasks that sub-class the `Guide` task and also posts form files to blob and
+      registers the guide to the Marketplace.
+- New `_guide` module - only to be referenced when doing initial development of a Guide
+  - `guide`'s `local_start' method
+    - Allows to run mock guides engine locally that ables to debug `Guide` task types with Form Kit playground.
+
+### Fixed
+
+- In `controls` module:
+  - modify `submit_control` method parameters - typings
+  - remove extra columns from payload to IoT API requests
+
+## 0.4.9
+
+### Added
+
+- New method added in `automation` module:
+  - `run_data_feed()` - Run python job based on data feed id. This will be sent to the queue for processing and will
+    undergo same procedure as the rest of the datafeed.
+    - Required parameters are `api_inputs` and `data_feed_id`
+    - This has a restriction of only allowing an AnalyticsTask type datafeed to be run and deployed as a Timer
+- New method added in `analytics` module:
+  - `upsert_performance_statistics` - this method should only be used by tasks used to populate the Portfolio
+    Benchmarking feature in the Switch Automation platform
+- New `controls` module added and new method added to this module:
+  - `submit_control()` - method to submit control of sensors
+    - this method returns a tuple: `(control_response, missing_response)`:
+      - `control_response` - is the list of sensors that are acknowledged and process by the MQTTT message broker
+      - `missing_response` = is the list of sensors that are sensors that were caught by the connection `time_out` -
+        default to 30 secs - meaning the response were no longer waited to be received by the python package.
+        Increasing the time out can potentially help with this.
+
+### Fixed
+
+- In the `integration` module, minor fixes to:
+  - An unhandled exception when using `pandas==2.1.1` on the following functions:
+    - `upsert_sites()`
+    - `upsert_device_sensors()`
+    - `upsert_device_sensors_ext()`
+    - `upsert_workorders()`
+    - `upsert_timeseries_ds()`
+    - `upsert_timeseries()`
+  - Handle deprecation of `pandas.DataFrame.append()` on the following functions:
+    - `upsert_device_sensors()`
+    - `upsert_device_sensors_ext()`
+  - An unhandled exception for `connect_to_sql()` function when the internal API call within
+    `_get_sql_connection_string()` fails.
+
+## 0.4.8
+
+### Added
+
+- New class added to the `pipeline` module:
+  - `BlobTask` - This class is used to create integrations that post data to the Switch Automation Platform using a
+    blob container & Event Hub Queue as the source.
+    - Please Note: This task type requires external setup in Azure by Switch Automation Developers before a task can be
+      registered or deployed.
+    - requires `process_file()` abstract method to be created when sub-classing
+- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
+  - this new method is only applicable for tasks that subclass the `BlobTask` base class.
+- In the `integration` module, new helper methods have been added:
+  - `connect_to_sql()` method creates a pyodbc connection object to enable easier querying of the SQL database via the
+    `pyodbc` library
+  - `amortise_across_days()` method enables easier amortisation of data across days in a period, either inclusive or
+    exclusive of end date.
+  - `get_metadata_where_clause()` method enables creation of `sql_where_clause` for the `get_device_sensors`() method
+    where for each metadata key the sql checks its not null.
+- In the `error_handlers` module:
+  - `check_duplicates()` method added to check for duplicates & post appropriate errors to Task Insights UI in the
+    Switch Automation platform.
+- In the `_utils._utils` module:
+  - `requests_retry_session2` helper function added to enable automatic retries of API calls
+
+### Updated
+
+- In the `integration` module:
+
+  - New parameter `include_removed_sites` added to the `get_sites()` function.
+    - Determines whether or not to include sites marked as "IsRemoved" in the returned dataframe.
+    - Defaults to False, indicating removed sites will not be included.
+  - Updated the`get_device_sensor()` method to check if requested metadata keys or requested
+    tag groups exist for the portfolio and exception if they don't.
+  - New parameter `send_notification` added to the `upsert_timeseries()` function.
+    - This enables Iq Notification messages to be sent when set to `True`
+    - Defaults to `False`
+  - For the `get_sites()`, `get_device_sensors()` and `get_data()` functions, additional parameters have
+    been added to allow customisation of the newly implemented retry logic:
+    - `retries : int`
+      - Number of retries performed beforereturning last retry instance's response status. Max retries = 10.
+        Defaults to 0 currently for backwards compatibility.
+    - `backoff_factor`
+      - If A backoff factor to apply between attempts after the second try (most errors are resolved immediately by a
+        second try without a delay).
+        {_backoff factor_} \* (2 \*\* ({_retry count_} - 1)) seconds
+
+- In the `error_handlers` module:
+  - For the `validate_datetime` function, added two new parameters to enable automatic
+    posting of errors to the Switch Platform:
+    - `errors` : boolean, defaults to False. To enable posting of errors, set to True.
+    - `api_inputs`: defaults to None. Needs to be set to the object returned from switch_api.initialize() if `errors=True`.
+
+### Fixed
+
+- In the `integration` module:
+  - Resolved outlier scenario resulting in unhandled exception on the `upsert_sites()` function.
+  - Minor fix to the `upsert_discovered_records()` method to handle the case when unexpected columns
+    are present in the dataframe passed to `df` input parameter
+
+## 0.4.6
+
+### Added
+
+- Task Priority and Task Framework data feed deployment settings
+  - Task Priority and Task Framework are now available to set when deploying data feeds
+    - Task Priority
+      - Determines the priority of the datafeed tasks when processing.
+      - This equates to how much resources would be alloted to run the task
+      - Available options are: `default`, `standard`, or `advanced`.
+        - set to `advanced` for higher resource when processing data feed task
+      - Defaults to 'default'.
+    - Task Framework
+      - Determines the framework of the datafeed tasks when processing.
+        - 'PythonScriptFramework' for the old task runner engine.
+        - 'TaskInsightsEngine' for the new task running in container apps.
+        - Defaults to 'PythonScriptFramework'
+
+## 0.4.5
+
+### Added
+
+- Email Sender Module
+  - Send emails to active users within a Portfolio in Switch Automation Platform
+  - Limitations:
+    - Emails cannot be sent to users outside of the Portfolio including other users within the platform
+    - Maximum of five attachments per email
+    - Each attachment has a maximum size of 5mb
+  - See function code documentation and usage example below
+- New `generate_filepath` method to provide a filepath where files can be stored
+  - Works well with the attachment feature of the Email Sender Module. Store files in the generated filepath of this method and pass into email attachments
+  - See function code documentation and usage example below
+
+### Email Sender Usage
+
+```python
+import switch_api as sw
+
+sw.email.send_email(
+    api_inputs=api_inputs,
+    subject='',
+    body='',
+    to_recipients=[],
+    cc_recipients=[], # Optional
+    bcc_recipients=[], # Optional
+    attachments=['/file/path/to/attachment.csv'], # Optional
+    conversation_id='' # Optional
+)
+```
+
+### generate_filepath Usage
+
+```python
+import switch_api as sw
+
+generated_attachment_filepath = sw.generate_filepath(api_inputs=api_inputs, filename='generated_attachment.txt')
+
+# Example of where it could be used
+sw.email.send_email(
+    ...
+    attachments=[generated_attachment_filepath]
+    ...
+)
+```
+
+### Fixed
+
+- Issue where `upsert_device_sensors_ext` method was not posting metadata and tag_columns to API
+
+## 0.3.3
+
+### Added
+
+- New `upsert_device_sensors_ext` method to the `integration` module.
+  - Compared to existing `upsert_device_sensors` following are supported:
+    - Installation Code or Installation Id may be provided
+      - BUT cannot provide mix of the two, all must have either code or id and not both.
+    - DriverClassName
+    - DriverDeviceType
+    - PropertyName
+
+### Added Feature - Switch Python Extensions
+
+- Extensions may be used in Task Insights and Switch Guides for code reuse
+- Extensions maybe located in any directory structure within the repo where the usage scripts are located
+- May need to adjust your environment to detect the files if you're not running a project environment
+  - Tested on VSCode and PyCharm - contact Switch Support for issues.
+
+#### Extensions Usage
+
+```python
+import switch_api as sw
+
+# Single import line per extension
+from extensions.my_extension import MyExtension
+
+@sw.extensions.provide(field="some_extension")
+class MyTask:
+    some_extension: MyExtension
+
+if __name__ == "__main__":
+    task = MyTask()
+    task.some_extension.do_something()
+```
+
+#### Extensions Registration
+
+```python
+import uuid
+import switch_api as sw
+
+class SimpleExtension(sw.extensions.ExtensionTask):
+    @property
+    def id(self) -> uuid.UUID:
+        # Unique ID for the extension.
+        # Generate in CLI using:
+        #   python -c 'import uuid; print(uuid.uuid4())'
+        return '46759cfe-68fa-440c-baa9-c859264368db'
+
+    @property
+    def description(self) -> str:
+        return 'Extension with a simple get_name function.'
+
+    @property
+    def author(self) -> str:
+        return 'Amruth Akoju'
+
+    @property
+    def version(self) -> str:
+        return '1.0.1'
+
+    def get_name(self):
+        return "Simple Extension"
+
+# Scaffold code for registration. This will not be persisted in the extension.
+if __name__ == '__main__':
+    task = SimpleExtension()
+
+    api_inputs = sw.initialize(api_project_id='<portfolio-id>')
+
+    # Usage test
+    print(task.get_name())
+
+    # =================================================================
+    # REGISTER TASK & DATAFEED ========================================
+    # =================================================================
+    register = sw.pipeline.Automation.register_task(api_inputs, task)
+    print(register)
+
+```
+
+### Updated
+
+- get_data now has an optional parameter to return a pandas.DataFrame or JSON
+
+## 0.2.27
+
+### Fix
+
+- Issue where Timezone DST Offsets API response of `upsert_timeseries` in `integration` module was handled incorrectly
+
+## 0.2.26
+
+### Updated
+
+- Optional `table_def` parameter on `upsert_data`, `append_data`, and `replace_data` in `integration` module
+  - Enable clients to specify the table structure. It will be merged to the inferred table structure.
+- `list_deployments` in Automation module now provides `Settings` and `DriverId` associated with the deployments
+
+## 0.2.25
+
+### Updated
+
+- Update handling of empty Timezone DST Offsets of `upsert_timeseries` in `integration` module
+
+## 0.2.24
+
+### Updated
+
+- Fix default `ingestion_mode` parameter value to 'Queue' instead of 'Queued' on `upsert_timeseries` in `integration` module
+
+## 0.2.23
+
+### Updated
+
+- Optional `ingestion_mode` parameter on `upsert_timeseries` in `integration` module
+  - Include `ingestionMode` in json payload passed to backend API
+  - `IngestionMode` type must be `Queue` or `Stream`
+  - Default `ingestion_mode` parameter value in `upsert_timeseries` is `Queue`
+  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
+
+## 0.2.22
+
+### Updated
+
+- Optional `ingestion_mode` parameter on `upsert_data` in `integration` module
+  - Include `ingestionMode` in json payload passed to backend API
+  - `IngestionMode` type must be `Queue` or `Stream`
+  - Default `ingestion_mode` parameter value in `upsert_data` is `Queue`
+  - To enable table streaming ingestion, please contact **helpdesk@switchautomation.com** for assistance.
+
+### Fix
+
+- sw.pipeline.logger handlers stacking
+
+## 0.2.21
+
+### Updated
+
+- Fix on `get_data` method in `dataset` module
+  - Sync parameter structure to backend API for `get_data`
+  - List of dict containing properties of `name`, `value`, and `type` items
+  - `type` property must be one of subset of the new Literal `DATA_SET_QUERY_PARAMETER_TYPES`
+
+## 0.2.20
+
+### Added
+
+- Newly supported Azure Storage Account: GatewayMqttStorage
+- An optional property on QueueTask to specific QueueType
+  - Default: DataIngestion
+
+## 0.2.19
+
+### Fixed
+
+- Fix on `upsert_timeseries` method in `integration` module
+  - Normalized TimestampId and TimestampLocalId seconds
+- Minor fix on `upsert_entities_affected` method in `integration` utils module
+  - Prevent upsert entities affected count when data feed file status Id is not valid
+- Minor fix on `get_metadata_keys` method in `integration` helper module
+  - Fix for issue when a portfolio does not contain any values in the ApiMetadata table
+
+## 0.2.18
+
+### Added
+
+- Added new `is_specific_timezone` parameter in `upsert_timeseries` method of `integration` module
+
+  - Accepts a timezone name as the specific timezone used by the source data.
+  - Can either be of type str or bool and defaults to the value of False.
+  - Cannot have value if 'is_local_time' is set to True.
+  - Retrieve list of available timezones using 'get_timezones' method in `integration` module
+
+    | is_specific_timezone | is_local_time | Description                                                                                                                                                     |
+    | -------------------- | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
+    | False                | False         | Datetimes in provided data is already in UTC and should remain as the value of Timestamp. The TimestampLocal (conversion to site-local Timezone) is calculated. |
+    | False                | True          | Datetimes in provided data is already in the site-local Timezone & should be used to set the value of the TimestampLocal field. The UTC Timestamp is calculated |
+    | Has Value            | True          | NOT ALLOWED                                                                                                                                                     |
+    | Has Value            | False         | Both Timestamp and TimestampLocal fields will are calculated. Datetime is converted to UTC then to Local.                                                       |
+    | True                 |               | NOT ALLOWED                                                                                                                                                     |
+    | '' (empty string)    |               | NOT ALLOWED                                                                                                                                                     |
+
+### Fixed
+
+- Minor fix on `upsert_tags` and `upsert_device_metadata` methods in `integration` module
+  - List of required_columns was incorrectly being updated when these functions were called
+- Minor fix on `upsert_event_work_order_id` method in `integration` module when attempting to update status of an Event
+
+### Updated
+
+- Update on `DiscoveryIntegrationInput` namedtuple - added `job_id`
+- Update `upsert_discovered_records` method required columns in `integration` module
+  - add required `JobId` column for Data Frame parameter
+
+## 0.2.17
+
+### Fixed
+
+- Fix on `upsert_timeseries()` method in `integration` module for duplicate records in ingestion files
+  - records whose Timestamp falls in the exact DST start created 2 records with identical values but different TimestampLocal
+    - one has the TimestampLocal of a DST and the other does not
+
+### Updated
+
+- Update on `get_sites()` method in `integration` module for `InstallationCode` column
+  - when the `InstallationCode' value is null in the database it returns an empty string
+  - `InstallationCode` column is explicity casted to dtype 'str'
+
+## 0.2.16
+
+### Added
+
+- Added new 5 minute interval for `EXPECTED_DELIVERY` Literal in `automation` module
+  - support for data feed deployments Email, FTP, Upload, and Timer
+  - usage: expected_delivery='5min'
+
+### Fixed
+
+- Minor fix on `upsert_timeseries()` method using `data_feed_file_status_id` parameter in `integration` module.
+  - `data_feed_file_status_id` parameter value now synced between process records and ingestion files when supplied
+
+### Updated
+
+- Reduced ingestion files records chunking by half in `upsert_timeseries()` method in `integration` module.
+  - from 100k records chunk down to 50k records chunk
+
+## 0.2.15
+
+### Updated
+
+- Optimized `upsert_timeseries()` method memory upkeep in `integration` module.
+
+## 0.2.14
+
+### Fixed
+
+- Minor fix on `invalid_file_format()` method creating structured logs in `error_handlers` module.
+
+## 0.2.13
+
+### Updated
+
+- Freeze Pandera[io] version to 0.7.1
+  - PandasDtype has been deprecated since 0.8.0
+
+### Compatibility
+
+- Ensure local environment is running Pandera==0.7.1 to match cloud container state
+- Downgrade/Upgrade otherwise by running:
+  - pip uninstall pandera
+  - pip install switch_api
+
+## 0.2.12
+
+### Added
+
+- Added `upsert_tags()` method to the `integration` module.
+  - Upsert tags to existing sites, devices, and sensors
+  - Upserting of tags are categorised by the tagging level which are Site, Device, and Sensor level
+  - Input dataframe requires `Identifier' column whose value depends on the tagging level specified
+    - For Site tag level, InstallationIds are expected to be in the `Identifier` column
+    - For Device tag level, DeviceIds are expected to be in the `Identifier` column
+    - For Sensor tag level, ObjectPropertyIds are expected to be in the `Identifier` column
+- Added `upsert_device_metadata()` method to the `integration` module.
+  - Upsert metadata to existing devices
+
+### Usage
+
+- `upsert_tags()`
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Device')
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Sensor')
+  - sw.integration.upsert_tags(api_inputs=api_inputs, df=raw_df, tag_level='Site')
+- `upsert_device_metadata()`
+  - sw.integration.upsert_device_metadata(api_inputs=api_inputs, df=raw_df)
+
+## 0.2.11
+
+### Added
+
+- New `cache` module that handles cache data related transactions
+  - `set_cache` method that stores data to cache
+  - `get_cache` method that gets stored data from cache
+  - Stored data can be scoped / retrieved into three categories namely Task, Portfolio, and DataFeed scopes
+    - For Task scope,
+      - Data cache can be retrieved by any Portfolio or Datafeed that runs in same Task
+      - provide TaskId (self.id when calling from the driver)
+    - For DataFeed scope,
+      - Data cache can be retrieved (or set) within the Datafeed deployed in portfolio
+      - Provide UUID4 for local testing. api_inputs.data_feed_id will be used when running in the cloud.
+    - For Portfolio scope:
+      - Data cache can be retrieved (or set) by any Datafeed deployed in portfolio
+      - scope_id will be ignored and api_inputs.api_project_id will be used.
+
+## 0.2.10
+
+### Fixed
+
+- Fixed issue with `upsert_timeseries_ds()` method in the `integration` module where required fields such as
+  `Timestamp`, `ObjectPropertyId`, `Value` were being removed.
+
+## 0.2.9
+
+### Added
+
+- Added `upsert_timeseries()` method to the `integration` module.
+  - Data ingested into table storage in addition to ADX Timeseries table
+  - Carbon calculation performed where appropriate
+    - Please note: If carbon or cost are included as fields in the `Meta` column then no carbon / cost calculation will be performed
+
+### Changed
+
+- Added `DriverClassName` to required columns for `upsert_discovered_records()` method in the `integration` module
+
+### Fixed
+
+- A minor fix to 15-minute interval in `upsert_timeseries_ds()` method in the `integration` module.
+
+## 0.2.8
+
+### Changed
+
+- For the `EventWorkOrderTask` class in the `pipeline` module, the `check_work_order_input_valid()` and the
+  `generate_work_order()` methods expect an additional 3 keys to be included by default in the dictionary passed to
+  the `work_order_input` parameter:
+  - `InstallationId`
+  - `EventLink`
+  - `EventSummary`
+
+### Fixed
+
+- Issue with the header/payload passed to the API within the `upsert_event_work_order_id()`
+  function of the `integration` module.
+
+## 0.2.7
+
+### Added
+
+- New method, `deploy_as_on_demand_data_feed()` added to the `Automation` class of the `pipeline` module
+  - this new method is only applicable for tasks that subclass the `EventWorkOrderTask` base class.
+
+### Changed
+
+- The `data_feed_id` is now a required parameter, not optional, for the following methods on the `Automation` class of
+  the `pipeline` module:
+  - `deploy_on_timer()`
+  - `deploy_as_email_data_feed()`
+  - `deploy_as_ftp_data_feed()`
+  - `deploy_as_upload_data_feed()`
+- The `email_address_domain` is now a required parameter, not optional, for the `deploy_as_email_data_feed()` method
+  on the `Automation` class of the `pipeline` module.
+
+### Fixed
+
+- issue with payload on `switch_api.pipeline.Automation.register_task()` method for `AnalyticsTask` and
+  `EventWorkOrderTask` base classes.
+
+## 0.2.6
+
+### Fixed
+
+- Fixed issues on 2 methods in the `Automation` class of the `pipeline` module:
+  - `delete_data_feed()`
+  - `cancel_deployed_data_feed()`
+
+### Added
+
+In the `pipeline` module:
+
+- Added new class `EventWorkOrderTask`
+  - This task type is for generation of work orders in 3rd party systems via the Switch Automation Platform's Events UI.
+
+### Changed
+
+In the `pipeline` module:
+
+- `AnalyticsTask` - added a new method & a new abstract property:
+  - `analytics_settings_definition` abstract property - defines the required inputs (& how these are displayed in the
+    Switch Automation Platform UI) for the task to successfully run
+  - added `check_analytics_settings_valid()` method that should be used to validate the
+    `analytics_settings` dictionary passed to the `start()` method contains the required keys for the task to
+    successfully run (as defined by the `analytics_settings_definition`)
+
+In the `error_handlers` module:
+
+- In the `post_errors()` function, the parameter `errors_df` is renamed to `errors` and now accepts strings in
+  addition to pandas.DataFrame
+
+### Removed
+
+Due to cutover to a new backend, the following have been removed:
+
+- `run_clone_modules()` function from the `analytics` module
+- the entire `platform_insights` module including the :
+  - `get_current_insights_by_equipment()` function
+
+## 0.2.5
+
+### Added
+
+- The `Automation` class of the `pipeline` module has 2 new methods added: -`delete_data_feed()`
+  - Used to delete an existing data feed and all related deployment settings
+  - `cancel_deployed_data_feed()`
+    - used to cancel the specified `deployment_type` for a given `data_feed_id`
+    - replaces and expands the functionality previously provided in the `cancel_deployed_timer()` method which has been
+      removed.
+
+### Removed
+
+- Removed the `cancel_deployed_timer()` method from the `Automation` class of the `pipeline` module
+  - this functionality is available through the new `cancel_deployed_data_feed()` method when `deployment_type`
+    parameter set to `['Timer']`
+
+## 0.2.4
+
+### Changed
+
+- New parameter `data_feed_name` added to the 4 deployment methods in the `pipeline` module's `Automation` class
+  - `deploy_as_email_data_feed()`
+  - `deploy_as_ftp_data_feed()`
+  - `deploy_as_upload_data_feed()`
+  - `deploy_on_timer()`
+
+## 0.2.3
+
+### Fixed
+
+- Resolved minor issue on `register_task()` method for the `Automation` class in the `pipeline` module.
+
+## 0.2.2
+
+### Fixed
+
+- Resolved minor issue on `upsert_discovered_records()` function in `integration` module related to device-level
+  and sensor-level tags.
+
+## 0.2.1
+
+### Added
+
+- New class added to the `pipeline` module
+  - `DiscoverableIntegrationTask` - for API integrations that are discoverable.
+    - requires `process()` & `run_discovery()` abstract methods to be created when sub-classing
+    - additional abstract property, `integration_device_type_definition`, required compared to base `Task`
+- New function `upsert_discovered_records()` added to the `integration` module
+  - Required for the `DiscoverableIntegrationTask.run_discovery()` method to upsert discovery records to Build -
+    Discovery & Selection UI
+
+### Fixed
+
+- Set minimum msal version required for the switch_api package to be installed.
+
+## 0.2.0
+
+Major overhaul done of the switch_api package. A complete replacement of the API used by the package was done.
+
+### Changed
+
+- The `user_id` parameter has been removed from the `switch_api.initialise()` function.
+  - Authentication of the user is now done via Switch Platform SSO. The call to initialise will trigger a web browser
+    window to open to the platform login screen.
+    - Note: each call to initialise for a portfolio in a different datacentre will open up browser and requires user to
+      input their username & password.
+    - for initialise on a different portfolio within the same datacentre, the authentication is cached so user will not
+      be asked to login again.
+- `api_inputs` is now a required parameter for the `switch_api.pipeline.Automation.register_task()`
+- The `deploy_on_timer()`, `deploy_as_email_data_feed()`, `deploy_as_upload_data_feed()`, and
+  `deploy_as_ftp_data_feed()` methods on the `switch_api.pipeline.Automation` class have an added parameter:
+  `data_feed_id`
+  - This new parameter allows user to update an existing deployment for the portfolio specified in the `api_inputs`.
+  - If `data_feed_id` is not supplied, a new data feed instance will be created (even if portfolio already has that
+    task deployed to it)
+
+## 0.1.18
+
+### Changed
+
+- removed rebuild of the ObjectProperties table in ADX on call to `upsert_device_sensors()`
+- removed rebuild of the Installation table in ADX on call to `upsert_sites()`
+
+## 0.1.17
+
+### Fixed
+
+- Fixed issue with `deploy_on_timer()` method of the `Automation` class in the `pipeline` module.
+- Fixed column header issue with the `get_tag_groups()` function of the `integration` module.
+- Fixed missing Meta column on table generated via `upsert_workorders()` function of the `integration` module.
+
+### Added
+
+- New method for uploading custom data to blob `Blob.custom_upload()`
+
+### Updated
+
+- Updated the `upsert_device_sensors()` to improve performance and aid release of future functionality.
+
+## 0.1.16
+
+### Added
+
+To the `pipeline` module:
+
+- New method `data_feed_history_process_errors()`, to the `Automation` class.
+  - This method returns a dataframe containing the distinct set of error types encountered for a specific
+    `data_feed_file_status_id`
+- New method `data_feed_history_errors_by_type` , to the `Automation` class.
+  - This method returns a dataframe containing the actual errors identified for the specified `error_type` and
+    `data_feed_file_status_id`
+
+Additional logging was also incorporated in the backend to support the Switch Platform UI.
+
+### Fixed
+
+- Fixed issue with `register()` method of the `Automation` class in the `pipeline` module.
+
+### Changed
+
+For the `pipeline` module:
+
+- Standardised the following methods of the `Automation` class to return pandas.DataFrame objects.
+- Added additional error checks to ensure only allowed values are passed to the various `Automation` class methods
+  for the parameters:
+  - `expected_delivery`
+  - `deploy_type`
+  - `queue_name`
+  - `error_type`
+
+For the `integration` module:
+
+- Added additional error checks to ensure only allowed values are passed to `post_errors` function for the parameters:
+  - `error_type`
+  - `process_status`
+
+For the `dataset` module:
+
+- Added additional error check to ensure only allowed values are provided for the `query_language` parameter of the
+  `get_data` function.
+
+For the `_platform` module:
+
+- Added additional error checks to ensure only allowed values are provided for the `account` parameter.
+
+## 0.1.14
+
+### Changed
+
+- updated get_device_sensors() to not auto-detect the data type - to prevent issues such as stripping leading zeroes,
+  etc from metadata values.
+
+## 0.1.13
+
+### Added
+
+To the `pipeline` module:
+
+- Added a new method, `data_feed_history_process_output`, to the `Automation` class
+
+## 0.1.11
+
+### Changed
+
+- Update to access to `logger` - now available as `switch_api.pipeline.logger()`
+- Update to function documentation
+
+## 0.1.10
+
+### Changed
+
+- Updated the calculation of min/max date (for timezone conversions) inside the `upsert_device_sensors` function as
+  the previous calculation method will not be supported in a future release of numpy.
+
+### Fixed
+
+- Fixed issue with retrieval of tag groups and tags via the functions:
+  - `get_sites`
+  - `get_device_sensors`
+
+## 0.1.9
+
+### Added
+
+- New module `platform_insights`
+
+In the `integration` module:
+
+- New function `get_sites` added to lookup site information (optionally with site-level tags)
+- New function `get_device_sensors` added to assist with lookup of device/sensor information, optionally including
+  either metadata or tags.
+- New function `get_tag_groups` added to lookup list of sensor-level tag groups
+- New function `get_metadata_keys` added to lookup list of device-level metadata keys
+
+### Changed
+
+- Modifications to connections to storage accounts.
+- Additional parameter `queue_name` added to the following methods of the `Automation` class of the `pipeline`
+  module:
+  - `deploy_on_timer`
+  - `deploy_as_email_data_feed`
+  - `deploy_as_upload_data_feed`
+  - `deploy_as_ftp_data_feed`
+
+### Fixed
+
+In the `pipeline` module:
+
+- Addressed issue with the schema validation for the `upsert_workorders` function
+
+## 0.1.8
+
+### Changed
+
+In the `integrations` module:
+
+- Updated to batch upserts by DeviceCode to improve reliability & performance of the `upsert_device_sensors` function.
+
+### Fixed
+
+In the `analytics` module:
+
+- typing issue that caused error in the import of the switch_api package for python 3.8
+
+## 0.1.7
+
+### Added
+
+In the `integrations` module:
+
+- Added new function `upsert_workorders`
+  - Provides ability to ingest work order data into the Switch Automation Platform.
+  - Documentation provides details on required & optional fields in the input dataframe and also provides information
+    on allowed values for some fields.
+  - Two attributes available for function, added to assist with creation of scripts by providing list of required &
+    optional fields:
+    - `upsert_workorders.df_required_columns`
+    - `upsert_workorders.df_optional_columns`
+- Added new function `get_states_by_country`:
+  - Retrieves the list of states for a given country. Returns a dataframe containing both the state name and
+    abbreviation.
+- Added new function `get_equipment_classes`:
+  - Retrieves the list of allowed values for Equipment Class.
+    - EquipmentClass is a required field for the upsert_device_sensors function
+
+### Changed
+
+In the `integrations` module:
+
+- For the `upsert_device_sensors` function:
+  - New attributes added to assist with creation of tasks:
+    - `upsert_device_sensors.df_required_columns` - returns list of required columns for the input `df`
+  - Two new fields required to be present in the dataframe passed to function by parameter `df`:
+    - `EquipmentClass`
+    - `EquipmentLabel`
+  - Fix to documentation so required fields in documentation match.
+- For the `upsert_sites` function:
+  - New attributes added to assist with creation of tasks:
+    - `upsert_sites.df_required_columns` - returns list of required columns for the input `df`
+    - `upsert_sites.df_optional_columns` - returns list of required columns for the input `df`
+- For the `get_templates` function:
+  - Added functionality to filter by type via new parameter `object_property_type`
+  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
+    converted to lowercase.
+- For the `get_units_of_measure` function:
+  - Added functionality to filter by type via new parameter `object_property_type`
+  - Fixed capitalisation issue where first character of column names in dataframe returned by the function had been
+    converted to lowercase.
+
+In the `analytics` module:
+
+- Modifications to type hints and documentation for the functions:
+  - `get_clone_modules_list`
+  - `run_clone_modules`
+- Additional logging added to `run_clone_modules`
+
+## 0.1.6
+
+### Added
+
+- Added new function `upsert_timeseries_ds()` to the `integrations` module
+
+### Changed
+
+- Additional logging added to `invalid_file_format()` function from the `error_handlers` module.
+
+### Removed
+
+- Removed `append_timeseries()` function
+
+## 0.1.5
+
+### Fixed
+
+- bug with `upsert_sites()` function that caused optional columns to be treated as required columns.
+
+### Added
+
+Added additional functions to the `error_handlers` module:
+
+- `validate_datetime()` - which checks whether the values of the datetime column(s) of the source file are valid. Any
+  datetime errors identified by this function should be passed to the `post_errors()` function.
+- `post_errors()` - used to post errors (apart from those identified by the `invalid_file_format()` function) to
+  the data feed dashboard.
+
+## 0.1.4
+
+### Changed
+
+Added additional required properties to the Abstract Base Classes (ABC): Task, IntegrationTask, AnalyticsTask,
+LogicModuleTask. These properties are:
+
+- Author
+- Version
+
+Added additional parameter `query_language` to the `switch.integration.get_data()` function. Allowed values for this
+parameter are:
+
+- `sql`
+- `kql`
+
+Removed the `name_as_filename` and `treat_as_timeseries` parameter from the following functions:
+
+- `switch.integration.replace_data()`
+- `switch.integration.append_data()`
+- `switch.integration.upload_data()`
+
+
```

### Comparing `switch_api-0.5.4b2/switch_api.egg-info/SOURCES.txt` & `switch_api-0.5.4b3/switch_api.egg-info/SOURCES.txt`

 * *Files identical despite different names*

