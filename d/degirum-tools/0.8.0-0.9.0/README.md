# Comparing `tmp/degirum_tools-0.8.0-py3-none-any.whl.zip` & `tmp/degirum_tools-0.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,24 +1,27 @@
-Zip file size: 58541 bytes, number of entries: 22
--rw-r--r--  2.0 unx      658 b- defN 24-Feb-26 05:22 degirum_tools/__init__.py
--rw-r--r--  2.0 unx      230 b- defN 24-Feb-26 05:22 degirum_tools/_version.py
--rw-r--r--  2.0 unx     7158 b- defN 24-Feb-26 05:22 degirum_tools/audio_support.py
--rw-r--r--  2.0 unx    30923 b- defN 24-Feb-26 05:22 degirum_tools/compound_models.py
--rw-r--r--  2.0 unx    10119 b- defN 24-Feb-26 05:22 degirum_tools/detection_eval.py
--rw-r--r--  2.0 unx     6731 b- defN 24-Feb-26 05:22 degirum_tools/environment.py
--rw-r--r--  2.0 unx     1538 b- defN 24-Feb-26 05:22 degirum_tools/image_tools.py
--rw-r--r--  2.0 unx    11554 b- defN 24-Feb-26 05:22 degirum_tools/inference_support.py
--rw-r--r--  2.0 unx     7215 b- defN 24-Feb-26 05:22 degirum_tools/line_count.py
--rw-r--r--  2.0 unx    18817 b- defN 24-Feb-26 05:22 degirum_tools/math_support.py
--rw-r--r--  2.0 unx     5783 b- defN 24-Feb-26 05:22 degirum_tools/object_selector.py
--rw-r--r--  2.0 unx    35629 b- defN 24-Feb-26 05:22 degirum_tools/object_tracker.py
--rw-r--r--  2.0 unx     1644 b- defN 24-Feb-26 05:22 degirum_tools/result_analyzer_base.py
--rw-r--r--  2.0 unx    26533 b- defN 24-Feb-26 05:22 degirum_tools/streams.py
--rw-r--r--  2.0 unx    17182 b- defN 24-Feb-26 05:22 degirum_tools/ui_support.py
--rw-r--r--  2.0 unx     7662 b- defN 24-Feb-26 05:22 degirum_tools/video_support.py
--rw-r--r--  2.0 unx    12865 b- defN 24-Feb-26 05:22 degirum_tools/zone_count.py
--rw-r--r--  2.0 unx     1070 b- defN 24-Feb-26 05:22 degirum_tools-0.8.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     1800 b- defN 24-Feb-26 05:22 degirum_tools-0.8.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Feb-26 05:22 degirum_tools-0.8.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       14 b- defN 24-Feb-26 05:22 degirum_tools-0.8.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1888 b- defN 24-Feb-26 05:22 degirum_tools-0.8.0.dist-info/RECORD
-22 files, 207105 bytes uncompressed, 55475 bytes compressed:  73.2%
+Zip file size: 63020 bytes, number of entries: 25
+-rw-r--r--  2.0 unx      754 b- defN 24-Mar-20 18:49 degirum_tools/__init__.py
+-rw-r--r--  2.0 unx      230 b- defN 24-Mar-20 18:49 degirum_tools/_version.py
+-rw-r--r--  2.0 unx     7158 b- defN 24-Mar-20 18:49 degirum_tools/audio_support.py
+-rw-r--r--  2.0 unx     6250 b- defN 24-Mar-20 18:49 degirum_tools/classification_eval.py
+-rw-r--r--  2.0 unx    30923 b- defN 24-Mar-20 18:49 degirum_tools/compound_models.py
+-rw-r--r--  2.0 unx     7693 b- defN 24-Mar-20 18:49 degirum_tools/detection_eval.py
+-rw-r--r--  2.0 unx     6731 b- defN 24-Mar-20 18:49 degirum_tools/environment.py
+-rw-r--r--  2.0 unx     2466 b- defN 24-Mar-20 18:49 degirum_tools/eval_support.py
+-rw-r--r--  2.0 unx     1538 b- defN 24-Mar-20 18:49 degirum_tools/image_tools.py
+-rw-r--r--  2.0 unx    11554 b- defN 24-Mar-20 18:49 degirum_tools/inference_support.py
+-rw-r--r--  2.0 unx     7215 b- defN 24-Mar-20 18:49 degirum_tools/line_count.py
+-rw-r--r--  2.0 unx    18817 b- defN 24-Mar-20 18:49 degirum_tools/math_support.py
+-rw-r--r--  2.0 unx     5783 b- defN 24-Mar-20 18:49 degirum_tools/object_selector.py
+-rw-r--r--  2.0 unx    35629 b- defN 24-Mar-20 18:49 degirum_tools/object_tracker.py
+-rw-r--r--  2.0 unx     4106 b- defN 24-Mar-20 18:49 degirum_tools/regression_eval.py
+-rw-r--r--  2.0 unx     1644 b- defN 24-Mar-20 18:49 degirum_tools/result_analyzer_base.py
+-rw-r--r--  2.0 unx    26533 b- defN 24-Mar-20 18:49 degirum_tools/streams.py
+-rw-r--r--  2.0 unx    19066 b- defN 24-Mar-20 18:49 degirum_tools/ui_support.py
+-rw-r--r--  2.0 unx     7662 b- defN 24-Mar-20 18:49 degirum_tools/video_support.py
+-rw-r--r--  2.0 unx    12865 b- defN 24-Mar-20 18:49 degirum_tools/zone_count.py
+-rw-r--r--  2.0 unx     1070 b- defN 24-Mar-20 18:50 degirum_tools-0.9.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1800 b- defN 24-Mar-20 18:50 degirum_tools-0.9.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Mar-20 18:50 degirum_tools-0.9.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       14 b- defN 24-Mar-20 18:50 degirum_tools-0.9.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2155 b- defN 24-Mar-20 18:50 degirum_tools-0.9.0.dist-info/RECORD
+25 files, 219748 bytes uncompressed, 59532 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -3,23 +3,29 @@
 
 Filename: degirum_tools/_version.py
 Comment: 
 
 Filename: degirum_tools/audio_support.py
 Comment: 
 
+Filename: degirum_tools/classification_eval.py
+Comment: 
+
 Filename: degirum_tools/compound_models.py
 Comment: 
 
 Filename: degirum_tools/detection_eval.py
 Comment: 
 
 Filename: degirum_tools/environment.py
 Comment: 
 
+Filename: degirum_tools/eval_support.py
+Comment: 
+
 Filename: degirum_tools/image_tools.py
 Comment: 
 
 Filename: degirum_tools/inference_support.py
 Comment: 
 
 Filename: degirum_tools/line_count.py
@@ -30,14 +36,17 @@
 
 Filename: degirum_tools/object_selector.py
 Comment: 
 
 Filename: degirum_tools/object_tracker.py
 Comment: 
 
+Filename: degirum_tools/regression_eval.py
+Comment: 
+
 Filename: degirum_tools/result_analyzer_base.py
 Comment: 
 
 Filename: degirum_tools/streams.py
 Comment: 
 
 Filename: degirum_tools/ui_support.py
@@ -45,23 +54,23 @@
 
 Filename: degirum_tools/video_support.py
 Comment: 
 
 Filename: degirum_tools/zone_count.py
 Comment: 
 
-Filename: degirum_tools-0.8.0.dist-info/LICENSE
+Filename: degirum_tools-0.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: degirum_tools-0.8.0.dist-info/METADATA
+Filename: degirum_tools-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: degirum_tools-0.8.0.dist-info/WHEEL
+Filename: degirum_tools-0.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: degirum_tools-0.8.0.dist-info/top_level.txt
+Filename: degirum_tools-0.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: degirum_tools-0.8.0.dist-info/RECORD
+Filename: degirum_tools-0.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## degirum_tools/__init__.py

```diff
@@ -6,20 +6,23 @@
 #
 
 # flake8: noqa
 
 from ._version import __version__, __version_info__
 from .audio_support import *
 from .compound_models import *
+from .classification_eval import *
+from .detection_eval import *
 from .environment import *
 from .inference_support import *
 from .line_count import *
 from .math_support import *
 from .object_selector import *
 from .object_tracker import *
+from .regression_eval import *
 from .ui_support import *
 from .video_support import *
 from .zone_count import *
 
 # aliases for backward compatibility
 from .environment import (
     in_colab as _in_colab,
```

## degirum_tools/_version.py

```diff
@@ -2,9 +2,9 @@
 # _version.py: degirum_tools package version
 #
 # Copyright DeGirum Corporation 2024
 # All rights reserved
 #
 
 # >>> increment version here vvv
-__version_info__ = ("0", "8", "0")
+__version_info__ = ("0", "9", "0")
 __version__ = ".".join(__version_info__)
```

## degirum_tools/detection_eval.py

```diff
@@ -1,214 +1,79 @@
 #
-# detection_eval.py: evaluation toolkit for detection models used in PySDK samples
+# detection_eval.py: object detection models evaluator
 #
-# Copyright DeGirum Corporation 2023
+# Copyright DeGirum Corporation 2024
 # All rights reserved
 #
 
-import yaml
-import json
-import os
-from typing import List
-import numpy as np
+import json, os, degirum as dg, numpy as np
+from typing import List, Optional
 from pycocotools.coco import COCO
 from pycocotools.cocoeval import COCOeval
 from .math_support import xyxy2xywh
+from .eval_support import ModelEvaluatorBase
+from .ui_support import Progress, stdoutRedirector
 
 
-def process_keypoints(keypoints_res: List[dict]) -> List[float]:
+class ObjectDetectionModelEvaluator(ModelEvaluatorBase):
     """
-    Convert PySDK keypoint results format to pycocotools keypoint format
-
-    Args:
-        keypoints_res: The keypoint results dictionary output from PySDK.
-
-    Returns:
-        keypoints: The list of keypoint results in pycocotools format.
-    """
-    keypoints: List[float] = []
-    for ldmks in keypoints_res:
-        kypts = ldmks["landmark"]
-        kypts_score = ldmks["score"]
-        keypoints.extend(float(x) for x in kypts)
-        keypoints.append(kypts_score)
-    return keypoints
-
-
-def save_results_coco_json(results, jdict, image_id, class_map=None):
-    """Serialize YOLO predictions to COCO json format."""
-    max_category_id = 0
-    for result in results:
-        box = xyxy2xywh(np.asarray(result["bbox"]).reshape(1, 4) * 1.0)  # xywh
-        box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner
-        box = box.reshape(-1).tolist()
-        category_id = (
-            class_map[result["category_id"]] if class_map else result["category_id"]
-        )
-        # detection base result
-        detected_elem = {
-            "image_id": image_id,
-            "category_id": category_id,
-            "bbox": [np.round(x, 3) for x in box],
-            "score": np.round(result["score"], 5),
-        }
-        # pose model addition
-        if "landmarks" in result:
-            detected_elem["keypoints"] = process_keypoints(result["landmarks"])
-        #
-        jdict.append(detected_elem)
-        max_category_id = max(max_category_id, category_id)
-    return max_category_id
-
-
-def evaluate_coco(
-    anno: COCO, pred: COCO, mAP_type: str = "bbox", img_id_list: List[str] = []
-):
-    """Evaluation process based on the annotation COCO object and the predic
-
-        Args:
-            anno (COCO): COCO ground truth annotation object
-            pred (COCO): COCO prediction object
-            img_id_list (List): List of the image ids to evaluate on.
-
-    Returns the mAP statistics.
+    This class evaluates the mAP for Object Detection models.
     """
-    eval_obj = COCOeval(anno, pred, mAP_type)
-    if img_id_list:
-        eval_obj.params.imgIds = [id for id in img_id_list]  # image IDs to evaluate
-    eval_obj.evaluate()
-    eval_obj.accumulate()
-    eval_obj.summarize()
-
-    return eval_obj.stats
-
 
-def is_pose_model(element: dict):
-    """Check if the it is a PySDK pose model
-        Args:
-            element (dict): detection result dict
-
-    Returns True if it is a pose model.
-    """
-    return True if "keypoints" in element else False
-
-
-class ObjectDetectionModelEvaluator:
-    def __init__(
-        self,
-        dg_model,
-        classmap=None,
-        pred_path=None,
-        output_confidence_threshold=0.001,
-        output_nms_threshold=0.7,
-        output_max_detections=300,
-        output_max_detections_per_class=100,
-        output_max_classes_per_detection=1,
-        output_use_regular_nms=True,
-        input_resize_method="bilinear",
-        input_pad_method="letterbox",
-        image_backend="opencv",
-        input_img_fmt="JPEG",
-        input_letterbox_fill_color=(114, 114, 114),
-        input_numpy_colorspace="auto",
-    ):
+    def __init__(self, model: dg.model.Model, **kwargs):
         """
         Constructor.
-            This class evaluates the mAP for Object Detection models.
 
-            Args:
-                dg_model (Detection model): Detection model from the Degirum model zoo.
-                class_map (json): A json file that contains classes with its class ids, each category would have a list of class ids.
-                pred_path (str): Path to save the predictions as a json file.
-                output_conf_threshold (float): Output Confidence threshold.
-                output_nms_threshold (float): Output Non-Max Suppression threshold.
-                max_detections (int): Maximum Detections.
-                max_detections_per_class (int): Maximum Detections Per Class.
-                max_classes_per_detection (int): Maximum Classes Per Detection.
-                use_regular_nms (boolean): Whether to use Regular Non-Max Suppression.
-                input_resize_method (str): Input Resize Method.
-                input_pad_method (str): Input Pad Method.
-                image_backend (str): Image Backend.
-                input_img_fmt (str): InputImgFmt.
-                input_letterbox_fill_color (tuple): the RGB color for padding used in letterbox
-                input_numpy_colorspace (str): input colorspace: ("BGR" to match OpenCV image backend)
+        Args:
+            model (Detection model): PySDK detection model object
+            kwargs (dict): arbitrary set of PySDK model parameters and the following evaluation parameters:
+                show_progress (bool): show progress bar
+                classmap (dict): dictionary which maps model category IDs to dataset category IDs
+                pred_path (str): path to save the predictions as a JSON file of None if not required
         """
 
-        self.dg_model = dg_model
-        self.classmap = classmap
-        self.pred_path = pred_path
-
-        if (
-            self.dg_model.output_postprocess_type in ["Detection", "DetectionYolo", "DetectionYoloV8"]
-        ):
-            self.dg_model.output_confidence_threshold = output_confidence_threshold
-            self.dg_model.output_nms_threshold = output_nms_threshold
-            self.dg_model.output_max_detections = output_max_detections
-            self.dg_model.output_max_detections_per_class = (
-                output_max_detections_per_class
-            )
-            self.dg_model.output_max_classes_per_detection = (
-                output_max_classes_per_detection
-            )
-            self.dg_model.output_use_regular_nms = output_use_regular_nms
-            self.dg_model.input_resize_method = input_resize_method
-            self.dg_model.input_pad_method = input_pad_method
-            self.dg_model.image_backend = image_backend
-            self.dg_model.input_image_format = input_img_fmt
-            self.dg_model.input_numpy_colorspace = input_numpy_colorspace
-            self.dg_model.input_letterbox_fill_color = input_letterbox_fill_color
-        else:
-            raise Exception("Model loaded for evaluation is not a Detection Model")
-
-    @classmethod
-    def init_from_yaml(cls, dg_model, config_yaml):
-        """
-        config_yaml (str) : Path of the yaml file that contains all the arguments.
+        #
+        # detection evaluator parameters:
+        #
 
-        """
-        with open(config_yaml) as f:
-            args = yaml.load(f, Loader=yaml.FullLoader)
+        # dictionary which maps model category IDs to dataset category IDs
+        self.classmap: Optional[dict] = None
+        # path to save the predictions as a JSON file
+        self.pred_path: Optional[str] = None
+
+        if model.output_postprocess_type not in [
+            "Detection",
+            "DetectionYolo",
+            "DetectionYoloV8",
+        ]:
+            raise Exception("Model loaded for evaluation is not a Detection Model")
 
-        return cls(
-            dg_model=dg_model,
-            classmap=args["classmap"],
-            pred_path=args["pred_path"],
-            output_confidence_threshold=args["output_confidence_threshold"],
-            output_nms_threshold=args["output_nms_threshold"],
-            output_max_detections=args["output_max_detections"],
-            output_max_detections_per_class=args["output_max_detections_per_class"],
-            output_max_classes_per_detection=args["output_max_classes_per_detection"],
-            output_use_regular_nms=args["output_use_regular_nms"],
-            input_resize_method=args["input_resize_method"],
-            input_pad_method=args["input_pad_method"],
-            image_backend=args["image_backend"],
-            input_img_fmt=args["input_img_fmt"],
-            input_letterbox_fill_color=tuple(args["input_letterbox_fill_color"]),
-            input_numpy_colorspace=args["input_numpy_colorspace"],
-        )
+        # base constructor assigns kwargs to model or to self
+        super().__init__(model, **kwargs)
 
     def evaluate(
         self,
         image_folder_path: str,
         ground_truth_annotations_path: str,
-        num_val_images: int = 0,
-        print_frequency: int = 0,
-    ):
-        """Evaluation for the Detection model.
+        max_images: int = 0,
+    ) -> list:
+        """
+        Evaluation for the detection model.
 
-            Args:
-                image_folder_path (str): Path to the image dataset.
-                ground_truth_annotations_path (str): Path to the groundtruth json annotations.
-                num_val_images (int): max number of images used for evaluation. 0: all images in image_folder_path is used.
-                print_frequency (int): Number of image batches to be evaluated before printing num evaluated images
+        Args:
+            image_folder_path (str): Path to images
+            ground_truth_annotations_path (str): Path to the ground truth JSON annotations file (COCO format)
+            max_images (int): max number of images used for evaluation. 0: all images in `image_folder_path` are used.
 
-        Returns the mAP statistics.
+        Returns the mAP statistics: [bbox_stats, kp_stats] for pose detection models and [bbox_stats] for non-pose models.
         """
+
         jdict: List[dict] = []
-        anno = COCO(ground_truth_annotations_path)
+        with stdoutRedirector():
+            anno = COCO(ground_truth_annotations_path)
         num_images = len(anno.dataset["images"])
         files_dict = anno.dataset["images"][0:num_images]
         path_list: List[str] = []
         img_id_list: List[str] = []
         for image_number in range(0, num_images):
             image_id = files_dict[image_number]["id"]
             path = os.path.join(
@@ -219,42 +84,125 @@
                 img_id_list.append(image_id)
 
         # sort the image ids
         sorted_indices = sorted(range(len(img_id_list)), key=lambda i: img_id_list[i])
         sorted_img_id_list = [img_id_list[i] for i in sorted_indices]
         sorted_path_list = [path_list[i] for i in sorted_indices]
 
-        if num_val_images > 0:
-            sorted_path_list = sorted_path_list[0:num_val_images]
+        if max_images > 0:
+            sorted_path_list = sorted_path_list[0:max_images]
 
-        with self.dg_model:
+        with self.model:
+            if self.show_progress:
+                progress = Progress(len(sorted_path_list))
             for image_number, predictions in enumerate(
-                self.dg_model.predict_batch(sorted_path_list)
+                self.model.predict_batch(sorted_path_list)
             ):
-                if print_frequency > 0:
-                    if image_number % print_frequency == print_frequency - 1:
-                        print(image_number + 1)
+                if self.show_progress:
+                    progress.step()
                 image_id = sorted_img_id_list[image_number]
-                save_results_coco_json(
+                ObjectDetectionModelEvaluator._save_results_coco_json(
                     predictions.results, jdict, image_id, self.classmap
                 )
 
         # save the predictions to a json file
         if self.pred_path:
             with open(self.pred_path, "w") as f:
                 json.dump(jdict, f, indent=4)
 
-        pred = anno.loadRes(jdict)
+        with stdoutRedirector():
+            pred = anno.loadRes(jdict)
+
+            stats = []
+            # bounding box map calculation
+            bbox_stats = ObjectDetectionModelEvaluator._evaluate_coco(
+                anno, pred, mAP_type="bbox", img_id_list=sorted_img_id_list
+            )
+            stats.append(bbox_stats)
+            # pose keypoint map calculation
+            if ObjectDetectionModelEvaluator._is_pose_model(jdict[0]):
+                kp_stats = ObjectDetectionModelEvaluator._evaluate_coco(
+                    anno, pred, mAP_type="keypoints", img_id_list=sorted_img_id_list
+                )
+                stats.append(kp_stats)
+            return stats
+
+    @staticmethod
+    def _process_keypoints(keypoints_res: List[dict]) -> List[float]:
+        """
+        Convert PySDK keypoint results format to pycocotools keypoint format
+
+        Args:
+            keypoints_res: The keypoint results dictionary output from PySDK.
 
-        stats = []
-        # bounding box map calculation
-        bbox_stats = evaluate_coco(
-            anno, pred, mAP_type="bbox", img_id_list=sorted_img_id_list
-        )
-        stats.append(bbox_stats)
-        # pose keypoint map calculation
-        if is_pose_model(jdict[0]):
-            kp_stats = evaluate_coco(
-                anno, pred, mAP_type="keypoints", img_id_list=sorted_img_id_list
+        Returns:
+            keypoints: The list of keypoint results in pycocotools format.
+        """
+        keypoints: List[float] = []
+        for ldmks in keypoints_res:
+            kypts = ldmks["landmark"]
+            kypts_score = ldmks["score"]
+            keypoints.extend(float(x) for x in kypts)
+            keypoints.append(kypts_score)
+        return keypoints
+
+    @staticmethod
+    def _save_results_coco_json(results, jdict, image_id, class_map=None):
+        """Serialize YOLO predictions to COCO json format."""
+        max_category_id = 0
+        for result in results:
+            box = xyxy2xywh(np.asarray(result["bbox"]).reshape(1, 4) * 1.0)  # xywh
+            box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner
+            box = box.reshape(-1).tolist()
+            category_id = (
+                class_map[result["category_id"]] if class_map else result["category_id"]
             )
-            stats.append(kp_stats)
-        return stats
+            # detection base result
+            detected_elem = {
+                "image_id": image_id,
+                "category_id": category_id,
+                "bbox": [np.round(x, 3) for x in box],
+                "score": np.round(result["score"], 5),
+            }
+            # pose model addition
+            if "landmarks" in result:
+                detected_elem["keypoints"] = (
+                    ObjectDetectionModelEvaluator._process_keypoints(
+                        result["landmarks"]
+                    )
+                )
+            #
+            jdict.append(detected_elem)
+            max_category_id = max(max_category_id, category_id)
+        return max_category_id
+
+    @staticmethod
+    def _evaluate_coco(
+        anno: COCO, pred: COCO, mAP_type: str = "bbox", img_id_list: List[str] = []
+    ):
+        """Evaluation process based on the ground truth COCO object and the prediction object
+
+            Args:
+                anno (COCO): COCO ground truth annotation object
+                pred (COCO): COCO prediction object
+                img_id_list (List): List of the image ids to evaluate on.
+
+        Returns the mAP statistics.
+        """
+        eval_obj = COCOeval(anno, pred, mAP_type)
+        if img_id_list:
+            eval_obj.params.imgIds = [id for id in img_id_list]  # image IDs to evaluate
+        eval_obj.evaluate()
+        eval_obj.accumulate()
+        eval_obj.summarize()
+
+        return eval_obj.stats
+
+    @staticmethod
+    def _is_pose_model(element: dict):
+        """Check if the it is a PySDK pose model
+            Args:
+                element (dict): detection result dict
+
+        Returns True if it is a pose model.
+        """
+        return True if "keypoints" in element else False
```

## degirum_tools/ui_support.py

```diff
@@ -3,15 +3,15 @@
 #
 # Copyright DeGirum Corporation 2023-2024
 # All rights reserved
 #
 # Implements classes and functions to handle image display, progress indication, etc.
 #
 
-import cv2, os, time, PIL.Image, numpy as np
+import cv2, sys, os, time, PIL.Image, numpy as np, random, string
 from .environment import get_test_mode, in_colab, in_notebook, to_valid_filename
 from .image_tools import crop, luminance
 from dataclasses import dataclass
 from typing import Optional, Any, List
 from enum import Enum
 from pathlib import Path
 
@@ -23,42 +23,48 @@
 
 def color_complement(color):
     """Return color complement: 255 - color"""
     adj_color = (color[0] if isinstance(color, list) else color)[::-1]
     return tuple([255 - c for c in adj_color])
 
 
-def ipython_display(obj: Any, clear: bool = False):
+def ipython_display(obj: Any, clear: bool = False, display_id: Optional[str] = None):
     """
     Display object in IPython environment
 
     Args:
         obj - object to display; can be PIL/OpenCV image object or image/video filename/URL
         clear - True to clear previous output
     """
 
     import IPython.display
 
     if isinstance(obj, PIL.Image.Image):
         # PIL image
-        IPython.display.display(obj, clear=clear)
+        IPython.display.display(obj, clear=clear, display_id=display_id)
     elif isinstance(obj, np.ndarray):
         # OpenCV image
-        IPython.display.display(PIL.Image.fromarray(obj[..., ::-1]), clear=clear)
+        IPython.display.display(
+            PIL.Image.fromarray(obj[..., ::-1]), clear=clear, display_id=display_id
+        )
     elif isinstance(obj, str):
         # filename or URL
         is_url = obj.startswith("http")
         if obj.endswith(".mp4") or obj.endswith(".avi"):
             # video
             IPython.display.display(
-                IPython.display.Video(obj, embed=in_colab() and not is_url), clear=clear
+                IPython.display.Video(obj, embed=in_colab() and not is_url),
+                clear=clear,
+                display_id=display_id,
             )
         else:
             # assume image
-            IPython.display.display(IPython.display.Image(obj), clear=clear)
+            IPython.display.display(
+                IPython.display.Image(obj), clear=clear, display_id=display_id
+            )
     else:
         raise Exception(f"ipython_display: unsupported object type {type(obj)}")
 
 
 class CornerPosition(Enum):
     """Corner position options"""
 
@@ -245,29 +251,44 @@
         self._no_gui = not Display._check_gui() or get_test_mode()
         self._w = w
         self._h = h
         self._video_writer: Optional[Any] = None
         self._video_file: Optional[str] = None
         self._display_id: Optional[str] = None
 
+    def _update_notebook_display(self, obj: Any):
+        """Update notebook display with given object
+
+        Args:
+            obj - object to display
+        """
+
+        import IPython.display
+
+        if self._display_id is None:
+            self._display_id = "dg_show_" + "".join(random.choices(string.digits, k=10))
+            IPython.display.display(obj, display_id=self._display_id)
+        else:
+            IPython.display.update_display(obj, display_id=self._display_id)
+
     def __enter__(self):
         return self
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         # close OpenCV window in any
         if self._window_created:
             cv2.destroyWindow(self._capt)
 
         # close video writer if any, and show video in Colab
         if self._video_writer is not None:
             self._video_writer.release()
             if in_colab():
                 import IPython.display
 
-                IPython.display.display(
+                self._update_notebook_display(
                     IPython.display.Video(self._video_file, embed=True)
                 )
 
         return exc_type is KeyboardInterrupt  # ignore KeyboardInterrupt errors
 
     @property
     def window_name(self) -> str:
@@ -307,19 +328,17 @@
     def show(self, img: Any, waitkey_delay: int = 1):
         """Show image or model result
 
         img - numpy array with valid OpenCV image, or PIL image, or model result object
         waitkey_delay - delay in ms for waitKey() call; use 0 to show still images, use 1 for streaming video
         """
 
-        import IPython.display
-
         # show image in notebook
         def show_in_notebook(img):
-            IPython.display.display(PIL.Image.fromarray(img[..., ::-1]), clear=True)
+            self._update_notebook_display(PIL.Image.fromarray(img[..., ::-1]))
 
         if hasattr(img, "image_overlay"):
             # special case for model results: call it recursively
             self.show(img.image_overlay, waitkey_delay)
             return
 
         if isinstance(img, PIL.Image.Image):
@@ -348,22 +367,18 @@
                     self._video_writer.write(img)
 
                     class printer(str):
                         def __repr__(self):
                             return self
 
                     if self._video_writer.count % 10 == 0:
-                        if self._display_id is None:
-                            self._display_id = "dg_show_" + str(time.time_ns())
-
-                        IPython.display.display(
+                        self._update_notebook_display(
                             printer(
                                 f"{self._video_file}: frame {self._video_writer.count}, {fps:.1f} FPS"
-                            ),
-                            display_id=self._display_id,
+                            )
                         )
 
             elif self._no_gui and in_notebook():
                 # show image in notebook when possible
                 show_in_notebook(img)
             else:
                 # show image in OpenCV window
@@ -432,36 +447,45 @@
         """
         self._display_id: Optional[str] = None
         self._len = bar_len
         self._last_step = last_step
         self._start_step = start_step
         self._time_to_refresh = lambda: time.time() - self._last_update_time > 0.5
         self._speed_units = speed_units
+        self._message = ""
         self.reset()
 
     def reset(self):
         self._start_time = time.time()
         self._step = self._start_step
         self._percent = 0.0
         self._last_updated_percent = self._percent
         self._last_update_time = 0.0
         self._tip_phase = 0
+        self._longest_line = 0
         self._update()
 
-    def step(self, steps: int = 1):
+    def step(self, steps: int = 1, *, message: Optional[str] = None):
         """Update progress by given number of steps
         steps - number of steps to advance
+        message - optional message to display
         """
         assert (
             self._last_step is not None
         ), "Progress indicator: to do stepping last step must be assigned on construction"
+        assert (
+            self._last_step > self._start_step
+        ), f"Progress indicator: last step {self._last_step} must be greater than start step {self._start_step}"
+
         self._step += steps
         self._percent = (
             100 * (self._step - self._start_step) / (self._last_step - self._start_step)
         )
+        if message is not None:
+            self._message = message
         if (
             self._percent - self._last_updated_percent >= 100 / self._len
             or self._percent >= 100
             or self._time_to_refresh()
         ):
             self._update()
 
@@ -486,14 +510,23 @@
             self._step = round(
                 0.01 * self._percent * (self._last_step - self._start_step)
                 + self._start_step
             )
         if delta >= 100 / self._len or self._time_to_refresh():
             self._update()
 
+    @property
+    def message(self) -> str:
+        return self._message
+
+    @message.setter
+    def message(self, value: str):
+        self._message = value
+        self._update()
+
     def _update(self):
         """Update progress bar"""
         self._last_updated_percent = self._percent
         bars = int(self._percent / 100 * self._len)
         elapsed_s = time.time() - self._start_time
 
         tips = "−\\/"
@@ -505,28 +538,57 @@
             prog_str += f" {self._step}/{self._last_step}"
 
         prog_str += f" [{elapsed_s:.1f}s elapsed"
         if self._percent > 0 and self._percent <= 100:
             remaining_est_s = elapsed_s * (100 - self._percent) / self._percent
             prog_str += f", {remaining_est_s:.1f}s remaining"
         if self._last_step is not None and elapsed_s > 0:
-            prog_str += f", {(self._step - self._start_step) / elapsed_s:.1f} {self._speed_units}]"
+            prog_str += f", {(self._step - self._start_step) / elapsed_s:.1f} {self._speed_units}] {self._message}"
         else:
             prog_str += "]"
 
-        class printer(str):
-            def __repr__(self):
-                return self
+        if in_notebook():
 
-        prog_str = printer(prog_str)
+            class printer(str):
+                def __repr__(self):
+                    return self
+
+            prog_str = printer(prog_str)
 
-        if in_notebook():
             import IPython.display
 
             if self._display_id is None:
-                self._display_id = "dg_progress_" + str(time.time_ns())
+                self._display_id = "dg_progress_" + "".join(
+                    random.choices(string.digits, k=10)
+                )
                 IPython.display.display(prog_str, display_id=self._display_id)
             else:
                 IPython.display.update_display(prog_str, display_id=self._display_id)
         else:
+            if len(prog_str) < self._longest_line:
+                prog_str += " " * (self._longest_line - len(prog_str))
+            else:
+                self._longest_line = len(prog_str)
+
             print(prog_str, end="\r")
+
         self._last_update_time = time.time()
+
+
+class stdoutRedirector:
+    """Redirect stdout to another stream"""
+
+    def __init__(self, stream: Optional[str] = None):
+        """
+        Constructor
+
+        Args:
+            stream: output stream to redirect to; None to redirect to null device
+        """
+        self._stdout = sys.stdout
+        self._stream = stream
+
+    def __enter__(self):
+        sys.stdout = open(os.devnull if self._stream is None else self._stream, "w")
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        sys.stdout = self._stdout
```

## Comparing `degirum_tools-0.8.0.dist-info/LICENSE` & `degirum_tools-0.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `degirum_tools-0.8.0.dist-info/METADATA` & `degirum_tools-0.9.0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: degirum_tools
-Version: 0.8.0
+Version: 0.9.0
 Summary: Tools for PySDK
 Author: DeGirum
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
```

## Comparing `degirum_tools-0.8.0.dist-info/RECORD` & `degirum_tools-0.9.0.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,22 +1,25 @@
-degirum_tools/__init__.py,sha256=pc99ZFnIFu4aBiwUgMhui7VMtH-CWFu9T1hmrQGWq5Y,658
-degirum_tools/_version.py,sha256=zo_HFAPNjIcFD0MY8QZmv-uZa4s3cNDSHJ1VWs28p6U,230
+degirum_tools/__init__.py,sha256=agZIRSiEqkj3a6HSywJC_zTQmis1oVpUHVIfHiarnxI,754
+degirum_tools/_version.py,sha256=GW2LN_OSXTEJce3Tvwip-nRA3iv-OFMwrUAGJqy9f-s,230
 degirum_tools/audio_support.py,sha256=bmKLFWl8iq1s2-3-CZmFgIHBp5kxH77c1ZRVqwmX74Y,7158
+degirum_tools/classification_eval.py,sha256=u6YWRb3_RTGOu2SOvfqvLOpQEtU_HE7zQU8D2LPN08I,6250
 degirum_tools/compound_models.py,sha256=MNcKB2JTXFcum5kKuaQWv8YPzTcKPl_PH8N3JQEbaYg,30923
-degirum_tools/detection_eval.py,sha256=KNNOlnMdsQ7LqzGw4qxe2zhyQ-PNTwpj4Xiht4n-9cE,10119
+degirum_tools/detection_eval.py,sha256=d0-TMrJHhglOTOyRiPgp0Z3tsNokganFZOieTomeFg4,7693
 degirum_tools/environment.py,sha256=l1vICgyX_8QrTkyNSLfl7071h4Ou98s379xHgfglhnw,6731
+degirum_tools/eval_support.py,sha256=U09DSQlka53a9l2yUKJNwVBQ78vKCUBhlUV8bC9c1GQ,2466
 degirum_tools/image_tools.py,sha256=YrHpkQBFYNLJbs0dnzVqzbm_Sawj5GvKIMRX1_L3028,1538
 degirum_tools/inference_support.py,sha256=51TlpA6bnwa5h1wCSold3iym4exVbB36a4fDjkatBXU,11554
 degirum_tools/line_count.py,sha256=FXOoTfa0acHBgAzFZq6W8uVBU9hultpeBbNqOdt3wZ0,7215
 degirum_tools/math_support.py,sha256=xvATsh5JpZs6keo3phJGvXoZTJfLXkFIg8tIsVHFeWs,18817
 degirum_tools/object_selector.py,sha256=j6M1Dy0QjJ3njQ7bslpfxzLDwauoIEa-VJevFu2Y1rM,5783
 degirum_tools/object_tracker.py,sha256=1SnodUWfZKqvMDlahYX6dGfXLHt_DcDmCPp4Y4MIBAg,35629
+degirum_tools/regression_eval.py,sha256=ZLPoaTCYUgGLfgZKRPLXrgFih15KZS-D6qHW4qftmq0,4106
 degirum_tools/result_analyzer_base.py,sha256=DLmjFjniZT-qgTAY4jSPK9aRMVQkJ30ouvsYI3xOlAU,1644
 degirum_tools/streams.py,sha256=Ngrcvf4jIKh_66wQv0mHPQuchHk_yYEgc0MhJeZIje4,26533
-degirum_tools/ui_support.py,sha256=-yfQYyms9DB_unqCpPFKT_RyXX7J4a5XOF1TuM3G30U,17182
+degirum_tools/ui_support.py,sha256=UJApZwccUWymL4lhGXgkM2-6p1-VON6NiBdhRwBMaVM,19066
 degirum_tools/video_support.py,sha256=IxuKVSeyUyCRclzo4jw5w4e11gi355GAdGHsKI9aam8,7662
 degirum_tools/zone_count.py,sha256=3xt5jAJmelzXjyz5VjG9SsMeCFBMEMpAYxNK7-kv_ws,12865
-degirum_tools-0.8.0.dist-info/LICENSE,sha256=Vc-Xo47AuHoe4kUmPeda5mftjEKG1vdm_NxhJI-b0C8,1070
-degirum_tools-0.8.0.dist-info/METADATA,sha256=UZV8JaqjxaGU4nntaPIyhRndBC8TkycWsqUTORnQYOs,1800
-degirum_tools-0.8.0.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-degirum_tools-0.8.0.dist-info/top_level.txt,sha256=PGr1NkNaRd6-VfCJPLDQkZWFYreqnmgc6SItEsTFXGg,14
-degirum_tools-0.8.0.dist-info/RECORD,,
+degirum_tools-0.9.0.dist-info/LICENSE,sha256=Vc-Xo47AuHoe4kUmPeda5mftjEKG1vdm_NxhJI-b0C8,1070
+degirum_tools-0.9.0.dist-info/METADATA,sha256=Fm6l9B1Y5cxyIiZ2f7tmIW5jumxZRKVW63kqmlwDlGs,1800
+degirum_tools-0.9.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+degirum_tools-0.9.0.dist-info/top_level.txt,sha256=PGr1NkNaRd6-VfCJPLDQkZWFYreqnmgc6SItEsTFXGg,14
+degirum_tools-0.9.0.dist-info/RECORD,,
```

